{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337c5b4d-2166-4dbb-8d40-b1e24e8150c7",
   "metadata": {},
   "source": [
    "# Motion Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326611f7-9177-4d12-8a1a-e9368a2e0fd6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4d46c-8f01-42b6-8879-0f8a770ea528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import utils\n",
    "from common import bvh_tools as bvh\n",
    "from common import fbx_tools as fbx\n",
    "from common import mocap_tools as mocap\n",
    "import motion_analysis as ma\n",
    "\n",
    "import torch\n",
    "from common.quaternion import qmul, qrot, qnormalize_np, slerp, qfix\n",
    "from common.pose_renderer import PoseRenderer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195ab52-2159-4279-b096-66bdf38c855e",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c81a0-bdfd-4fa0-945f-cd77c3ed692c",
   "metadata": {},
   "source": [
    "## Mocap Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50767207-34a4-4439-b9d7-27ffaecfbdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mocap_file_path = \"../../Data/Mocap/\"\n",
    "mocap_files = [\"Daniel_ChineseRoom_Take1_50fps.fbx\"]\n",
    "mocap_valid_frame_ranges = [ [ 400, 24600 ] ]\n",
    "mocap_pos_scale = 1.0\n",
    "mocap_fps = 50\n",
    "mocap_config_path = \"configs\"\n",
    "mocap_joint_weight_file = \"joint_weights_qualisys_fbx.json\"\n",
    "mocap_body_weight = 60\n",
    "\n",
    "mocap_pos_scale_gui = widgets.FloatText(mocap_pos_scale, description=\"Mocap Position Scale:\", style={'description_width': 'initial'})\n",
    "mocap_fps_gui = widgets.IntText(mocap_fps, description=\"Mocap FPS:\", style={'description_width': 'initial'})\n",
    "\n",
    "mocap_files_all = [f for f in os.listdir(mocap_file_path) if os.path.isfile(os.path.join(mocap_file_path, f))]\n",
    "#print(mocap_files_all)\n",
    "\n",
    "mocap_valid_frame_ranges_gui = widgets.Text(str(mocap_valid_frame_ranges), description='Mocap Valid Frames:', style={'description_width': 'initial'})\n",
    "\n",
    "mocap_joint_weight_files = [f for f in os.listdir(mocap_config_path) if os.path.isfile(os.path.join(mocap_config_path, f))]\n",
    "#print(mocap_joint_weight_files)\n",
    "\n",
    "mocap_files_gui = widgets.SelectMultiple(\n",
    "    options=mocap_files_all,\n",
    "    value=mocap_files,  # default: first option selected; can be empty\n",
    "    description='Mocap Files:',\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "mocap_joint_weight_files_gui = widgets.Dropdown(\n",
    "    options=mocap_joint_weight_files,\n",
    "    value=mocap_joint_weight_file,  # default selected value\n",
    "    description='Mocap Joint Weight Files:',\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "mocap_body_weight_gui = widgets.FloatText(mocap_body_weight, description=\"Mocap Body Weight (kg):\", style={'description_width': 'initial'})\n",
    "\n",
    "display(mocap_pos_scale_gui)\n",
    "display(mocap_fps_gui)\n",
    "display(mocap_files_gui)\n",
    "display(mocap_valid_frame_ranges_gui)\n",
    "display(mocap_joint_weight_files_gui)\n",
    "display(mocap_body_weight_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4fcdd-4ff9-446a-b611-c33eb1d3bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mocap_pos_scale = mocap_pos_scale_gui.value\n",
    "mocap_fps = mocap_fps_gui.value\n",
    "mocap_files = list(mocap_files_gui.value)\n",
    "mocap_valid_frame_ranges = ast.literal_eval(mocap_valid_frame_ranges_gui.value)\n",
    "mocap_joint_weight_file = mocap_joint_weight_files_gui.value\n",
    "mocap_body_weight = mocap_body_weight_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f9179-8516-4e48-8896-64334ddd0dfc",
   "metadata": {},
   "source": [
    "## Analysis Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac475be1-f77d-40ea-98aa-57ffb56d0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "mocap_excerpt_length = 80\n",
    "mocap_excerpt_offset = 40\n",
    "mocap_smooth_length = 25\n",
    "\n",
    "motion_feature_names = [\"bsphere\", \"space_effort\"]\n",
    "motion_feature_average = True # average motion features in the time domain\n",
    "\n",
    "mocap_excerpt_length_gui = widgets.IntText(value=mocap_excerpt_length, description=\"Mocap Excerpt Length (Frames):\", style={'description_width': 'initial'})\n",
    "mocap_excerpt_offset_gui = widgets.IntText(value=mocap_excerpt_offset, description=\"Mocap Excerpt Offset (Frames):\", style={'description_width': 'initial'})\n",
    "mocap_smooth_length_gui = widgets.IntText(value=mocap_smooth_length, description=\"Mocap Smooth Length (Frames):\", style={'description_width': 'initial'})\n",
    "motion_feature_names_gui = widgets.SelectMultiple(\n",
    "    options=[\"rot_local\", \"pos_world_m\", \"pos_world_smooth\", \"pos_scalar\", \"vel_world\", \"vel_world_smooth\", \"vel_world_scalar\", \"accel_world\", \"accel_world_smooth\", \"accel_world_scalar\", \"jerk_world\", \"jerk_world_smooth\", \"jerk_world_scalar\", \"qom\", \"bbox\", \"bsphere\", \"weight_effort\", \"space_effort\", \"time_effort\", \"flow_effort\"],\n",
    "    value=motion_feature_names,\n",
    "    description='Motion Features',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "motion_feature_average_gui = widgets.Checkbox(\n",
    "    value=motion_feature_average,\n",
    "    description='Motion Feature Average',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(mocap_excerpt_length_gui)\n",
    "display(mocap_excerpt_offset_gui)\n",
    "display(mocap_smooth_length_gui)\n",
    "display(motion_feature_names_gui)\n",
    "display(motion_feature_average_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d107c-a511-4c28-a095-a85973dd7fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mocap_excerpt_length = mocap_excerpt_length_gui.value\n",
    "mocap_excerpt_offset = mocap_excerpt_offset_gui.value\n",
    "mocap_smooth_length = mocap_smooth_length_gui.value\n",
    "audio_feature_names = list(motion_feature_names_gui.value)\n",
    "motion_feature_average = motion_feature_average_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e5459-3fa1-43bb-adfe-8ae78242e401",
   "metadata": {},
   "source": [
    "## Visualisation Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beacc145-f5a8-4f5e-b50f-9510dd34fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_ele = 90.0\n",
    "view_azi = -90.0\n",
    "view_line_width = 1.0\n",
    "view_size = 4.0\n",
    "\n",
    "view_ele_gui = widgets.FloatText(value=view_ele, description=\"View Elevation:\", style={'description_width': 'initial'})\n",
    "view_azi_gui = widgets.FloatText(value=view_azi, description=\"View Azimuth:\", style={'description_width': 'initial'})\n",
    "view_line_width_gui = widgets.FloatText(value=view_line_width, description=\"View Line Width:\", style={'description_width': 'initial'})\n",
    "view_size_gui = widgets.FloatText(value=view_size, description=\"View Size:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(view_ele_gui)\n",
    "display(view_azi_gui)\n",
    "display(view_line_width_gui)\n",
    "display(view_size_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a2340-c7e3-4f62-b2c0-d38032cc043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_ele = view_ele_gui.value\n",
    "view_azi = view_azi_gui.value\n",
    "view_line_width = view_line_width_gui.value\n",
    "view_size = view_size_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440807e4-f499-47e9-9af1-019382bfc21b",
   "metadata": {},
   "source": [
    "## Load Mocap Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b993ec-7b34-47aa-98d1-c2c1b7947ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bvh_tools = bvh.BVH_Tools()\n",
    "fbx_tools = fbx.FBX_Tools()\n",
    "mocap_tools = mocap.Mocap_Tools()\n",
    "\n",
    "all_mocap_data = []\n",
    "\n",
    "for mocap_file in mocap_files:\n",
    "    \n",
    "    print(\"process file \", mocap_file)\n",
    "    \n",
    "    if mocap_file.endswith(\".bvh\") or mocap_file.endswith(\".BVH\"):\n",
    "        bvh_data = bvh_tools.load(mocap_file_path + \"/\" + mocap_file)\n",
    "        mocap_data = mocap_tools.bvh_to_mocap(bvh_data)\n",
    "    elif mocap_file.endswith(\".fbx\") or mocap_file.endswith(\".FBX\"):\n",
    "        fbx_data = fbx_tools.load(mocap_file_path + \"/\" + mocap_file)\n",
    "        mocap_data = mocap_tools.fbx_to_mocap(fbx_data)[0] # first skeleton only\n",
    "    \n",
    "    mocap_data[\"skeleton\"][\"offsets\"] *= mocap_pos_scale\n",
    "    mocap_data[\"motion\"][\"pos_local\"] *= mocap_pos_scale\n",
    "    \n",
    "    # set x and z offset of root joint to zero\n",
    "    mocap_data[\"skeleton\"][\"offsets\"][0, 0] = 0.0 \n",
    "    mocap_data[\"skeleton\"][\"offsets\"][0, 2] = 0.0 \n",
    "    \n",
    "    if mocap_file.endswith(\".bvh\") or mocap_file.endswith(\".BVH\"):\n",
    "        mocap_data[\"motion\"][\"rot_local\"] = mocap_tools.euler_to_quat_bvh(mocap_data[\"motion\"][\"rot_local_euler\"], mocap_data[\"rot_sequence\"])\n",
    "    elif mocap_file.endswith(\".fbx\") or mocap_file.endswith(\".FBX\"):\n",
    "        mocap_data[\"motion\"][\"rot_local\"] = mocap_tools.euler_to_quat(mocap_data[\"motion\"][\"rot_local_euler\"], mocap_data[\"rot_sequence\"])\n",
    "        \n",
    "    mocap_data[\"motion\"][\"pos_world\"], mocap_data[\"motion\"][\"rot_world\"] = mocap_tools.local_to_world(mocap_data[\"motion\"][\"rot_local\"], mocap_data[\"motion\"][\"pos_local\"], mocap_data[\"skeleton\"])\n",
    "\n",
    "    all_mocap_data.append(mocap_data)\n",
    "\n",
    "# retrieve mocap properties\n",
    "\n",
    "mocap_data = all_mocap_data[0]\n",
    "joint_count = mocap_data[\"motion\"][\"rot_local\"].shape[1]\n",
    "joint_dim = mocap_data[\"motion\"][\"rot_local\"].shape[2]\n",
    "pose_dim = joint_count * joint_dim\n",
    "\n",
    "offsets = mocap_data[\"skeleton\"][\"offsets\"].astype(np.float32)\n",
    "parents = mocap_data[\"skeleton\"][\"parents\"]\n",
    "children = mocap_data[\"skeleton\"][\"children\"]\n",
    "\n",
    "# retrieve joint weight percentages\n",
    "\n",
    "with open(mocap_config_path+ \"/\" + mocap_joint_weight_file) as fh:\n",
    "    mocap_joint_weight_percentages = json.load(fh)\n",
    "mocap_joint_weight_percentages = mocap_joint_weight_percentages[\"jointWeights\"]\n",
    "\n",
    "mocap_joint_weight_percentages = np.array(mocap_joint_weight_percentages)\n",
    "mocap_joint_weight_percentages_total = np.sum(mocap_joint_weight_percentages)\n",
    "joint_weights = mocap_joint_weight_percentages * mocap_body_weight / 100.0\n",
    "\n",
    "# create edge list\n",
    "def get_edge_list(children):\n",
    "    edge_list = []\n",
    "\n",
    "    for parent_joint_index in range(len(children)):\n",
    "        for child_joint_index in children[parent_joint_index]:\n",
    "            edge_list.append([parent_joint_index, child_joint_index])\n",
    "    \n",
    "    return edge_list\n",
    "\n",
    "edge_list = get_edge_list(children)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b990ecef-601a-487e-9ffb-d3f0ebec187a",
   "metadata": {},
   "source": [
    "## Gather Mocap Exerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3316727-6178-49e3-a26d-f96bdf6ab72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mocap_pos_excerpts = []\n",
    "mocap_rot_excerpts = []\n",
    "\n",
    "for mocap_index, mocap_data in enumerate(all_mocap_data):\n",
    "    mocap_rot = mocap_data[\"motion\"][\"rot_local\"]\n",
    "    mocap_pos = mocap_data[\"motion\"][\"pos_world\"] / 100.0 # meters\n",
    "    \n",
    "    frame_range_start = mocap_valid_frame_ranges[mocap_index][0]\n",
    "    frame_range_end = mocap_valid_frame_ranges[mocap_index][1]\n",
    "    \n",
    "    for seq_excerpt_start in np.arange(frame_range_start, frame_range_end - mocap_excerpt_length, mocap_excerpt_offset):\n",
    "        \n",
    "        #print(\"valid: start \", frame_range_start, \" end \", frame_range_end, \" exc: start \", seq_excerpt_start, \" end \", (seq_excerpt_start + mocap_excerpt_length) )\n",
    "        \n",
    "        mocap_pos_excerpt =  mocap_pos[seq_excerpt_start:seq_excerpt_start + mocap_excerpt_length]\n",
    "        mocap_pos_excerpts.append(mocap_pos_excerpt)\n",
    "        \n",
    "        mocap_rot_excerpt =  mocap_rot[seq_excerpt_start:seq_excerpt_start + mocap_excerpt_length]\n",
    "        mocap_rot_excerpts.append(mocap_rot_excerpt)\n",
    "        \n",
    "mocap_pos_excerpts = np.stack(mocap_pos_excerpts, axis=0)\n",
    "mocap_rot_excerpts = np.stack(mocap_rot_excerpts, axis=0)\n",
    "\n",
    "mocap_excerpt_count = mocap_pos_excerpts.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f915380-8d97-47fa-91ad-2b780a125b8f",
   "metadata": {},
   "source": [
    "## Compute Motion Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28966d9-ff93-4c39-baee-db5fc6ef8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_features = {}\n",
    "\n",
    "motion_features[\"rot_local\"] = mocap_rot_excerpts\n",
    "motion_features[\"pos_world_m\"] = mocap_pos_excerpts\n",
    "\n",
    "# pos world smooth\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    pos_world = motion_features[\"pos_world_m\"][mocap_excerpt_index]\n",
    "    pos_world_smooth = ma.smooth(pos_world, mocap_smooth_length)\n",
    "    features.append(pos_world_smooth)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"pos_world_smooth\"] = features\n",
    "\n",
    "#pos_scalar\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    pos_world_smooth = motion_features[\"pos_world_smooth\"][mocap_excerpt_index]\n",
    "    pos_scalar = ma.scalar(pos_world_smooth, \"norm\")\n",
    "    features.append(pos_scalar)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"pos_scalar\"] = features\n",
    "\n",
    "#vel_world\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    pos_world_smooth = motion_features[\"pos_world_smooth\"][mocap_excerpt_index]\n",
    "    vel_world = ma.derivative(pos_world_smooth, 1.0 / mocap_fps)\n",
    "    features.append(vel_world)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"vel_world\"] = features\n",
    "\n",
    "#vel_world_smooth\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    vel_world = motion_features[\"vel_world\"][mocap_excerpt_index]\n",
    "    vel_world_smooth = ma.smooth(vel_world, mocap_smooth_length)\n",
    "    features.append(vel_world_smooth)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"vel_world_smooth\"] = features\n",
    "    \n",
    "#vel_world_scalar\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    vel_world_smooth = motion_features[\"vel_world_smooth\"][mocap_excerpt_index]\n",
    "    vel_world_scalar = ma.scalar(vel_world_smooth, \"norm\")\n",
    "    features.append(vel_world_scalar)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"vel_world_scalar\"] = features\n",
    "\n",
    "#accel_world\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    vel_world_smooth = motion_features[\"vel_world_smooth\"][mocap_excerpt_index]\n",
    "    accel_world = ma.derivative(vel_world_smooth, 1.0 / mocap_fps)\n",
    "    features.append(accel_world)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"accel_world\"] = features\n",
    "\n",
    "#accel_world_smooth\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    accel_world = motion_features[\"accel_world\"][mocap_excerpt_index]\n",
    "    accel_world_smooth = ma.smooth(accel_world, mocap_smooth_length)\n",
    "    features.append(accel_world_smooth)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"accel_world_smooth\"] = features\n",
    "\n",
    "#accel_world_scalar\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    accel_world_smooth = motion_features[\"accel_world_smooth\"][mocap_excerpt_index]\n",
    "    accel_world_scalar = ma.scalar(accel_world_smooth, \"norm\")\n",
    "    features.append(accel_world_scalar)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"accel_world_scalar\"] = features\n",
    "\n",
    "#jerk_world\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    accel_world_smooth = motion_features[\"accel_world_smooth\"][mocap_excerpt_index]\n",
    "    jerk_world = ma.derivative(accel_world_smooth, 1.0 / mocap_fps)\n",
    "    features.append(jerk_world)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"jerk_world\"] = features\n",
    "\n",
    "#jerk_world_smooth\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    jerk_world = motion_features[\"jerk_world\"][mocap_excerpt_index]\n",
    "    jerk_world_smooth = ma.smooth(jerk_world, mocap_smooth_length)\n",
    "    features.append(jerk_world_smooth)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"jerk_world_smooth\"] = features\n",
    "\n",
    "#jerk_world_scalar\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    jerk_world_smooth = motion_features[\"jerk_world_smooth\"][mocap_excerpt_index]\n",
    "    jerk_world_scalar = ma.scalar(jerk_world_smooth, \"norm\")\n",
    "    features.append(jerk_world_scalar)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"jerk_world_scalar\"] = features\n",
    "\n",
    "#qom\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    vel_world_scalar = motion_features[\"vel_world_scalar\"][mocap_excerpt_index]\n",
    "    qom = ma.quantity_of_motion(vel_world_scalar, joint_weights)\n",
    "    features.append(qom)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"qom\"] = features\n",
    "\n",
    "#bbox\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    pos_world = motion_features[\"pos_world_m\"][mocap_excerpt_index]\n",
    "    bbox = ma.bounding_box(pos_world)\n",
    "    features.append(bbox)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"bbox\"] = features\n",
    "\n",
    "#bsphere\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    pos_world = motion_features[\"pos_world_m\"][mocap_excerpt_index]\n",
    "    bsphere = ma.bounding_sphere(pos_world)\n",
    "    features.append(bsphere)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"bsphere\"] = features\n",
    "\n",
    "#weight_effort\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    vel_world_scalar = motion_features[\"vel_world_scalar\"][mocap_excerpt_index]\n",
    "    weight_effort = ma.weight_effort(vel_world_scalar, joint_weights, mocap_smooth_length)\n",
    "    features.append(weight_effort)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"weight_effort\"] = features\n",
    "\n",
    "#space_effort\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    pos_world = motion_features[\"pos_world_m\"][mocap_excerpt_index]\n",
    "    space_effort = ma.space_effort_v2(pos_world, joint_weights, mocap_smooth_length)\n",
    "    features.append(space_effort)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"space_effort\"] = features\n",
    "\n",
    "#time_effort\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    accel_world_scalar = motion_features[\"accel_world_scalar\"][mocap_excerpt_index]\n",
    "    time_effort = ma.time_effort(accel_world_scalar, joint_weights, mocap_smooth_length)\n",
    "    features.append(time_effort)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"time_effort\"] = features\n",
    "\n",
    "#flow_effort\n",
    "features = []\n",
    "for mocap_excerpt_index in range(mocap_excerpt_count):\n",
    "    jerk_world_scalar = motion_features[\"jerk_world_scalar\"][mocap_excerpt_index]\n",
    "    flow_effort = ma.flow_effort(jerk_world_scalar, joint_weights, mocap_smooth_length)\n",
    "    features.append(flow_effort)\n",
    "features = np.stack(features, axis=0)\n",
    "motion_features[\"flow_effort\"] = features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a74736-6470-4a53-bdc1-ddadab3b78ad",
   "metadata": {},
   "source": [
    "## Normalise Motion Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8b9c8-efa2-40c2-818b-7e5b37226a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for motion_feature_name in list(motion_features.keys()):\n",
    "    \n",
    "    #print(motion_feature_name)\n",
    "    \n",
    "    motion_feature = motion_features[motion_feature_name]\n",
    "    \n",
    "    #print(\"motion_feature s \", motion_feature.shape)\n",
    "    \n",
    "    if motion_feature_average == True:\n",
    "        motion_feature = np.mean(motion_feature, axis=1, keepdims=True)\n",
    "        #print(\"motion_feature avg s \", motion_feature.shape)\n",
    "    \n",
    "    shape_orig = motion_feature.shape\n",
    "    \n",
    "    motion_feature = motion_feature.reshape(shape_orig[0] * shape_orig[1], -1)\n",
    "    \n",
    "    #print(\"motion_feature 2 s \", motion_feature.shape)\n",
    "    \n",
    "    motion_feature_mean = np.mean(motion_feature, axis=0, keepdims=True)\n",
    "    motion_feature_std = np.std(motion_feature, axis=0, keepdims=True)\n",
    "    \n",
    "    #print(\"motion_feature_mean s \", motion_feature_mean.shape)\n",
    "    #print(\"motion_feature_std s \", motion_feature_std.shape)\n",
    "    \n",
    "    motion_feature_norm = (motion_feature - motion_feature_mean) / motion_feature_std\n",
    "    \n",
    "    #print(\"motion_feature_norm s \", motion_feature_norm.shape)\n",
    "    \n",
    "    motion_feature_norm = motion_feature_norm.reshape(shape_orig[0], -1)\n",
    "    \n",
    "    #print(\"motion_feature_norm 2 s \", motion_feature_norm.shape)\n",
    "    \n",
    "    motion_features[motion_feature_name + \"_norm\"] = motion_feature_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd736eb2-bb13-406f-95d6-014256135102",
   "metadata": {},
   "source": [
    "## Prepare Motion Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73460906-2425-409f-861d-ad82dfe16208",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_excerpt_overlap = mocap_excerpt_length - mocap_excerpt_offset\n",
    "rot_envelope_part1 = np.linspace(0.0, 1.0, motion_excerpt_overlap)\n",
    "rot_envelope_part2 = np.ones([mocap_excerpt_offset])\n",
    "rot_envelope = np.concatenate([rot_envelope_part1, rot_envelope_part2], axis=0)\n",
    "\n",
    "def motion_blend(target_motion, motion_excerpt, start_pose_index):\n",
    "\n",
    "    #print(\"motion_excerpt s \", motion_excerpt.shape, \" target_motion s \", target_motion.shape, )\n",
    "\n",
    "    for epI in range(mocap_excerpt_length):\n",
    "        tpI = epI + start_pose_index\n",
    "        slerp_value = rot_envelope[epI]\n",
    "        \n",
    "        #print(\"epI \", epI, \" tpI \", tpI, \" slerp \", slerp_value)\n",
    "        \n",
    "        for jI in range(joint_count):\n",
    "            \n",
    "            quat1 = target_motion[tpI, jI]\n",
    "            quat2 = motion_excerpt[epI , jI]\n",
    "            quat_blend = slerp(quat1, quat2, slerp_value)\n",
    "            \n",
    "            \"\"\"\n",
    "            if jI == 1:\n",
    "                print(\"jI \", jI, \" q1 \", quat1, \" q2 \", quat2, \" qb \", quat_blend)\n",
    "            \"\"\"\n",
    "            \n",
    "            target_motion[tpI, jI] = quat_blend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b2d57-3ee1-463a-b6aa-a222b90fe612",
   "metadata": {},
   "source": [
    "## Find Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef8eeb-52c6-4131-8510-bc7693e194f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all motion excerpts and concatenate motion features\n",
    "motion_features_proc = []\n",
    "for motion_feature_name in motion_feature_names:\n",
    "    motion_norm_feature_name = motion_feature_name + \"_norm\"\n",
    "    motion_feature = motion_features[motion_norm_feature_name]\n",
    "    \n",
    "    #print(\"name \", motion_norm_feature_name, \" shape \", motion_feature.shape)\n",
    "    \n",
    "    motion_features_proc.append(motion_feature)\n",
    "    \n",
    "motion_features_proc = np.concatenate(motion_features_proc, axis=1)\n",
    "motion_proc = np.copy(mocap_rot_excerpts)\n",
    "\n",
    "#motion_proc.shape\n",
    "\n",
    "# prepare empty motion to copy motions corresponding to nearest features into\n",
    "nn_element_count = motion_features_proc.shape[0]\n",
    "gen_motion_pose_count = 2 * mocap_excerpt_length + (nn_element_count - 1) * mocap_excerpt_offset\n",
    "gen_motion = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "gen_motion = np.reshape(gen_motion, [1, 1, 4])\n",
    "gen_motion = np.repeat(gen_motion, joint_count, axis=1)\n",
    "gen_motion = np.repeat(gen_motion, gen_motion_pose_count, axis=0)\n",
    "\n",
    "# select first motion feature to begin search with\n",
    "nn_current_index = 0\n",
    "nn_current_motion = motion_proc[nn_current_index]\n",
    "nn_current_feature = motion_features_proc[nn_current_index]\n",
    "nn_current_feature = np.expand_dims(nn_current_feature, 0)\n",
    "\n",
    "#print(\"gen_motion s \", gen_motion.shape)\n",
    "\n",
    "# add first motion excerpt to gen motion\n",
    "motion_blend(gen_motion, nn_current_motion, 0)\n",
    "\n",
    "# iterate through all neighbors\n",
    "pI = mocap_excerpt_offset\n",
    "\n",
    "remaining_neighbor_gui = widgets.Label(value=str(nn_element_count))\n",
    "\n",
    "display(remaining_neighbor_gui)\n",
    "\n",
    "while nn_element_count > 0:\n",
    "    \n",
    "    remaining_neighbor_gui.value = str(nn_element_count)\n",
    "    \n",
    "    # search nearest element\n",
    "    nn_distances = np.linalg.norm(motion_features_proc - nn_current_feature, axis=1)\n",
    "    k = 2\n",
    "    nn_indices = nn_distances.argsort()[:k]\n",
    "\n",
    "    # replace current element with nearest element\n",
    "    nn_previous_index = nn_current_index\n",
    "    nn_current_index = nn_indices[1]\n",
    "    nn_current_motion = motion_proc[nn_current_index]\n",
    "    nn_current_feature = motion_features_proc[nn_current_index]\n",
    "    nn_current_feature = np.expand_dims(nn_current_feature, 0)\n",
    "    \n",
    "    # blend motion corresponding to current element into gen motion\n",
    "    motion_blend(gen_motion, nn_current_motion, pI)\n",
    "    \n",
    "    # remove previous element\n",
    "    if nn_previous_index == 0:\n",
    "        motion_proc = np.copy(motion_proc[nn_previous_index + 1:])\n",
    "        motion_features_proc = np.copy(motion_features_proc[nn_previous_index + 1:])\n",
    "    elif nn_previous_index == motion_proc.shape[0] - 1:\n",
    "        motion_proc = np.copy(motion_proc[:nn_previous_index])\n",
    "        motion_features_proc = np.copy(motion_features_proc[:nn_previous_index])\n",
    "    else:\n",
    "        motion_proc = np.copy(np.concatenate([motion_proc[:nn_previous_index], motion_proc[nn_previous_index + 1:]], axis=0))\n",
    "        motion_features_proc = np.copy(np.concatenate([motion_features_proc[:nn_previous_index], motion_features_proc[nn_previous_index + 1:]], axis=0))\n",
    "\n",
    "    nn_element_count -= 1\n",
    "    pI += mocap_excerpt_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f9a28-2a8a-4344-b964-5388c0cec275",
   "metadata": {},
   "source": [
    "## Rendering and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee188ef-18e0-40a9-a6de-38881c60054d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poseRenderer = PoseRenderer(edge_list)\n",
    "\n",
    "def forward_kinematics(rotations, root_positions):\n",
    "    \"\"\"\n",
    "    Perform forward kinematics using the given trajectory and local rotations.\n",
    "    Arguments (where N = batch size, L = sequence length, J = number of joints):\n",
    "     -- rotations: (N, L, J, 4) tensor of unit quaternions describing the local rotations of each joint.\n",
    "     -- root_positions: (N, L, 3) tensor describing the root joint positions.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(rotations.shape) == 4\n",
    "    assert rotations.shape[-1] == 4\n",
    "    \n",
    "    toffsets = torch.tensor(offsets)\n",
    "    \n",
    "    positions_world = []\n",
    "    rotations_world = []\n",
    "\n",
    "    expanded_offsets = toffsets.expand(rotations.shape[0], rotations.shape[1], offsets.shape[0], offsets.shape[1])\n",
    "\n",
    "    # Parallelize along the batch and time dimensions\n",
    "    for jI in range(offsets.shape[0]):\n",
    "        if parents[jI] == -1:\n",
    "            positions_world.append(root_positions)\n",
    "            rotations_world.append(rotations[:, :, 0])\n",
    "        else:\n",
    "            positions_world.append(qrot(rotations_world[parents[jI]], expanded_offsets[:, :, jI]) \\\n",
    "                                   + positions_world[parents[jI]])\n",
    "            if len(children[jI]) > 0:\n",
    "                rotations_world.append(qmul(rotations_world[parents[jI]], rotations[:, :, jI]))\n",
    "            else:\n",
    "                # This joint is a terminal node -> it would be useless to compute the transformation\n",
    "                rotations_world.append(None)\n",
    "\n",
    "    return torch.stack(positions_world, dim=3).permute(0, 1, 3, 2)\n",
    "\n",
    "def export_sequence_anim(pose_sequence, file_name):\n",
    "    \n",
    "    pose_count = pose_sequence.shape[0]\n",
    "    pose_sequence = np.reshape(pose_sequence, (pose_count, joint_count, joint_dim))\n",
    "    \n",
    "    \n",
    "    pose_sequence = torch.tensor(np.expand_dims(pose_sequence, axis=0)).to(torch.float32)\n",
    "    zero_trajectory = torch.tensor(np.zeros((1, pose_count, 3))).to(torch.float32)\n",
    "    \n",
    "    skel_sequence = forward_kinematics(pose_sequence, zero_trajectory)\n",
    "    \n",
    "    skel_sequence = skel_sequence.detach().cpu().numpy()\n",
    "    skel_sequence = np.squeeze(skel_sequence)    \n",
    "    \n",
    "    view_min, view_max = utils.get_equal_mix_max_positions(skel_sequence)\n",
    "    skel_images = poseRenderer.create_pose_images(skel_sequence, view_min, view_max, view_ele, view_azi, view_line_width, view_size, view_size)\n",
    "    skel_images[0].save(file_name, save_all=True, append_images=skel_images[1:], optimize=False, duration=33.0, loop=0)\n",
    "\n",
    "def export_sequence_bvh(pose_sequence, file_name):\n",
    "    \n",
    "    pose_count = pose_sequence.shape[0]\n",
    "\n",
    "    pred_dataset = {}\n",
    "    pred_dataset[\"frame_rate\"] = mocap_data[\"frame_rate\"]\n",
    "    pred_dataset[\"rot_sequence\"] = mocap_data[\"rot_sequence\"]\n",
    "    pred_dataset[\"skeleton\"] = mocap_data[\"skeleton\"]\n",
    "    pred_dataset[\"motion\"] = {}\n",
    "    pred_dataset[\"motion\"][\"pos_local\"] = np.repeat(np.expand_dims(pred_dataset[\"skeleton\"][\"offsets\"], axis=0), pose_count, axis=0)\n",
    "    pred_dataset[\"motion\"][\"rot_local\"] = pose_sequence\n",
    "    pred_dataset[\"motion\"][\"rot_local_euler\"] = mocap_tools.quat_to_euler_bvh(pred_dataset[\"motion\"][\"rot_local\"], pred_dataset[\"rot_sequence\"])\n",
    "\n",
    "    pred_bvh = mocap_tools.mocap_to_bvh(pred_dataset)\n",
    "    \n",
    "    bvh_tools.write(pred_bvh, file_name)\n",
    "\n",
    "def export_sequence_fbx(pose_sequence, file_name):\n",
    "    \n",
    "    pose_count = pose_sequence.shape[0]\n",
    "    \n",
    "    pred_dataset = {}\n",
    "    pred_dataset[\"frame_rate\"] = mocap_data[\"frame_rate\"]\n",
    "    pred_dataset[\"rot_sequence\"] = mocap_data[\"rot_sequence\"]\n",
    "    pred_dataset[\"skeleton\"] = mocap_data[\"skeleton\"]\n",
    "    pred_dataset[\"motion\"] = {}\n",
    "    pred_dataset[\"motion\"][\"pos_local\"] = np.repeat(np.expand_dims(pred_dataset[\"skeleton\"][\"offsets\"], axis=0), pose_count, axis=0)\n",
    "    pred_dataset[\"motion\"][\"rot_local\"] = pose_sequence\n",
    "    pred_dataset[\"motion\"][\"rot_local_euler\"] = mocap_tools.quat_to_euler(pred_dataset[\"motion\"][\"rot_local\"], pred_dataset[\"rot_sequence\"])\n",
    "    \n",
    "    pred_fbx = mocap_tools.mocap_to_fbx([pred_dataset])\n",
    "    \n",
    "    fbx_tools.write(pred_fbx, file_name)\n",
    "\n",
    "# export gen motion\n",
    "export_sequence_anim(gen_motion, \"results/nearest_neighbors.gif\")\n",
    "export_sequence_bvh(gen_motion,  \"results/nearest_neighbors.bvh\")\n",
    "export_sequence_fbx(gen_motion,  \"results/nearest_neighbors.fbx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
