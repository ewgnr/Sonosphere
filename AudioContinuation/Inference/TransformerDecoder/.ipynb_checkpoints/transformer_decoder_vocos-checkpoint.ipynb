{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d1e903-1f33-4a46-8b69-b9c329995839",
   "metadata": {},
   "source": [
    "# Audio Continuation Transformer Decoder (Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070177bd-9324-4974-b01d-affc473eb63c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff77a26-c7cf-4730-b8c4-11f758e225cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "from collections import OrderedDict\n",
    "\n",
    "from vocos import Vocos\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764f852-1753-450c-bc75-a343f58beb71",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e31a9b-6c4b-4916-a2ec-6053587da1c9",
   "metadata": {},
   "source": [
    "### Compute Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5725ca1-a556-46fc-9928-5be8c9354abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b74bf9-4a6b-4fbb-8642-d3cc209489cd",
   "metadata": {},
   "source": [
    "### Audio Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17f04a-b41b-414e-a6ca-48797ec2836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_sample_rate = 48000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fbddd9-1dc8-43ae-9c23-9bcc953917a0",
   "metadata": {},
   "source": [
    "### Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec80cea-9743-4db4-8725-1fbe773e03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer_count = 6\n",
    "decoder_head_count = 8\n",
    "decoder_embed_dim = 512\n",
    "decoder_ff_dim = 2048\n",
    "decoder_dropout = 0.1\n",
    "\n",
    "load_weights = True\n",
    "decoder_weights_file = \"../../../../Data/Models/AudioContinuation/TransformerDecoder/Gutenberg/weights/decoder_weights_epoch_200\"\n",
    "\n",
    "decoder_layer_count_gui = widgets.IntText(value=decoder_layer_count, description=\"Decoder Layer Count:\", style={'description_width': 'initial'})\n",
    "decoder_head_count_gui = widgets.IntText(value=decoder_head_count, description=\"Decoder Head Count:\", style={'description_width': 'initial'})\n",
    "decoder_embed_dim_gui = widgets.IntText(value=decoder_embed_dim, description=\"Decoder Embed Dim:\", style={'description_width': 'initial'})\n",
    "decoder_ff_dim_gui = widgets.IntText(value=decoder_ff_dim, description=\"Decoder Fordward Dim:\", style={'description_width': 'initial'})\n",
    "decoder_dropout_gui = widgets.FloatText(value=decoder_dropout, description=\"Decoder Dropout:\", style={'description_width': 'initial'})\n",
    "\n",
    "decoder_weights_file_gui = widgets.Text(value=decoder_weights_file, description=\"Decoder Weights File:\", style={'description_width': 'initial'}) \n",
    "\n",
    "display(decoder_layer_count_gui)\n",
    "display(decoder_head_count_gui)\n",
    "display(decoder_embed_dim_gui)\n",
    "display(decoder_ff_dim_gui)\n",
    "display(decoder_dropout_gui)\n",
    "display(decoder_weights_file_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb93c5-93b0-4f80-b737-86bc67795e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer_count = decoder_layer_count_gui.value\n",
    "decoder_head_count = decoder_head_count_gui.value\n",
    "decoder_embed_dim = decoder_embed_dim_gui.value\n",
    "decoder_ff_dim = decoder_ff_dim_gui.value\n",
    "decoder_dropout = decoder_dropout_gui.value\n",
    "decoder_weights_file = decoder_weights_file_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855005b-9506-4dea-9e5b-f61f20158ba3",
   "metadata": {},
   "source": [
    "### Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314bfc0-11e3-4c3d-a76f-ee17452e0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_input_length = 64\n",
    "\n",
    "seq_input_length_gui = widgets.IntText(value=seq_input_length, description=\"Sequence Input Length:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(seq_input_length_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47374b-cf97-4732-8d98-4cb879e5b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_input_length = seq_input_length_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b89e4-45a1-4afc-b067-ffa8951b4d22",
   "metadata": {},
   "source": [
    "## Load Vocos Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5eaea-0f0f-48e1-af71-b2e093727a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocos = Vocos.from_pretrained(\"kittn/vocos-mel-48khz-alpha1\")\n",
    "\n",
    "dummy_waveform = torch.zeros((1, audio_sample_rate), dtype=torch.float32).to(device)\n",
    "dummy_mels = vocos.feature_extractor(dummy_waveform)\n",
    "\n",
    "audio_features_dim = dummy_mels.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ad599-1d2f-491a-8a35-8e8e1cc03c63",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317eb5b-67d7-4cd5-b17c-e8a7e829cf9d",
   "metadata": {},
   "source": [
    "## PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ed00e-f836-447e-b610-eb8bfddc29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bc645-f986-4f8e-ab4a-7782ba6eb753",
   "metadata": {},
   "source": [
    "## Create TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49439334-3e51-4e5e-944d-2387cb4ef9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        audio_dim,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        num_decoder_layers,\n",
    "        ff_dim,\n",
    "        dropout_p,\n",
    "        pos_encoding_max_length\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.audio2embed = nn.Linear(audio_dim, embed_dim) # map audio data to embedding\n",
    "\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=embed_dim, dropout_p=dropout_p, max_len=pos_encoding_max_length\n",
    "        )\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout_p, batch_first=True)\n",
    "        #self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers = num_decoder_layers)\n",
    "\n",
    "        # build a decoder directly from TransformerDecoderLayer\n",
    "        # rather than using the nn.TransformerDecoder module which requires also a Transformer Encoder\n",
    "        self.decoder = self.layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=embed_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=ff_dim,\n",
    "                dropout=dropout_p,\n",
    "                activation='gelu',\n",
    "                batch_first=True\n",
    "            ) for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "        self.embed2audio = nn.Linear(embed_dim, audio_dim) # map embedding to audio data\n",
    "\n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "       \n",
    "    def forward(self, audio_data):\n",
    "        \n",
    "        #print(\"forward\")\n",
    "        \n",
    "        #print(\"audio_data s \", audio_data.shape)\n",
    "        \n",
    "        # dummy \"memory\" as zero (only self-attention is used)\n",
    "        memory = torch.zeros(audio_data.size(0), audio_data.size(1), self.embed_dim, device=audio_data.device)\n",
    "\n",
    "        #print(\"memory s \", memory.shape)\n",
    "\n",
    "        # Lower triangular matrix for autoregressive masking\n",
    "        tgt_mask = self.get_tgt_mask(audio_data.shape[1]).to(audio_data.device)\n",
    "\n",
    "        #print(\"tgt_mask s \", tgt_mask.shape)\n",
    "\n",
    "        audio_embedded = self.audio2embed(audio_data) * math.sqrt(self.embed_dim)\n",
    "        \n",
    "        #print(\"audio_embedded 1 s \", audio_embedded.shape)\n",
    "        \n",
    "        audio_embedded = self.positional_encoder(audio_embedded)\n",
    "        \n",
    "        #print(\"audio_embedded 2 s \", audio_embedded.shape)\n",
    "        \n",
    "        x = audio_embedded\n",
    "        \n",
    "        #print(\"x s \", x.shape)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            #print(\"x in s \", x.shape)\n",
    "            \n",
    "            x = layer(x, memory, tgt_mask=tgt_mask)\n",
    "            \n",
    "            #print(\"x out s \", x.shape)\n",
    "\n",
    "        decoder_out = x\n",
    "\n",
    "        out = self.embed2audio(decoder_out)\n",
    "        \n",
    "        out = out[:, -1, :] # only last time step \n",
    "        \n",
    "        return out\n",
    "\n",
    "decoder = TransformerDecoder(audio_dim=audio_features_dim,\n",
    "                          embed_dim=decoder_embed_dim, \n",
    "                          num_heads=decoder_head_count, \n",
    "                          num_decoder_layers=decoder_layer_count, \n",
    "                          ff_dim = decoder_ff_dim,\n",
    "                          dropout_p=decoder_dropout,\n",
    "                          pos_encoding_max_length=seq_input_length).to(device)\n",
    "\n",
    "print(decoder)\n",
    "\n",
    "if load_weights == True:\n",
    "    if device == 'cuda':\n",
    "        decoder.load_state_dict(torch.load(decoder_weights_file))\n",
    "    else:\n",
    "        decoder.load_state_dict(torch.load(decoder_weights_file, map_location=device ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd12001-f0b2-4e05-8851-563fb26c2ee8",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b37fa1-acb1-4608-b0cf-feef6e411d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()\n",
    "\n",
    "def export_orig_audio(waveform_data, start_time, end_time, file_name):\n",
    "    \n",
    "    start_time_samples = int(start_time * audio_sample_rate)\n",
    "    end_time_samples = int(end_time * audio_sample_rate)\n",
    "    \n",
    "    torchaudio.save(file_name, waveform_data[:, start_time_samples:end_time_samples], audio_sample_rate)\n",
    "\n",
    "def export_ref_audio(waveform_data, start_time, end_time, file_name):\n",
    "    \n",
    "    start_time_samples = int(start_time * audio_sample_rate)\n",
    "    end_time_samples = int(end_time * audio_sample_rate)\n",
    "    \n",
    "    # audio features\n",
    "    audio_features = vocos.feature_extractor(waveform_data[:, start_time_samples:end_time_samples])\n",
    "    \n",
    "    ref_audio = vocos.decode(audio_features)\n",
    "    \n",
    "    torchaudio.save(file_name, ref_audio.detach().cpu(), audio_sample_rate)\n",
    "    \n",
    "\n",
    "def export_pred_audio(waveform_data, start_time, end_time, file_name):\n",
    "    \n",
    "    start_time_samples = int(start_time * audio_sample_rate)\n",
    "    end_time_samples = int(end_time * audio_sample_rate)\n",
    "    \n",
    "    # audio features\n",
    "    audio_features = vocos.feature_extractor(waveform_data[:, start_time_samples:end_time_samples])\n",
    "    \n",
    "    #print(\"audio_features s \", audio_features.shape)\n",
    "    \n",
    "    audio_features = audio_features.squeeze(0)\n",
    "    audio_features = torch.permute(audio_features, (1, 0))\n",
    "    audio_feature_count = audio_features.shape[0]\n",
    "    \n",
    "    #print(\"audio_feature_count \", audio_feature_count)\n",
    "    \n",
    "    input_features = audio_features[:seq_input_length]\n",
    "    input_features = input_features.unsqueeze(0)\n",
    "    \n",
    "    output_features_length = audio_feature_count - seq_input_length\n",
    "    \n",
    "    #print(\"output_features_length \", output_features_length)\n",
    "    \n",
    "    _input_features = input_features  \n",
    "    pred_features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for o_i in range(1, output_features_length):\n",
    "            \n",
    "            _input_features = _input_features.to(device)\n",
    "            \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "            \n",
    "            _pred_features = decoder(_input_features)\n",
    "            _pred_features = torch.unsqueeze(_pred_features, axis=1)\n",
    "\n",
    "            _input_features = _input_features[:, 1:, :].detach().clone()\n",
    "            _pred_features = _pred_features.detach().clone()\n",
    "            \n",
    "            pred_features.append(_pred_features.cpu())\n",
    "            \n",
    "            _input_features = torch.cat((_input_features, _pred_features), axis=1)\n",
    "                \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "            \n",
    "    pred_features = torch.cat(pred_features, axis=1)\n",
    "    pred_features = torch.permute(pred_features, (0, 2, 1))\n",
    "    pred_audio = vocos.decode(pred_features)\n",
    "    \n",
    "    torchaudio.save(file_name, pred_audio.detach().cpu(), audio_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1858b39-d518-49b2-91a9-b90e0ce95dfb",
   "metadata": {},
   "source": [
    "### Perform Audio Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c4a81-94cd-45e8-b9dc-71fbbf382921",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"../../../../Data/Audio/Gutenberg/Night_and_Day_by_Virginia_Woolf_48khz.wav\"\n",
    "audio_start_time_sec = 60.0\n",
    "audio_end_time_sec = 70.0\n",
    "\n",
    "audio_file_gui = widgets.Text(value=audio_file, description=\"Audio File:\", style={'description_width': 'initial'})\n",
    "audio_start_time_sec_gui = widgets.FloatText(value=audio_start_time_sec, description=\"Audio Start Time [Seconds]:\", style={'description_width': 'initial'})\n",
    "audio_end_time_sec_gui = widgets.FloatText(value=audio_end_time_sec, description=\"Audio End Time [Seconds]\", style={'description_width': 'initial'})\n",
    "\n",
    "display(audio_file_gui)\n",
    "display(audio_start_time_sec_gui)\n",
    "display(audio_end_time_sec_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2bec8-5902-464c-8269-76e86c1e7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = audio_file_gui.value\n",
    "audio_start_time_sec = audio_start_time_sec_gui.value\n",
    "audio_end_time_sec = audio_end_time_sec_gui.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab301dd6-85cc-4fd8-b184-f7f11ef0ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_data, _ = torchaudio.load(audio_file)\n",
    "\n",
    "export_orig_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/orig_{}-{}.wav\".format(audio_start_time_sec, audio_end_time_sec))\n",
    "export_ref_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/ref_{}-{}.wav\".format(audio_start_time_sec, audio_end_time_sec))\n",
    "export_pred_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/pred_{}-{}.wav\".format(audio_start_time_sec, audio_end_time_sec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
