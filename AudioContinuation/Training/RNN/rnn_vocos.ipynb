{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aafb3e65-3228-4383-a1e9-6fd01c006c38",
   "metadata": {},
   "source": [
    "# Motion Continuation LSTM (Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a8445f-c23e-4cee-ad61-fdb6777a9cf4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4830874a-d591-4f9f-9c41-a4e26eb967b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "from collections import OrderedDict\n",
    "\n",
    "from vocos import Vocos\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4673fd7-4173-4c89-a701-4498ecbdf54e",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb3f079-4136-4e5d-b1fd-7b4a67738ce6",
   "metadata": {},
   "source": [
    "### Compute Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df04a9-b709-4338-be6e-dfe2d3c95716",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88779f7e-88ed-426c-800b-1f34a8afbd84",
   "metadata": {},
   "source": [
    "### Audio Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b833f1e-94bd-4b4c-9480-09b100781862",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = \"../../../../Data/Audio/Gutenberg/\"\n",
    "audio_files = [\"Night_and_Day_by_Virginia_Woolf_48khz.wav\"]\n",
    "audio_file_excerpts = [ [ 14.0, 314.0 ] ]\n",
    "audio_sample_rate = 48000\n",
    "\n",
    "audio_files_all = [f for f in os.listdir(audio_file_path) if os.path.isfile(os.path.join(audio_file_path, f))]\n",
    "\n",
    "audio_file_excerpts_gui = widgets.Text(str(audio_file_excerpts), description='Audio File Excerpts:', style={'description_width': 'initial'})\n",
    "\n",
    "audio_files_gui = widgets.SelectMultiple(\n",
    "    options=audio_files_all,\n",
    "    value=audio_files,  # default: first option selected; can be empty\n",
    "    description='Audio Files:',\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(audio_files_gui)\n",
    "display(audio_file_excerpts_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed7dc2-8964-4aaf-80be-60df98546fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = list(audio_files_gui.value)\n",
    "\n",
    "matches = re.findall(r\"\\[\\s*([-+]?\\d*\\.?\\d+)\\s*,\\s*([-+]?\\d*\\.?\\d+)\\s*\\]\", audio_file_excerpts_gui.value)\n",
    "audio_file_excerpts = [[float(a), float(b)] for a, b in matches]\n",
    "audio_file_excerpts = [[int(a * audio_sample_rate), int(b * audio_sample_rate)] for a, b in audio_file_excerpts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b25c5b-8c76-4645-a268-9783652be153",
   "metadata": {},
   "source": [
    "### Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7a52a-cb42-4e1f-b4b4-53398ef906a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer_dim = 512\n",
    "rnn_layer_count = 2\n",
    "\n",
    "save_weights = True\n",
    "load_weights = False\n",
    "rnn_weights_file = \"results/weights/rnn_weights_epoch_200\"\n",
    "\n",
    "rnn_layer_count_gui = widgets.IntText(value=rnn_layer_count, description=\"LSTM Layer Count:\", style={'description_width': 'initial'})\n",
    "rnn_layer_dim_gui = widgets.IntText(value=rnn_layer_dim, description=\"LSTM Layer Dim:\", style={'description_width': 'initial'})\n",
    "\n",
    "save_weights_gui = widgets.Checkbox(\n",
    "    value=save_weights,\n",
    "    description='Save Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "load_weights_gui = widgets.Checkbox(\n",
    "    value=load_weights,\n",
    "    description='Load Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "rnn_weights_file_gui = widgets.Text(value=rnn_weights_file, description=\"RNN Weights File:\", style={'description_width': 'initial'}) \n",
    "\n",
    "display(rnn_layer_count_gui)\n",
    "display(rnn_layer_dim_gui)\n",
    "display(save_weights_gui)\n",
    "display(load_weights_gui)\n",
    "display(rnn_weights_file_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85cedf5-9dbb-4323-8bfb-560b78dce2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer_count = rnn_layer_count_gui.value\n",
    "rnn_layer_dim = rnn_layer_dim_gui.value\n",
    "save_weights = save_weights_gui.value\n",
    "load_weights = load_weights_gui.value\n",
    "rnn_weights_file = rnn_weights_file_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f98119-462b-4425-b022-b14942f3d9b4",
   "metadata": {},
   "source": [
    "### Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d472f-e050-4605-8c23-0c042f5be296",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "test_percentage = 0.1\n",
    "\n",
    "seq_input_length = 64\n",
    "seq_output_length = 10 # this is only used for non-teacher forcing scenarios\n",
    "\n",
    "learning_rate = 1e-4\n",
    "teacher_forcing_prob = 0.0\n",
    "model_save_interval = 10\n",
    "\n",
    "epochs = 200\n",
    "save_history = True\n",
    "\n",
    "batch_size_gui = widgets.IntText(value=batch_size, description=\"Batch Size:\", style={'description_width': 'initial'})\n",
    "test_percentage_gui = widgets.FloatText(value=test_percentage, description=\"Test Ratio:\", style={'description_width': 'initial'})\n",
    "seq_input_length_gui = widgets.IntText(value=seq_input_length, description=\"Sequence Input Length:\", style={'description_width': 'initial'})\n",
    "seq_output_length_gui = widgets.IntText(value=seq_output_length, description=\"Sequence Output Length:\", style={'description_width': 'initial'})\n",
    "learning_rate_gui = widgets.FloatText(value=learning_rate, description=\"Autoencoder Learning Rate:\", style={'description_width': 'initial'})\n",
    "teacher_forcing_prob_gui = widgets.FloatText(value=teacher_forcing_prob, description=\"Teacher Forcing Probability:\", style={'description_width': 'initial'})\n",
    "model_save_interval_gui = widgets.IntText(value=model_save_interval, description=\"Model Save Interval:\", style={'description_width': 'initial'})\n",
    "epochs_gui = widgets.IntText(value=epochs, description=\"Epochs:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(batch_size_gui)\n",
    "display(test_percentage_gui)\n",
    "display(seq_input_length_gui)\n",
    "display(seq_output_length_gui)\n",
    "display(learning_rate_gui)\n",
    "display(teacher_forcing_prob_gui)\n",
    "display(model_save_interval_gui)\n",
    "display(epochs_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce371bb-7f82-4ac1-8793-6e377e530809",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = batch_size_gui.value\n",
    "test_percentage = test_percentage_gui.value\n",
    "seq_input_length = seq_input_length_gui.value\n",
    "seq_output_length = seq_output_length_gui.value\n",
    "learning_rate = learning_rate_gui.value\n",
    "teacher_forcing_prob = teacher_forcing_prob_gui.value\n",
    "model_save_interval = model_save_interval_gui.value\n",
    "epochs = epochs_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0083d-9142-4e53-9221-7982496ca51b",
   "metadata": {},
   "source": [
    "## Load Audio Data and Calculate Mel Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a08a1-a472-42b6-9a65-1d1e1b714b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocos = Vocos.from_pretrained(\"kittn/vocos-mel-48khz-alpha1\")\n",
    "\n",
    "all_audio_features = []\n",
    "\n",
    "for audio_file_index in range(len(audio_files)):\n",
    "    \n",
    "    print(\"audio file \", audio_files[audio_file_index])\n",
    "    \n",
    "    audio_file = audio_files[audio_file_index]\n",
    "    audio_file_excerpt = audio_file_excerpts[audio_file_index]\n",
    "    \n",
    "    waveform_range_start = audio_file_excerpt[0]\n",
    "    waveform_range_end = audio_file_excerpt[1]\n",
    "    \n",
    "    # load audio file\n",
    "    waveform_data, _ = torchaudio.load(audio_file_path + audio_file)\n",
    "    \n",
    "    # audio excerpt\n",
    "    waveform_data = waveform_data[:, waveform_range_start:waveform_range_end]\n",
    "    \n",
    "    print(\"waveform_data s \", waveform_data.shape)\n",
    "    \n",
    "    # audio features\n",
    "    audio_features = vocos.feature_extractor(waveform_data)\n",
    "    \n",
    "    print(\"audio_features s \", audio_features.shape)\n",
    "    \n",
    "    audio_features = audio_features.squeeze(0)\n",
    "    audio_features = torch.permute(audio_features, (1, 0))\n",
    "    \n",
    "    print(\"audio_features 2 s \", audio_features.shape)\n",
    "    \n",
    "    all_audio_features.append(audio_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a516d8-b3b9-4b34-90bf-829bc55c7dbb",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73c0cc8-c397-4fff-b40c-65c298c9ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "audio_features_dim = all_audio_features[0].shape[-1]\n",
    "\n",
    "audio_features_dim\n",
    "\n",
    "for audio_features in all_audio_features:\n",
    "    \n",
    "    total_sequence_length = audio_features.shape[0]\n",
    "    \n",
    "    for pI in range(total_sequence_length - seq_input_length - seq_output_length - 1):\n",
    "        X_sample = audio_features[pI:pI+seq_input_length]\n",
    "        X.append(X_sample)\n",
    "        \n",
    "        y_sample = audio_features[pI+seq_input_length:pI+seq_input_length+seq_output_length]\n",
    "        y.append(y_sample)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, ...], self.y[idx, ...]\n",
    "\n",
    "\n",
    "full_dataset = SequenceDataset(X, y)\n",
    "\n",
    "X_item, y_item = full_dataset[0]\n",
    "\n",
    "print(\"X_item s \", X_item.shape)\n",
    "print(\"y_item s \", y_item.shape)\n",
    "\n",
    "test_percentage = 0.1\n",
    "test_size = int(test_percentage * len(full_dataset))\n",
    "train_size = len(full_dataset) - test_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"X_batch s \", X_batch.shape)\n",
    "print(\"y_batch s \", y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9f4be-5e94-4910-8ed2-f4e1deebc863",
   "metadata": {},
   "source": [
    "## Create Recurrent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5aff6-655c-4be6-a99a-46768c3755f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reccurent(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layer_count):\n",
    "        super(Reccurent, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_count = layer_count\n",
    "        self.output_dim = output_dim\n",
    "            \n",
    "        rnn_layers = []\n",
    "        \n",
    "        rnn_layers.append((\"rnn\", nn.LSTM(self.input_dim, self.hidden_dim, self.layer_count, batch_first=True)))\n",
    "        self.rnn_layers = nn.Sequential(OrderedDict(rnn_layers))\n",
    "        \n",
    "        dense_layers = []\n",
    "        dense_layers.append((\"dense\", nn.Linear(self.hidden_dim, self.output_dim)))\n",
    "        self.dense_layers = nn.Sequential(OrderedDict(dense_layers))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, (_, _) = self.rnn_layers(x)\n",
    "        \n",
    "        x = x[:, -1, :] # only last time step \n",
    "        x = self.dense_layers(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "rnn = Reccurent(audio_features_dim, rnn_layer_dim, audio_features_dim, rnn_layer_count).to(device)\n",
    "print(rnn)\n",
    "\n",
    "# test Reccurent model\n",
    "\n",
    "batch_x, _ = next(iter(train_loader))\n",
    "rnn_in = batch_x.to(device)\n",
    "rnn_out = rnn(rnn_in)\n",
    "\n",
    "print(\"rnn_in s \", rnn_in.shape)\n",
    "print(\"rnn_out s \", rnn_out.shape)\n",
    "\n",
    "if load_weights == True:\n",
    "    rnn.load_state_dict(torch.load(rnn_weights_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf4570c-0f0d-427f-87f2-35a1ee820851",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c371be-75af-44ec-af63-0a298c884aff",
   "metadata": {},
   "source": [
    "### Create Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d5d8b-ea66-43c9-a472-6b370d9e0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.336) # reduce the learning every 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6454ffb-2739-4520-885b-69412ef712e1",
   "metadata": {},
   "source": [
    "### Create Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17065bd0-03f1-4364-bcb1-47d24c0ae8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_loss = nn.MSELoss()\n",
    "\n",
    "def loss(y, yhat):\n",
    "    _rec_loss = rec_loss(yhat, y)\n",
    "\n",
    "    return _rec_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b960f-31c6-426e-8c10-58a04f361dda",
   "metadata": {},
   "source": [
    "### Create Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7312f-7fa0-4338-9335-c783b940ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_features, target_features, teacher_forcing):\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    #print(\"ar_train_step\")    \n",
    "    #print(\"teacher_forcing \", teacher_forcing)\n",
    "    #print(\"pose_sequences s \", pose_sequences.shape)\n",
    "    #print(\"target_poses s \", target_poses.shape)\n",
    "\n",
    "    #_input_poses = pose_sequences.detach().clone()    \n",
    "    _input_features = input_features  \n",
    "    output_features_length = target_features.shape[1]\n",
    "    \n",
    "    #print(\"output_features_length \", output_features_length)\n",
    "    \n",
    "    _pred_features_for_loss = []\n",
    "    _target_features_for_loss = []\n",
    "    \n",
    "    for o_i in range(1, output_features_length):\n",
    "        \n",
    "        #print(\"_input_features s \", _input_features.shape)\n",
    "        \n",
    "        _pred_features = rnn(_input_features)\n",
    "        _pred_features = torch.unsqueeze(_pred_features, axis=1)\n",
    "        \n",
    "        #print(\"_pred_features s \", _pred_features.shape)\n",
    "        \n",
    "        _target_features = target_features[:,o_i,:].detach().clone()\n",
    "        _target_features = torch.unsqueeze(_target_features, axis=1)\n",
    "\n",
    "        #print(\"_target_features s \", _target_features.shape)\n",
    "        \n",
    "        _pred_features_for_loss.append(_pred_features)\n",
    "        _target_features_for_loss.append(_target_features)\n",
    "        \n",
    "        # shift input feature seqeunce one feature to the right\n",
    "        # remove feature from beginning input feature sequence\n",
    "        # detach necessary to avoid error concerning running backprob a second time\n",
    "        _input_features = _input_features[:, 1:, :].detach().clone()\n",
    "        _target_features = _target_features.detach().clone()\n",
    "        _pred_features = _pred_features.detach().clone()\n",
    "        \n",
    "        # add predicted or target feature to end of input feature sequence\n",
    "        if teacher_forcing == True:\n",
    "            _input_features = torch.concat((_input_features, _target_features), axis=1)\n",
    "        else:\n",
    "            _input_features = torch.cat((_input_features, _pred_features), axis=1)\n",
    "            \n",
    "        #print(\"_input_features s \", _input_features.shape)\n",
    "\n",
    "        \n",
    "    _pred_features_for_loss = torch.cat(_pred_features_for_loss, dim=1)\n",
    "    _target_features_for_loss = torch.cat(_target_features_for_loss, dim=1)\n",
    "    \n",
    "    #print(\"_pred_features_for_loss 2 s \", _pred_features_for_loss.shape)\n",
    "    #print(\"_target_features_for_loss 2 s \", _target_features_for_loss.shape)\n",
    "    \n",
    "    _loss = loss(_target_features_for_loss, _pred_features_for_loss) \n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    _loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print(\"_ar_loss_total mean s \", _ar_loss_total.shape)\n",
    "    \n",
    "    #return _ar_loss, _ar_norm_loss, _ar_quat_loss\n",
    "    \n",
    "    return _loss\n",
    "\n",
    "def test_step(input_features, target_features, teacher_forcing):\n",
    "    \n",
    "    rnn.eval()\n",
    "\n",
    "    #print(\"ar_train_step\")    \n",
    "    #print(\"teacher_forcing \", teacher_forcing)\n",
    "    #print(\"pose_sequences s \", pose_sequences.shape)\n",
    "    #print(\"target_poses s \", target_poses.shape)\n",
    "\n",
    "    #_input_poses = pose_sequences.detach().clone()    \n",
    "    _input_features = input_features  \n",
    "    output_features_length = target_features.shape[1]\n",
    "    \n",
    "    #print(\"output_features_length \", output_features_length)\n",
    "    \n",
    "    _pred_features_for_loss = []\n",
    "    _target_features_for_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for o_i in range(1, output_features_length):\n",
    "            \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "            \n",
    "            _pred_features = rnn(_input_features)\n",
    "            _pred_features = torch.unsqueeze(_pred_features, axis=1)\n",
    "            \n",
    "            #print(\"_pred_features s \", _pred_features.shape)\n",
    "            \n",
    "            _target_features = target_features[:,o_i,:].detach().clone()\n",
    "            _target_features = torch.unsqueeze(_target_features, axis=1)\n",
    "    \n",
    "            #print(\"_target_features s \", _target_features.shape)\n",
    "            \n",
    "            _pred_features_for_loss.append(_pred_features)\n",
    "            _target_features_for_loss.append(_target_features)\n",
    "            \n",
    "            # shift input feature seqeunce one feature to the right\n",
    "            # remove feature from beginning input feature sequence\n",
    "            # detach necessary to avoid error concerning running backprob a second time\n",
    "            _input_features = _input_features[:, 1:, :].detach().clone()\n",
    "            _target_features = _target_features.detach().clone()\n",
    "            _pred_features = _pred_features.detach().clone()\n",
    "            \n",
    "            # add predicted or target feature to end of input feature sequence\n",
    "            if teacher_forcing == True:\n",
    "                _input_features = torch.concat((_input_features, _target_features), axis=1)\n",
    "            else:\n",
    "                _input_features = torch.cat((_input_features, _pred_features), axis=1)\n",
    "                \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "\n",
    "        \n",
    "    _pred_features_for_loss = torch.cat(_pred_features_for_loss, dim=1)\n",
    "    _target_features_for_loss = torch.cat(_target_features_for_loss, dim=1)\n",
    "    \n",
    "    #print(\"_pred_features_for_loss 2 s \", _pred_features_for_loss.shape)\n",
    "    #print(\"_target_features_for_loss 2 s \", _target_features_for_loss.shape)\n",
    "    \n",
    "    _loss = loss(_target_features_for_loss, _pred_features_for_loss) \n",
    "    \n",
    "    #print(\"_ar_loss_total mean s \", _ar_loss_total.shape)\n",
    "    \n",
    "    #return _ar_loss, _ar_norm_loss, _ar_quat_loss\n",
    "    \n",
    "    rnn.eval()\n",
    "    \n",
    "    return _loss\n",
    "\n",
    "def train(train_dataloader, test_dataloader, epochs):\n",
    "    \n",
    "    loss_history = {}\n",
    "    loss_history[\"train\"] = []\n",
    "    loss_history[\"test\"] = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        _train_loss_per_epoch = []\n",
    "\n",
    "        for train_batch in train_dataloader:\n",
    "            input_feature_sequences = train_batch[0].to(device)\n",
    "            target_features = train_batch[1].to(device)\n",
    "            \n",
    "            use_teacher_forcing = np.random.uniform() < teacher_forcing_prob\n",
    "            \n",
    "            _loss = train_step(input_feature_sequences, target_features, use_teacher_forcing)\n",
    "            \n",
    "            _loss = _loss.detach().cpu().numpy()\n",
    "            \n",
    "            _train_loss_per_epoch.append(_loss)\n",
    "\n",
    "        _train_loss_per_epoch = np.mean(np.array(_train_loss_per_epoch))\n",
    "\n",
    "        _test_loss_per_epoch = []\n",
    "        \n",
    "        for test_batch in test_dataloader:\n",
    "            input_feature_sequences = train_batch[0].to(device)\n",
    "            target_features = train_batch[1].to(device)\n",
    "            \n",
    "            use_teacher_forcing = np.random.uniform() < teacher_forcing_prob\n",
    "            \n",
    "            _loss = test_step(input_feature_sequences, target_features, use_teacher_forcing)\n",
    "            \n",
    "            _loss = _loss.detach().cpu().numpy()\n",
    "            \n",
    "            _test_loss_per_epoch.append(_loss)\n",
    "        \n",
    "        _test_loss_per_epoch = np.mean(np.array(_test_loss_per_epoch))\n",
    "        \n",
    "        if epoch % model_save_interval == 0 and save_weights == True:\n",
    "            torch.save(rnn.state_dict(), \"results/weights/rnn_weights_epoch_{}\".format(epoch))\n",
    "        \n",
    "        loss_history[\"train\"].append(_train_loss_per_epoch)\n",
    "        loss_history[\"test\"].append(_test_loss_per_epoch)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print ('epoch {} : train: {:01.4f} test: {:01.4f} time {:01.2f}'.format(epoch + 1, _train_loss_per_epoch, _test_loss_per_epoch, time.time()-start))\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b63c5c-a88c-45c4-832f-f5f8b2ee24dd",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2efede-ac8a-4fc2-9d06-705121883fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = train(train_loader, test_loader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa557334-4192-4f7e-9e00-256b0e63d213",
   "metadata": {},
   "source": [
    "### Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d236e-ee22-4c71-ae33-54f4ae1243fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss_as_image(loss_history, image_file_name):\n",
    "    keys = list(loss_history.keys())\n",
    "    epochs = len(loss_history[keys[0]])\n",
    "    \n",
    "    for key in keys:\n",
    "        plt.plot(range(epochs), loss_history[key], label=key)\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(image_file_name)\n",
    "    plt.show()\n",
    "\n",
    "def save_loss_as_csv(loss_history, csv_file_name):\n",
    "    with open(csv_file_name, 'w') as csv_file:\n",
    "        csv_columns = list(loss_history.keys())\n",
    "        csv_row_count = len(loss_history[csv_columns[0]])\n",
    "        \n",
    "        \n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=',', lineterminator='\\n')\n",
    "        csv_writer.writeheader()\n",
    "    \n",
    "        for row in range(csv_row_count):\n",
    "        \n",
    "            csv_row = {}\n",
    "        \n",
    "            for key in loss_history.keys():\n",
    "                csv_row[key] = loss_history[key][row]\n",
    "\n",
    "            csv_writer.writerow(csv_row)\n",
    "\n",
    "\n",
    "save_loss_as_csv(loss_history, \"results/histories/rnn_history_{}.csv\".format(epochs))\n",
    "save_loss_as_image(loss_history, \"results/histories/rnn_history_{}.png\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09fb326-6e6d-4e1b-a8fb-a502d8d6ab9b",
   "metadata": {},
   "source": [
    "### Save Final Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9aa99-f5a0-47ca-92ad-b732e4c62df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn.state_dict(), \"results/weights/rnn_weights_epoch_{}\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e341fa-c0bc-42eb-9068-cd0be7ab84a5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd037226-93c5-400e-9cc3-391e8526f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.eval()\n",
    "\n",
    "def export_orig_audio(waveform_data, start_time, end_time, file_name):\n",
    "    \n",
    "    start_time_samples = int(start_time * audio_sample_rate)\n",
    "    end_time_samples = int(end_time * audio_sample_rate)\n",
    "    \n",
    "    torchaudio.save(file_name, waveform_data[:, start_time_samples:end_time_samples], audio_sample_rate)\n",
    "\n",
    "def export_ref_audio(waveform_data, start_time, end_time, file_name):\n",
    "    \n",
    "    start_time_samples = int(start_time * audio_sample_rate)\n",
    "    end_time_samples = int(end_time * audio_sample_rate)\n",
    "    \n",
    "    # audio features\n",
    "    audio_features = vocos.feature_extractor(waveform_data[:, start_time_samples:end_time_samples])\n",
    "    \n",
    "    ref_audio = vocos.decode(audio_features)\n",
    "    \n",
    "    torchaudio.save(file_name, ref_audio.detach().cpu(), audio_sample_rate)\n",
    "    \n",
    "def export_pred_audio(waveform_data, start_time, end_time, file_name):\n",
    "    \n",
    "    start_time_samples = int(start_time * audio_sample_rate)\n",
    "    end_time_samples = int(end_time * audio_sample_rate)\n",
    "    \n",
    "    # audio features\n",
    "    audio_features = vocos.feature_extractor(waveform_data[:, start_time_samples:end_time_samples])\n",
    "    \n",
    "    #print(\"audio_features s \", audio_features.shape)\n",
    "    \n",
    "    audio_features = audio_features.squeeze(0)\n",
    "    audio_features = torch.permute(audio_features, (1, 0))\n",
    "    audio_feature_count = audio_features.shape[0]\n",
    "    \n",
    "    #print(\"audio_feature_count \", audio_feature_count)\n",
    "    \n",
    "    input_features = audio_features[:seq_input_length]\n",
    "    input_features = input_features.unsqueeze(0)\n",
    "    \n",
    "    output_features_length = audio_feature_count - seq_input_length\n",
    "    \n",
    "    #print(\"output_features_length \", output_features_length)\n",
    "    \n",
    "    _input_features = input_features  \n",
    "    pred_features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for o_i in range(1, output_features_length):\n",
    "            \n",
    "            _input_features = _input_features.to(device)\n",
    "            \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "            \n",
    "            _pred_features = rnn(_input_features)\n",
    "            _pred_features = torch.unsqueeze(_pred_features, axis=1)\n",
    "\n",
    "            _input_features = _input_features[:, 1:, :].detach().clone()\n",
    "            _pred_features = _pred_features.detach().clone()\n",
    "            \n",
    "            pred_features.append(_pred_features.cpu())\n",
    "            \n",
    "            _input_features = torch.cat((_input_features, _pred_features), axis=1)\n",
    "                \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "            \n",
    "    pred_features = torch.cat(pred_features, axis=1)\n",
    "    pred_features = torch.permute(pred_features, (0, 2, 1))\n",
    "    pred_audio = vocos.decode(pred_features)\n",
    "    \n",
    "    torchaudio.save(file_name, pred_audio.detach().cpu(), audio_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7b5ef-f964-4535-87aa-0701457bb9b6",
   "metadata": {},
   "source": [
    "### Perform Audio Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77daf847-a477-4e0d-b882-3a4ed7f16495",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"../../../../Data/Audio/Gutenberg/Night_and_Day_by_Virginia_Woolf_48khz.wav\"\n",
    "audio_start_time_sec = 10.0\n",
    "audio_end_time_sec = 20.0\n",
    "\n",
    "audio_file_gui = widgets.Text(value=audio_file, description=\"Audio File:\", style={'description_width': 'initial'})\n",
    "audio_start_time_sec_gui = widgets.FloatText(value=audio_start_time_sec, description=\"Audio Start Time [Seconds]:\", style={'description_width': 'initial'})\n",
    "audio_end_time_sec_gui = widgets.FloatText(value=audio_end_time_sec, description=\"Audio End Time [Seconds]\", style={'description_width': 'initial'})\n",
    "\n",
    "display(audio_file_gui)\n",
    "display(audio_start_time_sec_gui)\n",
    "display(audio_end_time_sec_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a9eda-d8d4-46aa-bf9f-2a91fced684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = audio_file_gui.value\n",
    "audio_start_time_sec = audio_start_time_sec_gui.value\n",
    "audio_end_time_sec = audio_end_time_sec_gui.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151496b-3b7d-404d-b992-cf25b381885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_data, _ = torchaudio.load(audio_file)\n",
    "\n",
    "export_orig_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/orig_{}-{}.wav\".format(audio_start_time_sec, audio_end_time_sec))\n",
    "export_ref_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/ref_{}-{}.wav\".format(audio_start_time_sec, audio_end_time_sec))\n",
    "export_pred_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/pred_{}-{}_epoch_{}.wav\".format(audio_start_time_sec, audio_end_time_sec, epochs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
