{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d1e903-1f33-4a46-8b69-b9c329995839",
   "metadata": {},
   "source": [
    "# Audio Continuation Transformer Decoder (Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070177bd-9324-4974-b01d-affc473eb63c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff77a26-c7cf-4730-b8c4-11f758e225cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "from collections import OrderedDict\n",
    "\n",
    "from vocos import Vocos\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764f852-1753-450c-bc75-a343f58beb71",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e31a9b-6c4b-4916-a2ec-6053587da1c9",
   "metadata": {},
   "source": [
    "### Compute Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5725ca1-a556-46fc-9928-5be8c9354abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b74bf9-4a6b-4fbb-8642-d3cc209489cd",
   "metadata": {},
   "source": [
    "### Audio Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17f04a-b41b-414e-a6ca-48797ec2836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = \"../../../../Data/Audio/Gutenberg/\"\n",
    "audio_files = [\"Night_and_Day_by_Virginia_Woolf_48khz.wav\"]\n",
    "audio_file_excerpts = [ [ 14.0, 314.0 ] ]\n",
    "audio_sample_rate = 48000\n",
    "\n",
    "audio_files_all = [f for f in os.listdir(audio_file_path) if os.path.isfile(os.path.join(audio_file_path, f))]\n",
    "\n",
    "audio_file_excerpts_gui = widgets.Text(str(audio_file_excerpts), description='Audio File Excerpts:', style={'description_width': 'initial'})\n",
    "\n",
    "audio_files_gui = widgets.SelectMultiple(\n",
    "    options=audio_files_all,\n",
    "    value=audio_files,  # default: first option selected; can be empty\n",
    "    description='Audio Files:',\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(audio_files_gui)\n",
    "display(audio_file_excerpts_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951b1ae-7c85-494b-aed1-7a9f6ab014b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = list(audio_files_gui.value)\n",
    "\n",
    "matches = re.findall(r\"\\[\\s*([-+]?\\d*\\.?\\d+)\\s*,\\s*([-+]?\\d*\\.?\\d+)\\s*\\]\", audio_file_excerpts_gui.value)\n",
    "audio_file_excerpts = [[float(a), float(b)] for a, b in matches]\n",
    "audio_file_excerpts = [[int(a * audio_sample_rate), int(b * audio_sample_rate)] for a, b in audio_file_excerpts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fbddd9-1dc8-43ae-9c23-9bcc953917a0",
   "metadata": {},
   "source": [
    "### Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec80cea-9743-4db4-8725-1fbe773e03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer_count = 6\n",
    "decoder_head_count = 8\n",
    "decoder_embed_dim = 512\n",
    "decoder_ff_dim = 2048\n",
    "decoder_dropout = 0.1\n",
    "\n",
    "save_weights = True\n",
    "load_weights = False\n",
    "decoder_weights_file = \"results/weights/decoder_weights_epoch_200\"\n",
    "\n",
    "decoder_layer_count_gui = widgets.IntText(value=decoder_layer_count, description=\"Decoder Layer Count:\", style={'description_width': 'initial'})\n",
    "decoder_head_count_gui = widgets.IntText(value=decoder_head_count, description=\"Decoder Head Count:\", style={'description_width': 'initial'})\n",
    "decoder_embed_dim_gui = widgets.IntText(value=decoder_embed_dim, description=\"Decoder Embed Dim:\", style={'description_width': 'initial'})\n",
    "decoder_ff_dim_gui = widgets.IntText(value=decoder_ff_dim, description=\"Decoder Fordward Dim:\", style={'description_width': 'initial'})\n",
    "decoder_dropout_gui = widgets.FloatText(value=decoder_dropout, description=\"Decoder Dropout:\", style={'description_width': 'initial'})\n",
    "\n",
    "save_weights_gui = widgets.Checkbox(\n",
    "    value=save_weights,\n",
    "    description='Save Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "load_weights_gui = widgets.Checkbox(\n",
    "    value=load_weights,\n",
    "    description='Load Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "decoder_weights_file_gui = widgets.Text(value=decoder_weights_file, description=\"Decoder Weights File:\", style={'description_width': 'initial'}) \n",
    "\n",
    "display(decoder_layer_count_gui)\n",
    "display(decoder_head_count_gui)\n",
    "display(decoder_embed_dim_gui)\n",
    "display(decoder_ff_dim_gui)\n",
    "display(decoder_dropout_gui)\n",
    "display(save_weights_gui)\n",
    "display(load_weights_gui)\n",
    "display(decoder_weights_file_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb93c5-93b0-4f80-b737-86bc67795e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer_count = decoder_layer_count_gui.value\n",
    "decoder_head_count = decoder_head_count_gui.value\n",
    "decoder_embed_dim = decoder_embed_dim_gui.value\n",
    "decoder_ff_dim = decoder_ff_dim_gui.value\n",
    "decoder_dropout = decoder_dropout_gui.value\n",
    "save_weights = save_weights_gui.value\n",
    "load_weights = load_weights_gui.value\n",
    "decoder_weights_file = decoder_weights_file_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855005b-9506-4dea-9e5b-f61f20158ba3",
   "metadata": {},
   "source": [
    "### Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314bfc0-11e3-4c3d-a76f-ee17452e0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "test_percentage = 0.1\n",
    "\n",
    "seq_input_length = 64\n",
    "seq_output_length = 10 # this is only used for non-teacher forcing scenarios\n",
    "\n",
    "learning_rate = 1e-4\n",
    "teacher_forcing_prob = 0.0\n",
    "model_save_interval = 10\n",
    "\n",
    "epochs = 200\n",
    "save_history = True\n",
    "\n",
    "batch_size_gui = widgets.IntText(value=batch_size, description=\"Batch Size:\", style={'description_width': 'initial'})\n",
    "test_percentage_gui = widgets.FloatText(value=test_percentage, description=\"Test Ratio:\", style={'description_width': 'initial'})\n",
    "seq_input_length_gui = widgets.IntText(value=seq_input_length, description=\"Sequence Input Length:\", style={'description_width': 'initial'})\n",
    "seq_output_length_gui = widgets.IntText(value=seq_output_length, description=\"Sequence Output Length:\", style={'description_width': 'initial'})\n",
    "learning_rate_gui = widgets.FloatText(value=learning_rate, description=\"Transformer Decoder Learning Rate:\", style={'description_width': 'initial'})\n",
    "teacher_forcing_prob_gui = widgets.FloatText(value=teacher_forcing_prob, description=\"Teacher Forcing Probability:\", style={'description_width': 'initial'})\n",
    "model_save_interval_gui = widgets.IntText(value=model_save_interval, description=\"Model Save Interval:\", style={'description_width': 'initial'})\n",
    "epochs_gui = widgets.IntText(value=epochs, description=\"Epochs:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(batch_size_gui)\n",
    "display(test_percentage_gui)\n",
    "display(seq_input_length_gui)\n",
    "display(seq_output_length_gui)\n",
    "display(learning_rate_gui)\n",
    "display(teacher_forcing_prob_gui)\n",
    "display(model_save_interval_gui)\n",
    "display(epochs_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47374b-cf97-4732-8d98-4cb879e5b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = batch_size_gui.value\n",
    "test_percentage = test_percentage_gui.value\n",
    "seq_input_length = seq_input_length_gui.value\n",
    "seq_output_length = seq_output_length_gui.value\n",
    "learning_rate = learning_rate_gui.value\n",
    "teacher_forcing_prob = teacher_forcing_prob_gui.value\n",
    "model_save_interval = model_save_interval_gui.value\n",
    "epochs = epochs_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b89e4-45a1-4afc-b067-ffa8951b4d22",
   "metadata": {},
   "source": [
    "## Load Audio Data and Calculate Mel Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5eaea-0f0f-48e1-af71-b2e093727a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocos = Vocos.from_pretrained(\"kittn/vocos-mel-48khz-alpha1\")\n",
    "\n",
    "all_audio_features = []\n",
    "\n",
    "for audio_file_index in range(len(audio_files)):\n",
    "    \n",
    "    print(\"audio file \", audio_files[audio_file_index])\n",
    "    \n",
    "    audio_file = audio_files[audio_file_index]\n",
    "    audio_file_excerpt = audio_file_excerpts[audio_file_index]\n",
    "    \n",
    "    waveform_range_start = audio_file_excerpt[0]\n",
    "    waveform_range_end = audio_file_excerpt[1]\n",
    "    \n",
    "    # load audio file\n",
    "    waveform_data, _ = torchaudio.load(audio_file_path + audio_file)\n",
    "    \n",
    "    # audio excerpt\n",
    "    waveform_data = waveform_data[:, waveform_range_start:waveform_range_end]\n",
    "    \n",
    "    print(\"waveform_data s \", waveform_data.shape)\n",
    "    \n",
    "    # audio features\n",
    "    audio_features = vocos.feature_extractor(waveform_data)\n",
    "    \n",
    "    print(\"audio_features s \", audio_features.shape)\n",
    "    \n",
    "    audio_features = audio_features.squeeze(0)\n",
    "    audio_features = torch.permute(audio_features, (1, 0))\n",
    "    \n",
    "    print(\"audio_features 2 s \", audio_features.shape)\n",
    "    \n",
    "    all_audio_features.append(audio_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd72ff-a629-490b-a804-421379804512",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f34c5-b7ae-487a-9a13-b7044c1c9f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "audio_features_dim = all_audio_features[0].shape[-1]\n",
    "\n",
    "audio_features_dim\n",
    "\n",
    "for audio_features in all_audio_features:\n",
    "    \n",
    "    total_sequence_length = audio_features.shape[0]\n",
    "    \n",
    "    for pI in range(total_sequence_length - seq_input_length - seq_output_length - 1):\n",
    "        X_sample = audio_features[pI:pI+seq_input_length]\n",
    "        X.append(X_sample)\n",
    "        \n",
    "        y_sample = audio_features[pI+seq_input_length:pI+seq_input_length+seq_output_length]\n",
    "        y.append(y_sample)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, ...], self.y[idx, ...]\n",
    "\n",
    "\n",
    "full_dataset = SequenceDataset(X, y)\n",
    "\n",
    "X_item, y_item = full_dataset[0]\n",
    "\n",
    "print(\"X_item s \", X_item.shape)\n",
    "print(\"y_item s \", y_item.shape)\n",
    "\n",
    "test_size = int(test_percentage * len(full_dataset))\n",
    "train_size = len(full_dataset) - test_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"X_batch s \", X_batch.shape)\n",
    "print(\"y_batch s \", y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ad599-1d2f-491a-8a35-8e8e1cc03c63",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317eb5b-67d7-4cd5-b17c-e8a7e829cf9d",
   "metadata": {},
   "source": [
    "## PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ed00e-f836-447e-b610-eb8bfddc29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bc645-f986-4f8e-ab4a-7782ba6eb753",
   "metadata": {},
   "source": [
    "## Create TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49439334-3e51-4e5e-944d-2387cb4ef9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        audio_dim,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        num_decoder_layers,\n",
    "        ff_dim,\n",
    "        dropout_p,\n",
    "        pos_encoding_max_length\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.audio2embed = nn.Linear(audio_dim, embed_dim) # map audio data to embedding\n",
    "\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=embed_dim, dropout_p=dropout_p, max_len=pos_encoding_max_length\n",
    "        )\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout_p, batch_first=True)\n",
    "        #self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers = num_decoder_layers)\n",
    "\n",
    "        # build a decoder directly from TransformerDecoderLayer\n",
    "        # rather than using the nn.TransformerDecoder module which requires also a Transformer Encoder\n",
    "        self.decoder = self.layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=embed_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=ff_dim,\n",
    "                dropout=dropout_p,\n",
    "                activation='gelu',\n",
    "                batch_first=True\n",
    "            ) for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "        self.embed2audio = nn.Linear(embed_dim, audio_dim) # map embedding to audio data\n",
    "\n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "       \n",
    "    def forward(self, audio_data):\n",
    "        \n",
    "        #print(\"forward\")\n",
    "        \n",
    "        #print(\"audio_data s \", audio_data.shape)\n",
    "        \n",
    "        # dummy \"memory\" as zero (only self-attention is used)\n",
    "        memory = torch.zeros(audio_data.size(0), audio_data.size(1), self.embed_dim, device=audio_data.device)\n",
    "\n",
    "        #print(\"memory s \", memory.shape)\n",
    "\n",
    "        # Lower triangular matrix for autoregressive masking\n",
    "        tgt_mask = self.get_tgt_mask(audio_data.shape[1]).to(audio_data.device)\n",
    "\n",
    "        #print(\"tgt_mask s \", tgt_mask.shape)\n",
    "\n",
    "        audio_embedded = self.audio2embed(audio_data) * math.sqrt(self.embed_dim)\n",
    "        \n",
    "        #print(\"audio_embedded 1 s \", audio_embedded.shape)\n",
    "        \n",
    "        audio_embedded = self.positional_encoder(audio_embedded)\n",
    "        \n",
    "        #print(\"audio_embedded 2 s \", audio_embedded.shape)\n",
    "        \n",
    "        x = audio_embedded\n",
    "        \n",
    "        #print(\"x s \", x.shape)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            #print(\"x in s \", x.shape)\n",
    "            \n",
    "            x = layer(x, memory, tgt_mask=tgt_mask)\n",
    "            \n",
    "            #print(\"x out s \", x.shape)\n",
    "\n",
    "        decoder_out = x\n",
    "\n",
    "        out = self.embed2audio(decoder_out)\n",
    "        \n",
    "        out = out[:, -1, :] # only last time step \n",
    "        \n",
    "        return out\n",
    "\n",
    "decoder = TransformerDecoder(audio_dim=audio_features_dim,\n",
    "                          embed_dim=decoder_embed_dim, \n",
    "                          num_heads=decoder_head_count, \n",
    "                          num_decoder_layers=decoder_layer_count, \n",
    "                          ff_dim = decoder_ff_dim,\n",
    "                          dropout_p=decoder_dropout,\n",
    "                          pos_encoding_max_length=seq_input_length).to(device)\n",
    "\n",
    "print(decoder)\n",
    "\n",
    "if load_weights == True:\n",
    "    if device == 'cuda':\n",
    "        decoder.load_state_dict(torch.load(decoder_weights_file))\n",
    "    else:\n",
    "        decoder.load_state_dict(torch.load(decoder_weights_file, map_location=device ))\n",
    "\n",
    "# test transformer decoder\n",
    "x_batch, _ = next(iter(train_loader))\n",
    "\n",
    "decoder_input = x_batch.to(device)\n",
    "decoder_output = decoder(decoder_input)\n",
    "\n",
    "print(\"decoder_input s \", decoder_input.shape)\n",
    "print(\"decoder_output s \", decoder_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad4a02-b191-4515-926e-35618717a61a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e55d51-eb16-409d-9efd-6fef9b39aad1",
   "metadata": {},
   "source": [
    "### Create Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9706d24-6549-4523-86d2-eb5779978f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.336) # reduce the learning every 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2ec63-915f-4497-b2ac-44b996617b3d",
   "metadata": {},
   "source": [
    "### Create Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9eb80a-70f0-441f-be8c-3a1d861212c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_loss = nn.MSELoss()\n",
    "\n",
    "def loss(y, yhat):\n",
    "    _rec_loss = rec_loss(yhat, y)\n",
    "\n",
    "    return _rec_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6587475-6f16-4920-a7d3-7a4578720369",
   "metadata": {},
   "source": [
    "### Create Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b022d6b-49d4-4ed7-92fb-9161187d2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_features, target_features, teacher_forcing):\n",
    "    \n",
    "    decoder.train()\n",
    "\n",
    "    #print(\"ar_train_step\")    \n",
    "    #print(\"teacher_forcing \", teacher_forcing)\n",
    "    #print(\"pose_sequences s \", pose_sequences.shape)\n",
    "    #print(\"target_poses s \", target_poses.shape)\n",
    "\n",
    "    #_input_poses = pose_sequences.detach().clone()    \n",
    "    _input_features = input_features  \n",
    "    output_features_length = target_features.shape[1]\n",
    "    \n",
    "    #print(\"output_features_length \", output_features_length)\n",
    "    \n",
    "    _pred_features_for_loss = []\n",
    "    _target_features_for_loss = []\n",
    "    \n",
    "    for o_i in range(1, output_features_length):\n",
    "        \n",
    "        #print(\"_input_features s \", _input_features.shape)\n",
    "        \n",
    "        _pred_features = decoder(_input_features)\n",
    "        _pred_features = torch.unsqueeze(_pred_features, axis=1)\n",
    "        \n",
    "        #print(\"_pred_features s \", _pred_features.shape)\n",
    "        \n",
    "        _target_features = target_features[:,o_i,:].detach().clone()\n",
    "        _target_features = torch.unsqueeze(_target_features, axis=1)\n",
    "\n",
    "        #print(\"_target_features s \", _target_features.shape)\n",
    "        \n",
    "        _pred_features_for_loss.append(_pred_features)\n",
    "        _target_features_for_loss.append(_target_features)\n",
    "        \n",
    "        # shift input feature seqeunce one feature to the right\n",
    "        # remove feature from beginning input feature sequence\n",
    "        # detach necessary to avoid error concerning running backprob a second time\n",
    "        _input_features = _input_features[:, 1:, :].detach().clone()\n",
    "        _target_features = _target_features.detach().clone()\n",
    "        _pred_features = _pred_features.detach().clone()\n",
    "        \n",
    "        # add predicted or target feature to end of input feature sequence\n",
    "        if teacher_forcing == True:\n",
    "            _input_features = torch.concat((_input_features, _target_features), axis=1)\n",
    "        else:\n",
    "            _input_features = torch.cat((_input_features, _pred_features), axis=1)\n",
    "            \n",
    "        #print(\"_input_features s \", _input_features.shape)\n",
    "\n",
    "        \n",
    "    _pred_features_for_loss = torch.cat(_pred_features_for_loss, dim=1)\n",
    "    _target_features_for_loss = torch.cat(_target_features_for_loss, dim=1)\n",
    "    \n",
    "    #print(\"_pred_features_for_loss 2 s \", _pred_features_for_loss.shape)\n",
    "    #print(\"_target_features_for_loss 2 s \", _target_features_for_loss.shape)\n",
    "    \n",
    "    _loss = loss(_target_features_for_loss, _pred_features_for_loss) \n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    _loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print(\"_ar_loss_total mean s \", _ar_loss_total.shape)\n",
    "    \n",
    "    #return _ar_loss, _ar_norm_loss, _ar_quat_loss\n",
    "    \n",
    "    return _loss\n",
    "\n",
    "def test_step(input_features, target_features, teacher_forcing):\n",
    "    \n",
    "    decoder.eval()\n",
    "\n",
    "    #print(\"ar_train_step\")    \n",
    "    #print(\"teacher_forcing \", teacher_forcing)\n",
    "    #print(\"pose_sequences s \", pose_sequences.shape)\n",
    "    #print(\"target_poses s \", target_poses.shape)\n",
    "\n",
    "    #_input_poses = pose_sequences.detach().clone()    \n",
    "    _input_features = input_features  \n",
    "    output_features_length = target_features.shape[1]\n",
    "    \n",
    "    #print(\"output_features_length \", output_features_length)\n",
    "    \n",
    "    _pred_features_for_loss = []\n",
    "    _target_features_for_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for o_i in range(1, output_features_length):\n",
    "            \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "            \n",
    "            _pred_features = decoder(_input_features)\n",
    "            _pred_features = torch.unsqueeze(_pred_features, axis=1)\n",
    "            \n",
    "            #print(\"_pred_features s \", _pred_features.shape)\n",
    "            \n",
    "            _target_features = target_features[:,o_i,:].detach().clone()\n",
    "            _target_features = torch.unsqueeze(_target_features, axis=1)\n",
    "    \n",
    "            #print(\"_target_features s \", _target_features.shape)\n",
    "            \n",
    "            _pred_features_for_loss.append(_pred_features)\n",
    "            _target_features_for_loss.append(_target_features)\n",
    "            \n",
    "            # shift input feature seqeunce one feature to the right\n",
    "            # remove feature from beginning input feature sequence\n",
    "            # detach necessary to avoid error concerning running backprob a second time\n",
    "            _input_features = _input_features[:, 1:, :].detach().clone()\n",
    "            _target_features = _target_features.detach().clone()\n",
    "            _pred_features = _pred_features.detach().clone()\n",
    "            \n",
    "            # add predicted or target feature to end of input feature sequence\n",
    "            if teacher_forcing == True:\n",
    "                _input_features = torch.concat((_input_features, _target_features), axis=1)\n",
    "            else:\n",
    "                _input_features = torch.cat((_input_features, _pred_features), axis=1)\n",
    "                \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "\n",
    "        \n",
    "    _pred_features_for_loss = torch.cat(_pred_features_for_loss, dim=1)\n",
    "    _target_features_for_loss = torch.cat(_target_features_for_loss, dim=1)\n",
    "    \n",
    "    #print(\"_pred_features_for_loss 2 s \", _pred_features_for_loss.shape)\n",
    "    #print(\"_target_features_for_loss 2 s \", _target_features_for_loss.shape)\n",
    "    \n",
    "    _loss = loss(_target_features_for_loss, _pred_features_for_loss) \n",
    "    \n",
    "    #print(\"_ar_loss_total mean s \", _ar_loss_total.shape)\n",
    "    \n",
    "    #return _ar_loss, _ar_norm_loss, _ar_quat_loss\n",
    "    \n",
    "    decoder.eval()\n",
    "    \n",
    "    return _loss\n",
    "\n",
    "def train(train_dataloader, test_dataloader, epochs):\n",
    "    \n",
    "    loss_history = {}\n",
    "    loss_history[\"train\"] = []\n",
    "    loss_history[\"test\"] = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        _train_loss_per_epoch = []\n",
    "\n",
    "        for train_batch in train_dataloader:\n",
    "            input_feature_sequences = train_batch[0].to(device)\n",
    "            target_features = train_batch[1].to(device)\n",
    "            \n",
    "            use_teacher_forcing = np.random.uniform() < teacher_forcing_prob\n",
    "            \n",
    "            _loss = train_step(input_feature_sequences, target_features, use_teacher_forcing)\n",
    "            \n",
    "            _loss = _loss.detach().cpu().numpy()\n",
    "            \n",
    "            _train_loss_per_epoch.append(_loss)\n",
    "\n",
    "        _train_loss_per_epoch = np.mean(np.array(_train_loss_per_epoch))\n",
    "\n",
    "        _test_loss_per_epoch = []\n",
    "        \n",
    "        for test_batch in test_dataloader:\n",
    "            input_feature_sequences = train_batch[0].to(device)\n",
    "            target_features = train_batch[1].to(device)\n",
    "            \n",
    "            use_teacher_forcing = np.random.uniform() < teacher_forcing_prob\n",
    "            \n",
    "            _loss = test_step(input_feature_sequences, target_features, use_teacher_forcing)\n",
    "            \n",
    "            _loss = _loss.detach().cpu().numpy()\n",
    "            \n",
    "            _test_loss_per_epoch.append(_loss)\n",
    "        \n",
    "        _test_loss_per_epoch = np.mean(np.array(_test_loss_per_epoch))\n",
    "        \n",
    "        if epoch % model_save_interval == 0 and save_weights == True:\n",
    "            torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epoch))\n",
    "        \n",
    "        loss_history[\"train\"].append(_train_loss_per_epoch)\n",
    "        loss_history[\"test\"].append(_test_loss_per_epoch)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print ('epoch {} : train: {:01.4f} test: {:01.4f} time {:01.2f}'.format(epoch + 1, _train_loss_per_epoch, _test_loss_per_epoch, time.time()-start))\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2ec12a-9a28-4337-8f4e-d406e9b21698",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86c406-30fe-489a-862f-3e0ec7409024",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = train(train_loader, test_loader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18addee-05b8-4a56-a8d3-9df62b70ddb9",
   "metadata": {},
   "source": [
    "### Save Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df46126-9c20-455b-8295-f73d22d1c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss_as_image(loss_history, image_file_name):\n",
    "    keys = list(loss_history.keys())\n",
    "    epochs = len(loss_history[keys[0]])\n",
    "    \n",
    "    for key in keys:\n",
    "        plt.plot(range(epochs), loss_history[key], label=key)\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(image_file_name)\n",
    "    plt.show()\n",
    "\n",
    "def save_loss_as_csv(loss_history, csv_file_name):\n",
    "    with open(csv_file_name, 'w') as csv_file:\n",
    "        csv_columns = list(loss_history.keys())\n",
    "        csv_row_count = len(loss_history[csv_columns[0]])\n",
    "        \n",
    "        \n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=',', lineterminator='\\n')\n",
    "        csv_writer.writeheader()\n",
    "    \n",
    "        for row in range(csv_row_count):\n",
    "        \n",
    "            csv_row = {}\n",
    "        \n",
    "            for key in loss_history.keys():\n",
    "                csv_row[key] = loss_history[key][row]\n",
    "\n",
    "            csv_writer.writerow(csv_row)\n",
    "\n",
    "\n",
    "save_loss_as_csv(loss_history, \"results/histories/rnn_history_{}.csv\".format(epochs))\n",
    "save_loss_as_image(loss_history, \"results/histories/rnn_history_{}.png\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7fb07b-f864-4458-ba0e-346e1833a238",
   "metadata": {},
   "source": [
    "### Save Final Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a8ed8-a639-4ad7-b7c0-b891aa599856",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd12001-f0b2-4e05-8851-563fb26c2ee8",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b37fa1-acb1-4608-b0cf-feef6e411d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()\n",
    "\n",
    "def export_orig_audio(waveform_data, start_time, end_time, file_name):\n",
    "    \n",
    "    start_time_samples = int(start_time * audio_sample_rate)\n",
    "    end_time_samples = int(end_time * audio_sample_rate)\n",
    "    \n",
    "    torchaudio.save(file_name, waveform_data[:, start_time_samples:end_time_samples], audio_sample_rate)\n",
    "\n",
    "def export_ref_audio(waveform_data, start_time, end_time, file_name):\n",
    "    \n",
    "    start_time_samples = int(start_time * audio_sample_rate)\n",
    "    end_time_samples = int(end_time * audio_sample_rate)\n",
    "    \n",
    "    # audio features\n",
    "    audio_features = vocos.feature_extractor(waveform_data[:, start_time_samples:end_time_samples])\n",
    "    \n",
    "    ref_audio = vocos.decode(audio_features)\n",
    "    \n",
    "    torchaudio.save(file_name, ref_audio.detach().cpu(), audio_sample_rate)\n",
    "    \n",
    "\n",
    "def export_pred_audio(waveform_data, start_time, end_time, file_name):\n",
    "    \n",
    "    start_time_samples = int(start_time * audio_sample_rate)\n",
    "    end_time_samples = int(end_time * audio_sample_rate)\n",
    "    \n",
    "    # audio features\n",
    "    audio_features = vocos.feature_extractor(waveform_data[:, start_time_samples:end_time_samples])\n",
    "    \n",
    "    #print(\"audio_features s \", audio_features.shape)\n",
    "    \n",
    "    audio_features = audio_features.squeeze(0)\n",
    "    audio_features = torch.permute(audio_features, (1, 0))\n",
    "    audio_feature_count = audio_features.shape[0]\n",
    "    \n",
    "    #print(\"audio_feature_count \", audio_feature_count)\n",
    "    \n",
    "    input_features = audio_features[:seq_input_length]\n",
    "    input_features = input_features.unsqueeze(0)\n",
    "    \n",
    "    output_features_length = audio_feature_count - seq_input_length\n",
    "    \n",
    "    #print(\"output_features_length \", output_features_length)\n",
    "    \n",
    "    _input_features = input_features  \n",
    "    pred_features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for o_i in range(1, output_features_length):\n",
    "            \n",
    "            _input_features = _input_features.to(device)\n",
    "            \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "            \n",
    "            _pred_features = decoder(_input_features)\n",
    "            _pred_features = torch.unsqueeze(_pred_features, axis=1)\n",
    "\n",
    "            _input_features = _input_features[:, 1:, :].detach().clone()\n",
    "            _pred_features = _pred_features.detach().clone()\n",
    "            \n",
    "            pred_features.append(_pred_features.cpu())\n",
    "            \n",
    "            _input_features = torch.cat((_input_features, _pred_features), axis=1)\n",
    "                \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "            \n",
    "    pred_features = torch.cat(pred_features, axis=1)\n",
    "    pred_features = torch.permute(pred_features, (0, 2, 1))\n",
    "    pred_audio = vocos.decode(pred_features)\n",
    "    \n",
    "    torchaudio.save(file_name, pred_audio.detach().cpu(), audio_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1858b39-d518-49b2-91a9-b90e0ce95dfb",
   "metadata": {},
   "source": [
    "### Perform Audio Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c4a81-94cd-45e8-b9dc-71fbbf382921",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"../../../../Data/Audio/Gutenberg/Night_and_Day_by_Virginia_Woolf_48khz.wav\"\n",
    "audio_start_time_sec = 60.0\n",
    "audio_end_time_sec = 70.0\n",
    "\n",
    "audio_file_gui = widgets.Text(value=audio_file, description=\"Audio File:\", style={'description_width': 'initial'})\n",
    "audio_start_time_sec_gui = widgets.FloatText(value=audio_start_time_sec, description=\"Audio Start Time [Seconds]:\", style={'description_width': 'initial'})\n",
    "audio_end_time_sec_gui = widgets.FloatText(value=audio_end_time_sec, description=\"Audio End Time [Seconds]\", style={'description_width': 'initial'})\n",
    "\n",
    "display(audio_file_gui)\n",
    "display(audio_start_time_sec_gui)\n",
    "display(audio_end_time_sec_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2bec8-5902-464c-8269-76e86c1e7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = audio_file_gui.value\n",
    "audio_start_time_sec = audio_start_time_sec_gui.value\n",
    "audio_end_time_sec = audio_end_time_sec_gui.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab301dd6-85cc-4fd8-b184-f7f11ef0ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_data, _ = torchaudio.load(audio_file)\n",
    "\n",
    "export_orig_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/orig_{}-{}.wav\".format(audio_start_time_sec, audio_end_time_sec))\n",
    "export_ref_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/ref_{}-{}.wav\".format(audio_start_time_sec, audio_end_time_sec))\n",
    "export_pred_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/pred_{}-{}_epoch_{}.wav\".format(audio_start_time_sec, audio_end_time_sec, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538530b-9e12-4d9d-9328-e3cc69f50756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
