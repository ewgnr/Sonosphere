{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d1e903-1f33-4a46-8b69-b9c329995839",
   "metadata": {},
   "source": [
    "# Audio Continuation Transformer Decoder (Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070177bd-9324-4974-b01d-affc473eb63c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff77a26-c7cf-4730-b8c4-11f758e225cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "from collections import OrderedDict\n",
    "\n",
    "from vocos import Vocos\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764f852-1753-450c-bc75-a343f58beb71",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e31a9b-6c4b-4916-a2ec-6053587da1c9",
   "metadata": {},
   "source": [
    "### Compute Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5725ca1-a556-46fc-9928-5be8c9354abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b74bf9-4a6b-4fbb-8642-d3cc209489cd",
   "metadata": {},
   "source": [
    "### Audio Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a17f04a-b41b-414e-a6ca-48797ec2836f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b2ccd9cd844be3a6fed0767d1f9ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../../../../Data/Audio/Yannick_Leonie/48khz/', description='Audio File Path:', style=TextStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f57478ea19c4e0e92eeb5c1346ffcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=48000, description='Audio Sample Rate:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_file_path = \"../../../../Data/Audio/Yannick_Leonie/48khz/\"\n",
    "audio_files = [] # audio files are automatically collected from the specified folder and added here\n",
    "audio_sample_rate = 48000\n",
    "\n",
    "audio_files_all = [f for f in os.listdir(audio_file_path) if os.path.isfile(os.path.join(audio_file_path, f))]\n",
    "\n",
    "audio_file_path_gui = widgets.Text(str(audio_file_path), description='Audio File Path:', style={'description_width': 'initial'})\n",
    "audio_sample_rate_gui = widgets.IntText(audio_sample_rate, description='Audio Sample Rate:', style={'description_width': 'initial'})\n",
    "\n",
    "display(audio_file_path_gui)\n",
    "display(audio_sample_rate_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1951b1ae-7c85-494b-aed1-7a9f6ab014b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = audio_file_path_gui.value\n",
    "audio_sample_rate = audio_sample_rate_gui.value\n",
    "\n",
    "def gather_audio_files(root_dir, suffixes):\n",
    "    \n",
    "    if isinstance(suffixes, str):\n",
    "        suffixes_tuple = (suffixes,)\n",
    "    else:\n",
    "        suffixes_tuple = tuple(suffixes)\n",
    "\n",
    "    matches = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(suffixes_tuple):\n",
    "                matches.append(os.path.join(dirpath, filename))\n",
    "\n",
    "    return matches\n",
    "\n",
    "audio_files = gather_audio_files(audio_file_path, [\"wav\", \"aif\", \"aiff\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fbddd9-1dc8-43ae-9c23-9bcc953917a0",
   "metadata": {},
   "source": [
    "### Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec80cea-9743-4db4-8725-1fbe773e03a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d0a0f372e743ec879d883c5f877608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=6, description='Decoder Layer Count:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c605ef64949644b6b093bccad5be9fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=8, description='Decoder Head Count:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab00a89e7794e16bb1c6e7fa318d48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=512, description='Decoder Embed Dim:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2563646e3f5a4b1ba1c928d792920182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2048, description='Decoder Fordward Dim:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c36b67d23047aaaefcfd4dd558af7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.1, description='Decoder Dropout:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cfbdbc9cc4487da16a8ee21d5c32eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Save Weights', style=CheckboxStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c5ad714b5c40ad8430e30cf7918d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Load Weights', style=CheckboxStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1137cd8ba1459fba2eab701061239d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='results/weights/decoder_weights_epoch_200', description='Decoder Weights File:', style=TextStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoder_layer_count = 6\n",
    "decoder_head_count = 8\n",
    "decoder_embed_dim = 512\n",
    "decoder_ff_dim = 2048\n",
    "decoder_dropout = 0.1\n",
    "\n",
    "save_weights = True\n",
    "load_weights = False\n",
    "decoder_weights_file = \"results/weights/decoder_weights_epoch_200\"\n",
    "\n",
    "decoder_layer_count_gui = widgets.IntText(value=decoder_layer_count, description=\"Decoder Layer Count:\", style={'description_width': 'initial'})\n",
    "decoder_head_count_gui = widgets.IntText(value=decoder_head_count, description=\"Decoder Head Count:\", style={'description_width': 'initial'})\n",
    "decoder_embed_dim_gui = widgets.IntText(value=decoder_embed_dim, description=\"Decoder Embed Dim:\", style={'description_width': 'initial'})\n",
    "decoder_ff_dim_gui = widgets.IntText(value=decoder_ff_dim, description=\"Decoder Fordward Dim:\", style={'description_width': 'initial'})\n",
    "decoder_dropout_gui = widgets.FloatText(value=decoder_dropout, description=\"Decoder Dropout:\", style={'description_width': 'initial'})\n",
    "\n",
    "save_weights_gui = widgets.Checkbox(\n",
    "    value=save_weights,\n",
    "    description='Save Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "load_weights_gui = widgets.Checkbox(\n",
    "    value=load_weights,\n",
    "    description='Load Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "decoder_weights_file_gui = widgets.Text(value=decoder_weights_file, description=\"Decoder Weights File:\", style={'description_width': 'initial'}) \n",
    "\n",
    "display(decoder_layer_count_gui)\n",
    "display(decoder_head_count_gui)\n",
    "display(decoder_embed_dim_gui)\n",
    "display(decoder_ff_dim_gui)\n",
    "display(decoder_dropout_gui)\n",
    "display(save_weights_gui)\n",
    "display(load_weights_gui)\n",
    "display(decoder_weights_file_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43bb93c5-93b0-4f80-b737-86bc67795e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer_count = decoder_layer_count_gui.value\n",
    "decoder_head_count = decoder_head_count_gui.value\n",
    "decoder_embed_dim = decoder_embed_dim_gui.value\n",
    "decoder_ff_dim = decoder_ff_dim_gui.value\n",
    "decoder_dropout = decoder_dropout_gui.value\n",
    "save_weights = save_weights_gui.value\n",
    "load_weights = load_weights_gui.value\n",
    "decoder_weights_file = decoder_weights_file_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855005b-9506-4dea-9e5b-f61f20158ba3",
   "metadata": {},
   "source": [
    "### Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b314bfc0-11e3-4c3d-a76f-ee17452e0011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a5f7592f484afabc13ebf1a2f6a8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=32, description='Batch Size:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac62e97a06124217982a7d6885b64032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.1, description='Test Ratio:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1ff486506e4f80a32047baa1b36419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=64, description='Sequence Input Length:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d832dd47c24c31a6d1647ac480f1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=10, description='Sequence Output Length:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64852ff98fb146c29223e765a3ff1703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='Autoencoder Learning Rate:', style=DescriptionStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9380f08fd3004a81abbeb67cb648b420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='Teacher Forcing Probability:', style=DescriptionStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e796cde671483da5c8e4a18254bfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=10, description='Model Save Interval:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91dc6f9bbf4a47a6b7f945af53df5367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=200, description='Epochs:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "test_percentage = 0.1\n",
    "\n",
    "seq_input_length = 64\n",
    "seq_output_length = 10 # this is only used for non-teacher forcing scenarios\n",
    "\n",
    "learning_rate = 1e-4\n",
    "teacher_forcing_prob = 0.0\n",
    "model_save_interval = 10\n",
    "\n",
    "epochs = 200\n",
    "save_history = True\n",
    "\n",
    "batch_size_gui = widgets.IntText(value=batch_size, description=\"Batch Size:\", style={'description_width': 'initial'})\n",
    "test_percentage_gui = widgets.FloatText(value=test_percentage, description=\"Test Ratio:\", style={'description_width': 'initial'})\n",
    "seq_input_length_gui = widgets.IntText(value=seq_input_length, description=\"Sequence Input Length:\", style={'description_width': 'initial'})\n",
    "seq_output_length_gui = widgets.IntText(value=seq_output_length, description=\"Sequence Output Length:\", style={'description_width': 'initial'})\n",
    "learning_rate_gui = widgets.FloatText(value=learning_rate, description=\"Autoencoder Learning Rate:\", style={'description_width': 'initial'})\n",
    "teacher_forcing_prob_gui = widgets.FloatText(value=teacher_forcing_prob, description=\"Teacher Forcing Probability:\", style={'description_width': 'initial'})\n",
    "model_save_interval_gui = widgets.IntText(value=model_save_interval, description=\"Model Save Interval:\", style={'description_width': 'initial'})\n",
    "epochs_gui = widgets.IntText(value=epochs, description=\"Epochs:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(batch_size_gui)\n",
    "display(test_percentage_gui)\n",
    "display(seq_input_length_gui)\n",
    "display(seq_output_length_gui)\n",
    "display(learning_rate_gui)\n",
    "display(teacher_forcing_prob_gui)\n",
    "display(model_save_interval_gui)\n",
    "display(epochs_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab47374b-cf97-4732-8d98-4cb879e5b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = batch_size_gui.value\n",
    "test_percentage = test_percentage_gui.value\n",
    "seq_input_length = seq_input_length_gui.value\n",
    "seq_output_length = seq_output_length_gui.value\n",
    "learning_rate = learning_rate_gui.value\n",
    "teacher_forcing_prob = teacher_forcing_prob_gui.value\n",
    "model_save_interval = model_save_interval_gui.value\n",
    "epochs = epochs_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b89e4-45a1-4afc-b067-ffa8951b4d22",
   "metadata": {},
   "source": [
    "## Load Audio Data and Calculate Mel Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d5eaea-0f0f-48e1-af71-b2e093727a6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dbisig\\anaconda3\\envs\\ima2025\\lib\\site-packages\\vocos\\pretrained.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/BeeFlyBy_BU01.23.wav\n",
      "waveform_data s  torch.Size([1, 296192])\n",
      "audio_features s  torch.Size([1, 128, 1158])\n",
      "audio_features 2 s  torch.Size([1158, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/DS_SCB_fx_ambience_one_shot_walking_forest.wav\n",
      "waveform_data s  torch.Size([1, 1822400])\n",
      "audio_features s  torch.Size([1, 128, 7119])\n",
      "audio_features 2 s  torch.Size([7119, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/ESM_WNL_fx_birds_loop_song_musical_breeze_lively_forest_01.wav\n",
      "waveform_data s  torch.Size([1, 5056768])\n",
      "audio_features s  torch.Size([1, 128, 19754])\n",
      "audio_features 2 s  torch.Size([19754, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/ESM_WNL_fx_water_flow_loop_stream_forest_creek_quiet_insects_01.wav\n",
      "waveform_data s  torch.Size([1, 3799365])\n",
      "audio_features s  torch.Size([1, 128, 14842])\n",
      "audio_features 2 s  torch.Size([14842, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/EX_BUG_virtuoso_katydid_texture.wav\n",
      "waveform_data s  torch.Size([1, 589247])\n",
      "audio_features s  torch.Size([1, 128, 2302])\n",
      "audio_features 2 s  torch.Size([2302, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/EX_RE_perc_geofon_branch_thump.wav\n",
      "waveform_data s  torch.Size([1, 157946])\n",
      "audio_features s  torch.Size([1, 128, 617])\n",
      "audio_features 2 s  torch.Size([617, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/FF_AW_field_rec_dry_tree_branches.wav\n",
      "waveform_data s  torch.Size([1, 367804])\n",
      "audio_features s  torch.Size([1, 128, 1437])\n",
      "audio_features 2 s  torch.Size([1437, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/FootstepsLeaves_BW.7936.wav\n",
      "waveform_data s  torch.Size([1, 403712])\n",
      "audio_features s  torch.Size([1, 128, 1578])\n",
      "audio_features 2 s  torch.Size([1578, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/ForestRain_S08AM.37.wav\n",
      "waveform_data s  torch.Size([1, 5316096])\n",
      "audio_features s  torch.Size([1, 128, 20767])\n",
      "audio_features 2 s  torch.Size([20767, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/LBN_120_fx_rhythmic_nature_forest_walk.wav\n",
      "waveform_data s  torch.Size([1, 768000])\n",
      "audio_features s  torch.Size([1, 128, 3001])\n",
      "audio_features 2 s  torch.Size([3001, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/MNC_ASMR_Walking_On_Stones.wav\n",
      "waveform_data s  torch.Size([1, 429074])\n",
      "audio_features s  torch.Size([1, 128, 1677])\n",
      "audio_features 2 s  torch.Size([1677, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/NADERI_natural_branches_02.wav\n",
      "waveform_data s  torch.Size([1, 38104])\n",
      "audio_features s  torch.Size([1, 128, 149])\n",
      "audio_features 2 s  torch.Size([149, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/PSE_EFV1_FX_One_Shot_Foley_Cricket.wav\n",
      "waveform_data s  torch.Size([1, 1660134])\n",
      "audio_features s  torch.Size([1, 128, 6485])\n",
      "audio_features 2 s  torch.Size([6485, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/RiverForest_SFXB.142.wav\n",
      "waveform_data s  torch.Size([1, 5744896])\n",
      "audio_features s  torch.Size([1, 128, 22442])\n",
      "audio_features 2 s  torch.Size([22442, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/SCP_AP_perc_wood_carving_thunk_rip.wav\n",
      "waveform_data s  torch.Size([1, 45924])\n",
      "audio_features s  torch.Size([1, 128, 180])\n",
      "audio_features 2 s  torch.Size([180, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/SCP_MS_field_rec_birdsong_mallard_rugen.wav\n",
      "waveform_data s  torch.Size([1, 79122])\n",
      "audio_features s  torch.Size([1, 128, 310])\n",
      "audio_features 2 s  torch.Size([310, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/shs_oat_texture_forest_walk.wav\n",
      "waveform_data s  torch.Size([1, 795186])\n",
      "audio_features s  torch.Size([1, 128, 3107])\n",
      "audio_features 2 s  torch.Size([3107, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/SPLC-1078_FX_Oneshot_Thunder_Close_Loud.wav\n",
      "waveform_data s  torch.Size([1, 523285])\n",
      "audio_features s  torch.Size([1, 128, 2045])\n",
      "audio_features 2 s  torch.Size([2045, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/SPLC-1313_FX_Oneshot_Wood_Chopping_Axe.wav\n",
      "waveform_data s  torch.Size([1, 87552])\n",
      "audio_features s  torch.Size([1, 128, 343])\n",
      "audio_features 2 s  torch.Size([343, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/SPLC-3916_FX_Oneshot_Birds_Wing_Flaps_Pigeon.wav\n",
      "waveform_data s  torch.Size([1, 187904])\n",
      "audio_features s  torch.Size([1, 128, 735])\n",
      "audio_features 2 s  torch.Size([735, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/SPLC-3948_FX_Oneshot_Birds_Owl_Hooting_Reverb.wav\n",
      "waveform_data s  torch.Size([1, 206418])\n",
      "audio_features s  torch.Size([1, 128, 807])\n",
      "audio_features 2 s  torch.Size([807, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/SPLC-5810_FX_Oneshot_Animal_Leopard_Growls_Mouth_Purr.wav\n",
      "waveform_data s  torch.Size([1, 171654])\n",
      "audio_features s  torch.Size([1, 128, 671])\n",
      "audio_features 2 s  torch.Size([671, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/TreeBranchBreak_BW.18426.wav\n",
      "waveform_data s  torch.Size([1, 52224])\n",
      "audio_features s  torch.Size([1, 128, 205])\n",
      "audio_features 2 s  torch.Size([205, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/UMRU_rare_citi_crow.wav\n",
      "waveform_data s  torch.Size([1, 304941])\n",
      "audio_features s  torch.Size([1, 128, 1192])\n",
      "audio_features 2 s  torch.Size([1192, 128])\n",
      "audio file  ../../../../Data/Audio/Yannick_Leonie/48khz/WindForest_BW.59812.wav\n",
      "waveform_data s  torch.Size([1, 5766912])\n",
      "audio_features s  torch.Size([1, 128, 22528])\n",
      "audio_features 2 s  torch.Size([22528, 128])\n"
     ]
    }
   ],
   "source": [
    "vocos = Vocos.from_pretrained(\"kittn/vocos-mel-48khz-alpha1\")\n",
    "\n",
    "all_audio_features = []\n",
    "\n",
    "for audio_file_index in range(len(audio_files)):\n",
    "    \n",
    "    print(\"audio file \", audio_files[audio_file_index])\n",
    "    \n",
    "    audio_file = audio_files[audio_file_index]\n",
    "    \n",
    "    # load audio file\n",
    "    waveform_data, _ = torchaudio.load(audio_file_path + audio_file)\n",
    "    \n",
    "    print(\"waveform_data s \", waveform_data.shape)\n",
    "    \n",
    "    # audio features\n",
    "    audio_features = vocos.feature_extractor(waveform_data)\n",
    "    \n",
    "    print(\"audio_features s \", audio_features.shape)\n",
    "    \n",
    "    audio_features = audio_features.squeeze(0)\n",
    "    audio_features = torch.permute(audio_features, (1, 0))\n",
    "    \n",
    "    print(\"audio_features 2 s \", audio_features.shape)\n",
    "    \n",
    "    all_audio_features.append(audio_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd72ff-a629-490b-a804-421379804512",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e18f34c5-b7ae-487a-9a13-b7044c1c9f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_item s  torch.Size([64, 128])\n",
      "y_item s  torch.Size([10, 128])\n",
      "X_batch s  torch.Size([32, 64, 128])\n",
      "y_batch s  torch.Size([32, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "audio_features_dim = all_audio_features[0].shape[-1]\n",
    "\n",
    "audio_features_dim\n",
    "\n",
    "for audio_features in all_audio_features:\n",
    "    \n",
    "    total_sequence_length = audio_features.shape[0]\n",
    "    \n",
    "    for pI in range(total_sequence_length - seq_input_length - seq_output_length - 1):\n",
    "        X_sample = audio_features[pI:pI+seq_input_length]\n",
    "        X.append(X_sample)\n",
    "        \n",
    "        y_sample = audio_features[pI+seq_input_length:pI+seq_input_length+seq_output_length]\n",
    "        y.append(y_sample)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, ...], self.y[idx, ...]\n",
    "\n",
    "\n",
    "full_dataset = SequenceDataset(X, y)\n",
    "\n",
    "X_item, y_item = full_dataset[0]\n",
    "\n",
    "print(\"X_item s \", X_item.shape)\n",
    "print(\"y_item s \", y_item.shape)\n",
    "\n",
    "test_size = int(test_percentage * len(full_dataset))\n",
    "train_size = len(full_dataset) - test_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"X_batch s \", X_batch.shape)\n",
    "print(\"y_batch s \", y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ad599-1d2f-491a-8a35-8e8e1cc03c63",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317eb5b-67d7-4cd5-b17c-e8a7e829cf9d",
   "metadata": {},
   "source": [
    "## PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "395ed00e-f836-447e-b610-eb8bfddc29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bc645-f986-4f8e-ab4a-7782ba6eb753",
   "metadata": {},
   "source": [
    "## Create TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49439334-3e51-4e5e-944d-2387cb4ef9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoder(\n",
      "  (audio2embed): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (positional_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0-5): 6 x TransformerDecoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (multihead_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x TransformerDecoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (multihead_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (embed2audio): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n",
      "decoder_input s  torch.Size([32, 64, 128])\n",
      "decoder_output s  torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        audio_dim,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        num_decoder_layers,\n",
    "        ff_dim,\n",
    "        dropout_p,\n",
    "        pos_encoding_max_length\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.audio2embed = nn.Linear(audio_dim, embed_dim) # map audio data to embedding\n",
    "\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=embed_dim, dropout_p=dropout_p, max_len=pos_encoding_max_length\n",
    "        )\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout_p, batch_first=True)\n",
    "        #self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers = num_decoder_layers)\n",
    "\n",
    "        # build a decoder directly from TransformerDecoderLayer\n",
    "        # rather than using the nn.TransformerDecoder module which requires also a Transformer Encoder\n",
    "        self.decoder = self.layers = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=embed_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=ff_dim,\n",
    "                dropout=dropout_p,\n",
    "                activation='gelu',\n",
    "                batch_first=True\n",
    "            ) for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "        self.embed2audio = nn.Linear(embed_dim, audio_dim) # map embedding to audio data\n",
    "\n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "       \n",
    "    def forward(self, audio_data):\n",
    "        \n",
    "        #print(\"forward\")\n",
    "        \n",
    "        #print(\"audio_data s \", audio_data.shape)\n",
    "        \n",
    "        # dummy \"memory\" as zero (only self-attention is used)\n",
    "        memory = torch.zeros(audio_data.size(0), audio_data.size(1), self.embed_dim, device=audio_data.device)\n",
    "\n",
    "        #print(\"memory s \", memory.shape)\n",
    "\n",
    "        # Lower triangular matrix for autoregressive masking\n",
    "        tgt_mask = self.get_tgt_mask(audio_data.shape[1]).to(audio_data.device)\n",
    "\n",
    "        #print(\"tgt_mask s \", tgt_mask.shape)\n",
    "\n",
    "        audio_embedded = self.audio2embed(audio_data) * math.sqrt(self.embed_dim)\n",
    "        \n",
    "        #print(\"audio_embedded 1 s \", audio_embedded.shape)\n",
    "        \n",
    "        audio_embedded = self.positional_encoder(audio_embedded)\n",
    "        \n",
    "        #print(\"audio_embedded 2 s \", audio_embedded.shape)\n",
    "        \n",
    "        x = audio_embedded\n",
    "        \n",
    "        #print(\"x s \", x.shape)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            #print(\"x in s \", x.shape)\n",
    "            \n",
    "            x = layer(x, memory, tgt_mask=tgt_mask)\n",
    "            \n",
    "            #print(\"x out s \", x.shape)\n",
    "\n",
    "        decoder_out = x\n",
    "\n",
    "        out = self.embed2audio(decoder_out)\n",
    "        \n",
    "        out = out[:, -1, :] # only last time step \n",
    "        \n",
    "        return out\n",
    "\n",
    "decoder = TransformerDecoder(audio_dim=audio_features_dim,\n",
    "                          embed_dim=decoder_embed_dim, \n",
    "                          num_heads=decoder_head_count, \n",
    "                          num_decoder_layers=decoder_layer_count, \n",
    "                          ff_dim = decoder_ff_dim,\n",
    "                          dropout_p=decoder_dropout,\n",
    "                          pos_encoding_max_length=seq_input_length).to(device)\n",
    "\n",
    "print(decoder)\n",
    "\n",
    "if load_weights == True:\n",
    "    decoder.load_state_dict(torch.load(decoder_weights_file))\n",
    "\n",
    "# test transformer decoder\n",
    "x_batch, _ = next(iter(train_loader))\n",
    "\n",
    "decoder_input = x_batch.to(device)\n",
    "decoder_output = decoder(decoder_input)\n",
    "\n",
    "print(\"decoder_input s \", decoder_input.shape)\n",
    "print(\"decoder_output s \", decoder_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad4a02-b191-4515-926e-35618717a61a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e55d51-eb16-409d-9efd-6fef9b39aad1",
   "metadata": {},
   "source": [
    "### Create Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9706d24-6549-4523-86d2-eb5779978f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.336) # reduce the learning every 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2ec63-915f-4497-b2ac-44b996617b3d",
   "metadata": {},
   "source": [
    "### Create Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d9eb80a-70f0-441f-be8c-3a1d861212c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_loss = nn.MSELoss()\n",
    "\n",
    "def loss(y, yhat):\n",
    "    _rec_loss = rec_loss(yhat, y)\n",
    "\n",
    "    return _rec_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6587475-6f16-4920-a7d3-7a4578720369",
   "metadata": {},
   "source": [
    "### Create Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b022d6b-49d4-4ed7-92fb-9161187d2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_features, target_features, teacher_forcing):\n",
    "    \n",
    "    decoder.train()\n",
    "\n",
    "    #print(\"ar_train_step\")    \n",
    "    #print(\"teacher_forcing \", teacher_forcing)\n",
    "    #print(\"pose_sequences s \", pose_sequences.shape)\n",
    "    #print(\"target_poses s \", target_poses.shape)\n",
    "\n",
    "    #_input_poses = pose_sequences.detach().clone()    \n",
    "    _input_features = input_features  \n",
    "    output_features_length = target_features.shape[1]\n",
    "    \n",
    "    #print(\"output_features_length \", output_features_length)\n",
    "    \n",
    "    _pred_features_for_loss = []\n",
    "    _target_features_for_loss = []\n",
    "    \n",
    "    for o_i in range(1, output_features_length):\n",
    "        \n",
    "        #print(\"_input_features s \", _input_features.shape)\n",
    "        \n",
    "        _pred_features = decoder(_input_features)\n",
    "        _pred_features = torch.unsqueeze(_pred_features, axis=1)\n",
    "        \n",
    "        #print(\"_pred_features s \", _pred_features.shape)\n",
    "        \n",
    "        _target_features = target_features[:,o_i,:].detach().clone()\n",
    "        _target_features = torch.unsqueeze(_target_features, axis=1)\n",
    "\n",
    "        #print(\"_target_features s \", _target_features.shape)\n",
    "        \n",
    "        _pred_features_for_loss.append(_pred_features)\n",
    "        _target_features_for_loss.append(_target_features)\n",
    "        \n",
    "        # shift input feature seqeunce one feature to the right\n",
    "        # remove feature from beginning input feature sequence\n",
    "        # detach necessary to avoid error concerning running backprob a second time\n",
    "        _input_features = _input_features[:, 1:, :].detach().clone()\n",
    "        _target_features = _target_features.detach().clone()\n",
    "        _pred_features = _pred_features.detach().clone()\n",
    "        \n",
    "        # add predicted or target feature to end of input feature sequence\n",
    "        if teacher_forcing == True:\n",
    "            _input_features = torch.concat((_input_features, _target_features), axis=1)\n",
    "        else:\n",
    "            _input_features = torch.cat((_input_features, _pred_features), axis=1)\n",
    "            \n",
    "        #print(\"_input_features s \", _input_features.shape)\n",
    "\n",
    "        \n",
    "    _pred_features_for_loss = torch.cat(_pred_features_for_loss, dim=1)\n",
    "    _target_features_for_loss = torch.cat(_target_features_for_loss, dim=1)\n",
    "    \n",
    "    #print(\"_pred_features_for_loss 2 s \", _pred_features_for_loss.shape)\n",
    "    #print(\"_target_features_for_loss 2 s \", _target_features_for_loss.shape)\n",
    "    \n",
    "    _loss = loss(_target_features_for_loss, _pred_features_for_loss) \n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    _loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print(\"_ar_loss_total mean s \", _ar_loss_total.shape)\n",
    "    \n",
    "    #return _ar_loss, _ar_norm_loss, _ar_quat_loss\n",
    "    \n",
    "    return _loss\n",
    "\n",
    "def test_step(input_features, target_features, teacher_forcing):\n",
    "    \n",
    "    decoder.eval()\n",
    "\n",
    "    #print(\"ar_train_step\")    \n",
    "    #print(\"teacher_forcing \", teacher_forcing)\n",
    "    #print(\"pose_sequences s \", pose_sequences.shape)\n",
    "    #print(\"target_poses s \", target_poses.shape)\n",
    "\n",
    "    #_input_poses = pose_sequences.detach().clone()    \n",
    "    _input_features = input_features  \n",
    "    output_features_length = target_features.shape[1]\n",
    "    \n",
    "    #print(\"output_features_length \", output_features_length)\n",
    "    \n",
    "    _pred_features_for_loss = []\n",
    "    _target_features_for_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for o_i in range(1, output_features_length):\n",
    "            \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "            \n",
    "            _pred_features = decoder(_input_features)\n",
    "            _pred_features = torch.unsqueeze(_pred_features, axis=1)\n",
    "            \n",
    "            #print(\"_pred_features s \", _pred_features.shape)\n",
    "            \n",
    "            _target_features = target_features[:,o_i,:].detach().clone()\n",
    "            _target_features = torch.unsqueeze(_target_features, axis=1)\n",
    "    \n",
    "            #print(\"_target_features s \", _target_features.shape)\n",
    "            \n",
    "            _pred_features_for_loss.append(_pred_features)\n",
    "            _target_features_for_loss.append(_target_features)\n",
    "            \n",
    "            # shift input feature seqeunce one feature to the right\n",
    "            # remove feature from beginning input feature sequence\n",
    "            # detach necessary to avoid error concerning running backprob a second time\n",
    "            _input_features = _input_features[:, 1:, :].detach().clone()\n",
    "            _target_features = _target_features.detach().clone()\n",
    "            _pred_features = _pred_features.detach().clone()\n",
    "            \n",
    "            # add predicted or target feature to end of input feature sequence\n",
    "            if teacher_forcing == True:\n",
    "                _input_features = torch.concat((_input_features, _target_features), axis=1)\n",
    "            else:\n",
    "                _input_features = torch.cat((_input_features, _pred_features), axis=1)\n",
    "                \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "\n",
    "        \n",
    "    _pred_features_for_loss = torch.cat(_pred_features_for_loss, dim=1)\n",
    "    _target_features_for_loss = torch.cat(_target_features_for_loss, dim=1)\n",
    "    \n",
    "    #print(\"_pred_features_for_loss 2 s \", _pred_features_for_loss.shape)\n",
    "    #print(\"_target_features_for_loss 2 s \", _target_features_for_loss.shape)\n",
    "    \n",
    "    _loss = loss(_target_features_for_loss, _pred_features_for_loss) \n",
    "    \n",
    "    #print(\"_ar_loss_total mean s \", _ar_loss_total.shape)\n",
    "    \n",
    "    #return _ar_loss, _ar_norm_loss, _ar_quat_loss\n",
    "    \n",
    "    decoder.eval()\n",
    "    \n",
    "    return _loss\n",
    "\n",
    "def train(train_dataloader, test_dataloader, epochs):\n",
    "    \n",
    "    loss_history = {}\n",
    "    loss_history[\"train\"] = []\n",
    "    loss_history[\"test\"] = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        _train_loss_per_epoch = []\n",
    "\n",
    "        for train_batch in train_dataloader:\n",
    "            input_feature_sequences = train_batch[0].to(device)\n",
    "            target_features = train_batch[1].to(device)\n",
    "            \n",
    "            use_teacher_forcing = np.random.uniform() < teacher_forcing_prob\n",
    "            \n",
    "            _loss = train_step(input_feature_sequences, target_features, use_teacher_forcing)\n",
    "            \n",
    "            _loss = _loss.detach().cpu().numpy()\n",
    "            \n",
    "            _train_loss_per_epoch.append(_loss)\n",
    "\n",
    "        _train_loss_per_epoch = np.mean(np.array(_train_loss_per_epoch))\n",
    "\n",
    "        _test_loss_per_epoch = []\n",
    "        \n",
    "        for test_batch in test_dataloader:\n",
    "            input_feature_sequences = train_batch[0].to(device)\n",
    "            target_features = train_batch[1].to(device)\n",
    "            \n",
    "            use_teacher_forcing = np.random.uniform() < teacher_forcing_prob\n",
    "            \n",
    "            _loss = test_step(input_feature_sequences, target_features, use_teacher_forcing)\n",
    "            \n",
    "            _loss = _loss.detach().cpu().numpy()\n",
    "            \n",
    "            _test_loss_per_epoch.append(_loss)\n",
    "        \n",
    "        _test_loss_per_epoch = np.mean(np.array(_test_loss_per_epoch))\n",
    "        \n",
    "        if epoch % model_save_interval == 0 and save_weights == True:\n",
    "            torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epoch))\n",
    "        \n",
    "        loss_history[\"train\"].append(_train_loss_per_epoch)\n",
    "        loss_history[\"test\"].append(_test_loss_per_epoch)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print ('epoch {} : train: {:01.4f} test: {:01.4f} time {:01.2f}'.format(epoch + 1, _train_loss_per_epoch, _test_loss_per_epoch, time.time()-start))\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2ec12a-9a28-4337-8f4e-d406e9b21698",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86c406-30fe-489a-862f-3e0ec7409024",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = train(train_loader, test_loader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18addee-05b8-4a56-a8d3-9df62b70ddb9",
   "metadata": {},
   "source": [
    "### Save Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df46126-9c20-455b-8295-f73d22d1c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss_as_image(loss_history, image_file_name):\n",
    "    keys = list(loss_history.keys())\n",
    "    epochs = len(loss_history[keys[0]])\n",
    "    \n",
    "    for key in keys:\n",
    "        plt.plot(range(epochs), loss_history[key], label=key)\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(image_file_name)\n",
    "    plt.show()\n",
    "\n",
    "def save_loss_as_csv(loss_history, csv_file_name):\n",
    "    with open(csv_file_name, 'w') as csv_file:\n",
    "        csv_columns = list(loss_history.keys())\n",
    "        csv_row_count = len(loss_history[csv_columns[0]])\n",
    "        \n",
    "        \n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=',', lineterminator='\\n')\n",
    "        csv_writer.writeheader()\n",
    "    \n",
    "        for row in range(csv_row_count):\n",
    "        \n",
    "            csv_row = {}\n",
    "        \n",
    "            for key in loss_history.keys():\n",
    "                csv_row[key] = loss_history[key][row]\n",
    "\n",
    "            csv_writer.writerow(csv_row)\n",
    "\n",
    "\n",
    "save_loss_as_csv(loss_history, \"results/histories/rnn_history_{}.csv\".format(epochs))\n",
    "save_loss_as_image(loss_history, \"results/histories/rnn_history_{}.png\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7fb07b-f864-4458-ba0e-346e1833a238",
   "metadata": {},
   "source": [
    "### Save Final Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a8ed8-a639-4ad7-b7c0-b891aa599856",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd12001-f0b2-4e05-8851-563fb26c2ee8",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b37fa1-acb1-4608-b0cf-feef6e411d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()\n",
    "\n",
    "def export_orig_audio(waveform_data, start_time, end_time, file_name):\n",
    "    \n",
    "    start_time_samples = int(start_time * audio_sample_rate)\n",
    "    end_time_samples = int(end_time * audio_sample_rate)\n",
    "    \n",
    "    torchaudio.save(file_name, waveform_data[:, start_time_samples:end_time_samples], audio_sample_rate)\n",
    "\n",
    "def export_ref_audio(waveform_data, start_time, end_time, file_name):\n",
    "    \n",
    "    start_time_samples = int(start_time * audio_sample_rate)\n",
    "    end_time_samples = int(end_time * audio_sample_rate)\n",
    "    \n",
    "    # audio features\n",
    "    audio_features = vocos.feature_extractor(waveform_data[:, start_time_samples:end_time_samples])\n",
    "    \n",
    "    ref_audio = vocos.decode(audio_features)\n",
    "    \n",
    "    torchaudio.save(file_name, ref_audio.detach().cpu(), audio_sample_rate)\n",
    "    \n",
    "\n",
    "def export_pred_audio(waveform_data, start_time, end_time, file_name):\n",
    "    \n",
    "    start_time_samples = int(start_time * audio_sample_rate)\n",
    "    end_time_samples = int(end_time * audio_sample_rate)\n",
    "    \n",
    "    # audio features\n",
    "    audio_features = vocos.feature_extractor(waveform_data[:, start_time_samples:end_time_samples])\n",
    "    \n",
    "    #print(\"audio_features s \", audio_features.shape)\n",
    "    \n",
    "    audio_features = audio_features.squeeze(0)\n",
    "    audio_features = torch.permute(audio_features, (1, 0))\n",
    "    audio_feature_count = audio_features.shape[0]\n",
    "    \n",
    "    #print(\"audio_feature_count \", audio_feature_count)\n",
    "    \n",
    "    input_features = audio_features[:seq_input_length]\n",
    "    input_features = input_features.unsqueeze(0)\n",
    "    \n",
    "    output_features_length = audio_feature_count - seq_input_length\n",
    "    \n",
    "    #print(\"output_features_length \", output_features_length)\n",
    "    \n",
    "    _input_features = input_features  \n",
    "    pred_features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for o_i in range(1, output_features_length):\n",
    "            \n",
    "            _input_features = _input_features.to(device)\n",
    "            \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "            \n",
    "            _pred_features = decoder(_input_features)\n",
    "            _pred_features = torch.unsqueeze(_pred_features, axis=1)\n",
    "\n",
    "            _input_features = _input_features[:, 1:, :].detach().clone()\n",
    "            _pred_features = _pred_features.detach().clone()\n",
    "            \n",
    "            pred_features.append(_pred_features.cpu())\n",
    "            \n",
    "            _input_features = torch.cat((_input_features, _pred_features), axis=1)\n",
    "                \n",
    "            #print(\"_input_features s \", _input_features.shape)\n",
    "            \n",
    "    pred_features = torch.cat(pred_features, axis=1)\n",
    "    pred_features = torch.permute(pred_features, (0, 2, 1))\n",
    "    pred_audio = vocos.decode(pred_features)\n",
    "    \n",
    "    torchaudio.save(file_name, pred_audio.detach().cpu(), audio_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1858b39-d518-49b2-91a9-b90e0ce95dfb",
   "metadata": {},
   "source": [
    "### Perform Audio Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c4a81-94cd-45e8-b9dc-71fbbf382921",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"../../../../Data/Audio/Gutenberg/Night_and_Day_by_Virginia_Woolf_48khz.wav\"\n",
    "audio_start_time_sec = 60.0\n",
    "audio_end_time_sec = 100.0\n",
    "\n",
    "audio_file_gui = widgets.Text(value=audio_file, description=\"Audio File:\", style={'description_width': 'initial'})\n",
    "audio_start_time_sec_gui = widgets.FloatText(value=audio_start_time_sec, description=\"Audio Start Time [Seconds]:\", style={'description_width': 'initial'})\n",
    "audio_end_time_sec_gui = widgets.FloatText(value=audio_end_time_sec, description=\"Audio End Time [Seconds]\", style={'description_width': 'initial'})\n",
    "\n",
    "display(audio_file_gui)\n",
    "display(audio_start_time_sec_gui)\n",
    "display(audio_end_time_sec_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2bec8-5902-464c-8269-76e86c1e7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = audio_file_gui.value\n",
    "audio_start_time_sec = audio_start_time_sec_gui.value\n",
    "audio_end_time_sec = audio_end_time_sec_gui.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab301dd6-85cc-4fd8-b184-f7f11ef0ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_data, _ = torchaudio.load(audio_file)\n",
    "\n",
    "export_orig_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/orig_{}-{}.wav\".format(audio_start_time_sec, audio_end_time_sec))\n",
    "export_ref_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/ref_{}-{}.wav\".format(audio_start_time_sec, audio_end_time_sec))\n",
    "export_pred_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/pred_{}-{}_epoch_{}.wav\".format(audio_start_time_sec, audio_end_time_sec, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5e5dd1-4948-45d7-94ad-0ccb1ffee38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_start_time_sec = 0.0\n",
    "audio_end_time_sec = 20.0\n",
    "\n",
    "for audio_file_index in range(len(audio_files)):\n",
    "    audio_file = audio_files[audio_file_index]\n",
    "    waveform_data, _ = torchaudio.load(audio_file_path + audio_file)\n",
    "\n",
    "    export_orig_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/orig_{}_{}-{}.wav\".format(audio_file, audio_start_time_sec, audio_end_time_sec))\n",
    "    export_ref_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/ref_{}_{}-{}.wav\".format(audio_file, audio_start_time_sec, audio_end_time_sec))\n",
    "    export_pred_audio(waveform_data, audio_start_time_sec, audio_end_time_sec, \"results/audio/pred_{}_{}-{}_epoch_{}.wav\".format(audio_file, audio_start_time_sec, audio_end_time_sec, epochs))y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
