{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf0c862-18d5-4355-bd3b-d884d333c69c",
   "metadata": {},
   "source": [
    "# Audio Autoencoder (RNN Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b84300-6339-4f17-b3a8-e6cd1ae11eea",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8041f-9c87-4c59-b621-e66be00cafae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "import torchaudio\n",
    "import simpleaudio as sa\n",
    "import numpy as np\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import os, time\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from vocos import Vocos\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a8d7b-b3a1-4bb6-adda-74229bbe52d5",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e081551d-ad45-4aae-86f9-5b2863d0ea87",
   "metadata": {},
   "source": [
    "## Compute Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53149b-6deb-434e-bd33-7df4f42e0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f652c9b-172c-4400-a2ee-c975e20a6299",
   "metadata": {},
   "source": [
    "## Audio Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b10ba9-b15c-45b7-a513-290eaac1869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = \"../../../Data/Audio/Gutenberg/\"\n",
    "audio_files = [\"Night_and_Day_by_Virginia_Woolf_48khz.wav\"]\n",
    "audio_sample_rate = 48000 # numer of audio samples per sec\n",
    "audio_channel_count = 1\n",
    "audio_window_length = 2048 # this results in 9 mel spectra\n",
    "\n",
    "audio_file_path_gui = widgets.Text(value=audio_file_path, description=\"Audio File Path:\", style={'description_width': 'initial'}) \n",
    "\n",
    "audio_files_gui = widgets.Textarea(\n",
    "    value=','.join(audio_files),\n",
    "    placeholder='Enter file names separated by commas',\n",
    "    description='Audio Files:',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "audio_sample_rate_gui = widgets.IntText(value=audio_sample_rate, description=\"Audio Sample Rate:\", style={'description_width': 'initial'})\n",
    "audio_channel_count_gui = widgets.IntText(value=audio_channel_count, description=\"Audio Channel Count:\", style={'description_width': 'initial'})\n",
    "audio_window_length_gui = widgets.IntText(value=audio_window_length, description=\"Audio Window Length:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(audio_file_path_gui)\n",
    "display(audio_files_gui)\n",
    "display(audio_sample_rate_gui)\n",
    "display(audio_channel_count_gui)\n",
    "display(audio_window_length_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205c5d7-71f8-4607-ac91-4a36b77c9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = audio_file_path_gui.value\n",
    "audio_files = re.split(r'\\s*,\\s*', audio_files_gui.value)\n",
    "audio_sample_rate = audio_sample_rate_gui.value\n",
    "audio_channel_count = audio_channel_count_gui.value\n",
    "audio_window_length = audio_window_length_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb003c7-f3bd-4cb0-acf4-e05fcef60e00",
   "metadata": {},
   "source": [
    "## Autoencoder Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b447e072-b9fa-4b7c-8c58-fb57235c7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "sequence_length = None # will be calculate automatically\n",
    "ae_rnn_layer_count = 2\n",
    "ae_rnn_layer_size = 512\n",
    "ae_dense_layer_sizes = [ 512 ]\n",
    "\n",
    "save_weights = True\n",
    "load_weights = False\n",
    "encoder_weights_file = \"results/weights/encoder_weights_epoch_400\"\n",
    "decoder_weights_file = \"results/weights/decoder_weights_epoch_400\"\n",
    "\n",
    "latent_dim_gui = widgets.IntText(value=latent_dim, description=\"Latent Dimension:\", style={'description_width': 'initial'})\n",
    "ae_rnn_layer_count_gui = widgets.IntText(value=ae_rnn_layer_count, description=\"LSTM Layer Count:\", style={'description_width': 'initial'})\n",
    "ae_rnn_layer_size_gui = widgets.IntText(value=ae_rnn_layer_size, description=\"LSTM Layer Size:\", style={'description_width': 'initial'})\n",
    "\n",
    "ae_dense_layer_sizes_gui = widgets.Textarea(\n",
    "    value=','.join(list(map(str, ae_dense_layer_sizes))),\n",
    "    placeholder='Enter dense layer sizes separated by commas',\n",
    "    description='Dense Layer Sizes:',\n",
    "    layout=widgets.Layout(width='50%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "save_weights_gui = widgets.Checkbox(\n",
    "    value=save_weights,\n",
    "    description='Save Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "load_weights_gui = widgets.Checkbox(\n",
    "    value=load_weights,\n",
    "    description='Load Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "encoder_weights_file_gui = widgets.Text(value=encoder_weights_file, description=\"Encoder Weights File:\", style={'description_width': 'initial'}) \n",
    "decoder_weights_file_gui = widgets.Text(value=decoder_weights_file, description=\"Decoder Weights File:\", style={'description_width': 'initial'}) \n",
    "\n",
    "display(latent_dim_gui)\n",
    "display(ae_rnn_layer_count_gui)\n",
    "display(ae_rnn_layer_size_gui)\n",
    "display(ae_dense_layer_sizes_gui)\n",
    "display(save_weights_gui)\n",
    "display(load_weights_gui)\n",
    "display(encoder_weights_file_gui)\n",
    "display(decoder_weights_file_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf6272-0a55-41e6-be76-81dbe3a70994",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = latent_dim_gui.value\n",
    "ae_rnn_layer_count = ae_rnn_layer_count_gui.value\n",
    "ae_rnn_layer_size = ae_rnn_layer_size_gui.value\n",
    "ae_dense_layer_sizes  = [int(s) for s in re.split(r\"\\s*,\\s*\", ae_dense_layer_sizes_gui.value) if s.strip()]\n",
    "save_weights = save_weights_gui.value\n",
    "load_weights = load_weights_gui.value\n",
    "encoder_weights_file = encoder_weights_file_gui.value\n",
    "decoder_weights_file = decoder_weights_file_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534771a6-d3af-46b6-a18e-ec0305a35505",
   "metadata": {},
   "source": [
    "## Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba3484-5f9c-4784-af4c-27f4faea61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_count = 100000\n",
    "batch_size = 32\n",
    "\n",
    "ae_learning_rate = 1e-4\n",
    "ae_rec_loss_scale = 5.0\n",
    "ae_beta = 0.0 # will be calculated\n",
    "ae_beta_cycle_duration = 100\n",
    "ae_beta_min_const_duration = 20\n",
    "ae_beta_max_const_duration = 20\n",
    "ae_min_beta = 0.0\n",
    "ae_max_beta = 0.1\n",
    "\n",
    "epochs = 400\n",
    "model_save_interval = 50\n",
    "save_history = True\n",
    "\n",
    "data_count_gui = widgets.IntText(value=data_count, description=\"Dataset Size:\", style={'description_width': 'initial'})\n",
    "batch_size_gui = widgets.IntText(value=batch_size, description=\"Batch Size:\", style={'description_width': 'initial'})\n",
    "ae_learning_rate_gui = widgets.FloatText(value=ae_learning_rate, description=\"Autoencoder Learning Rate:\", style={'description_width': 'initial'})\n",
    "ae_rec_loss_scale_gui = widgets.FloatText(value=ae_rec_loss_scale, description=\"Autoencoder Reconstruction Loss Scale:\", style={'description_width': 'initial'})\n",
    "ae_min_beta_gui = widgets.FloatText(value=ae_min_beta, description=\"Minimum Beta Factor:\", style={'description_width': 'initial'})\n",
    "ae_max_beta_gui = widgets.FloatText(value=ae_max_beta, description=\"Maximum Beta Factor:\", style={'description_width': 'initial'})\n",
    "ae_beta_cycle_duration_gui = widgets.IntText(value=ae_beta_cycle_duration, description=\"Cycle Duration for Beta Factor:\", style={'description_width': 'initial'})\n",
    "ae_beta_min_const_duration_gui = widgets.IntText(value=ae_beta_min_const_duration, description=\"Duration for Minimum Beta Factor:\", style={'description_width': 'initial'})\n",
    "ae_beta_max_const_duration_gui = widgets.IntText(value=ae_beta_min_const_duration, description=\"Duration for Maximum Beta Factor:\", style={'description_width': 'initial'})\n",
    "epochs_gui = widgets.IntText(value=epochs, description=\"Number of Training Epochs:\", style={'description_width': 'initial'})\n",
    "model_save_interval_gui = widgets.IntText(value=model_save_interval, description=\"Model Save Interval:\", style={'description_width': 'initial'})\n",
    "\n",
    "save_history_gui = widgets.Checkbox(\n",
    "    value=save_history,\n",
    "    description='Save Training History',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(data_count_gui)\n",
    "display(batch_size_gui)\n",
    "display(ae_learning_rate_gui)\n",
    "display(ae_rec_loss_scale_gui)\n",
    "display(ae_min_beta_gui)\n",
    "display(ae_max_beta_gui)\n",
    "display(ae_beta_cycle_duration_gui)\n",
    "display(ae_beta_min_const_duration_gui)\n",
    "display(ae_beta_max_const_duration_gui)\n",
    "display(epochs_gui)\n",
    "display(model_save_interval_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e1fdfe-b6ad-440e-97d6-036a417e6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_count = data_count_gui.value\n",
    "batch_size = batch_size_gui.value\n",
    "ae_learning_rate = ae_learning_rate_gui.value\n",
    "ae_rec_loss_scale = ae_rec_loss_scale_gui.value\n",
    "ae_beta_cycle_duration = ae_beta_cycle_duration_gui.value\n",
    "ae_beta_min_const_duration = ae_beta_min_const_duration_gui.value\n",
    "ae_beta_max_const_duration = ae_beta_max_const_duration_gui.value\n",
    "ae_min_beta = ae_min_beta_gui.value\n",
    "ae_max_beta = ae_max_beta_gui.value\n",
    "epochs = epochs_gui.value\n",
    "model_save_interval = model_save_interval_gui.value\n",
    "save_history = save_history_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa14b8b-7a1d-47e0-929e-3d30ca3735b0",
   "metadata": {},
   "source": [
    "## Create Vocoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6fe13d-3dc5-43c5-9ddf-6ea294acbb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocos = Vocos.from_pretrained(\"kittn/vocos-mel-48khz-alpha1\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09373480-fa2c-4192-b21d-efe96f31b075",
   "metadata": {},
   "source": [
    "## Determine Number of Mel Filters and Mel Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd84bc3-477b-4d98-b9eb-3901481d1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocoder_features = vocos.feature_extractor(torch.rand(size=(1, audio_window_length), dtype=torch.float32).to(device))\n",
    "mel_count = vocoder_features.shape[-1]\n",
    "mel_filter_count = vocoder_features.shape[1]\n",
    "sequence_length = mel_count\n",
    "\n",
    "print(\"audio_window_length \", audio_window_length, \" mel_count \", mel_count, \" mel_filter_count \", mel_filter_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d1940-be77-416c-a02e-565e7075e13a",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0431bfa-8227-448d-8556-9dbc4337595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_file_path, audio_files, audio_window_length, audio_data_count):\n",
    "        self.audio_file_path = audio_file_path\n",
    "        self.audio_files = audio_files\n",
    "        self.audio_window_length = audio_window_length\n",
    "        self.audio_data_count = audio_data_count\n",
    "        \n",
    "        self.audio_waveforms = []\n",
    "        \n",
    "        for audio_file in self.audio_files:\n",
    "            audio_waveform, _ = torchaudio.load(self.audio_file_path + \"/\" + audio_file)\n",
    "            self.audio_waveforms.append(audio_waveform)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.audio_data_count\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        audio_index = torch.randint(0, len(self.audio_waveforms), size=(1,))\n",
    "        audio_waveform = self.audio_waveforms[audio_index]\n",
    "        \n",
    "        audio_length = audio_waveform.shape[1]\n",
    "        audio_excerpt_start = torch.randint(0, audio_length - self.audio_window_length, size=(1,))\n",
    "        audio_excerpt = audio_waveform[:, audio_excerpt_start:audio_excerpt_start+audio_window_length]\n",
    "        audio_excerpt = audio_excerpt[0]\n",
    "        \n",
    "        return audio_excerpt\n",
    "\n",
    "\n",
    "full_dataset = AudioDataset(audio_file_path, audio_files, audio_window_length, data_count)\n",
    "dataset_size = len(full_dataset)\n",
    "\n",
    "data_item = full_dataset[0]\n",
    "\n",
    "print(\"data_item s \", data_item.shape)\n",
    "\n",
    "dataloader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "batch_x = next(iter(dataloader))\n",
    "\n",
    "print(\"batch_x s \", batch_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775389d9-1d3e-4f55-a824-8487798a74f8",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae531e8-4cec-442d-a30b-7c89436775ef",
   "metadata": {},
   "source": [
    "## Create Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764960dc-978f-4a22-9865-19c82fa2bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, sequence_length, mel_filter_count, latent_dim, rnn_layer_count, rnn_layer_size, dense_layer_sizes):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.mel_filter_count = mel_filter_count\n",
    "        self.latent_dim = latent_dim\n",
    "        self.rnn_layer_count = rnn_layer_count\n",
    "        self.rnn_layer_size = rnn_layer_size \n",
    "        self.dense_layer_sizes = dense_layer_sizes\n",
    "    \n",
    "        # create recurrent layers\n",
    "        rnn_layers = []\n",
    "        rnn_layers.append((\"encoder_rnn_0\", nn.LSTM(self.mel_filter_count, self.rnn_layer_size, self.rnn_layer_count, batch_first=True)))\n",
    "        \n",
    "        self.rnn_layers = nn.Sequential(OrderedDict(rnn_layers))\n",
    "        \n",
    "        # create dense layers\n",
    "        \n",
    "        dense_layers = []\n",
    "        \n",
    "        dense_layers.append((\"encoder_dense_0\", nn.Linear(self.rnn_layer_size, self.dense_layer_sizes[0])))\n",
    "        dense_layers.append((\"encoder_dense_relu_0\", nn.ReLU()))\n",
    "        \n",
    "        dense_layer_count = len(self.dense_layer_sizes)\n",
    "        for layer_index in range(1, dense_layer_count):\n",
    "            dense_layers.append((\"encoder_dense_{}\".format(layer_index), nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index])))\n",
    "            dense_layers.append((\"encoder_dense_relu_{}\".format(layer_index), nn.ReLU()))\n",
    "            \n",
    "        self.dense_layers = nn.Sequential(OrderedDict(dense_layers))\n",
    "        \n",
    "        # create final dense layers\n",
    "            \n",
    "        self.fc_mu = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n",
    "        self.fc_std = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"x 1 \", x.shape)\n",
    "        \n",
    "        x, (_, _) = self.rnn_layers(x)\n",
    "        \n",
    "        #print(\"x 2 \", x.shape)\n",
    "        \n",
    "        x = x[:, -1, :] # only last time step \n",
    "        \n",
    "        #print(\"x 3 \", x.shape)\n",
    "        \n",
    "        x = self.dense_layers(x)\n",
    "        \n",
    "        #print(\"x 3 \", x.shape)\n",
    "        \n",
    "        mu = self.fc_mu(x)\n",
    "        std = self.fc_std(x)\n",
    "        \n",
    "        #print(\"mu s \", mu.shape, \" lvar s \", log_var.shape)\n",
    "    \n",
    "        return mu, std\n",
    "\n",
    "    def reparameterize(self, mu, std):\n",
    "        z = mu + std*torch.randn_like(std)\n",
    "        return z\n",
    "    \n",
    "encoder = Encoder(sequence_length, mel_filter_count, latent_dim, ae_rnn_layer_count, ae_rnn_layer_size, ae_dense_layer_sizes).to(device)\n",
    "\n",
    "print(encoder)\n",
    "\n",
    "if load_weights and encoder_weights_file:\n",
    "    encoder.load_state_dict(torch.load(encoder_weights_file, map_location=device))\n",
    "\n",
    "# test encoder\n",
    "audio_batch = next(iter(dataloader)).to(device)\n",
    "audio_batch_mels = vocos.feature_extractor(audio_batch.unsqueeze(1))\n",
    "audio_encoder_in = audio_batch_mels.squeeze(1).permute((0, 2, 1))\n",
    "audio_encoder_out_mu, audio_encoder_out_std = encoder(audio_encoder_in)\n",
    "audio_encoder_out = encoder.reparameterize(audio_encoder_out_mu, audio_encoder_out_std)\n",
    "\n",
    "print(\"audio_batch s \", audio_batch.shape)\n",
    "print(\"audio_batch_mels s \", audio_batch_mels.shape)\n",
    "print(\"audio_encoder_in s \", audio_encoder_in.shape)\n",
    "print(\"audio_encoder_out_mu s \", audio_encoder_out_mu.shape)\n",
    "print(\"audio_encoder_out_std s \", audio_encoder_out_std.shape)\n",
    "print(\"audio_encoder_out s \", audio_encoder_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe293f-61c8-4d66-b0d6-79e336b3cac9",
   "metadata": {},
   "source": [
    "## Create Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2e46f-0a8e-4a80-90bd-35443fd59dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, sequence_length, mel_filter_count, latent_dim, rnn_layer_count, rnn_layer_size, dense_layer_sizes):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.mel_filter_count = mel_filter_count\n",
    "        self.latent_dim = latent_dim\n",
    "        self.rnn_layer_size = rnn_layer_size\n",
    "        self.rnn_layer_count = rnn_layer_count\n",
    "        self.dense_layer_sizes = dense_layer_sizes\n",
    "\n",
    "        # create dense layers\n",
    "        dense_layers = []\n",
    "        \n",
    "        dense_layers.append((\"decoder_dense_0\", nn.Linear(latent_dim, self.dense_layer_sizes[0])))\n",
    "        dense_layers.append((\"decoder_relu_0\", nn.ReLU()))\n",
    "\n",
    "        dense_layer_count = len(self.dense_layer_sizes)\n",
    "        for layer_index in range(1, dense_layer_count):\n",
    "            dense_layers.append((\"decoder_dense_{}\".format(layer_index), nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index])))\n",
    "            dense_layers.append((\"decoder_dense_relu_{}\".format(layer_index), nn.ReLU()))\n",
    " \n",
    "        self.dense_layers = nn.Sequential(OrderedDict(dense_layers))\n",
    "        \n",
    "        # create rnn layers\n",
    "        rnn_layers = []\n",
    "\n",
    "        rnn_layers.append((\"decoder_rnn_0\", nn.LSTM(self.dense_layer_sizes[-1], self.rnn_layer_size, self.rnn_layer_count, batch_first=True)))\n",
    "        \n",
    "        self.rnn_layers = nn.Sequential(OrderedDict(rnn_layers))\n",
    "        \n",
    "        # final output dense layer\n",
    "        final_layers = []\n",
    "        \n",
    "        final_layers.append((\"decoder_dense_{}\".format(dense_layer_count), nn.Linear(self.rnn_layer_size, self.mel_filter_count)))\n",
    "        \n",
    "        self.final_layers = nn.Sequential(OrderedDict(final_layers))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"x 1 \", x.size())\n",
    "        \n",
    "        # dense layers\n",
    "        x = self.dense_layers(x)\n",
    "        #print(\"x 2 \", x.size())\n",
    "        \n",
    "        # repeat vector\n",
    "        x = torch.unsqueeze(x, dim=1)\n",
    "        x = x.repeat(1, sequence_length, 1)\n",
    "        #print(\"x 3 \", x.size())\n",
    "        \n",
    "        # rnn layers\n",
    "        x, (_, _) = self.rnn_layers(x)\n",
    "        #print(\"x 4 \", x.size())\n",
    "        \n",
    "        # final time distributed dense layer\n",
    "        x_reshaped = x.contiguous().view(-1, self.rnn_layer_size)  # (batch_size * sequence, input_size)\n",
    "        #print(\"x 5 \", x_reshaped.size())\n",
    "        \n",
    "        yhat = self.final_layers(x_reshaped)\n",
    "        #print(\"yhat 1 \", yhat.size())\n",
    "        \n",
    "        yhat = yhat.contiguous().view(-1, self.sequence_length, self.mel_filter_count)\n",
    "        #print(\"yhat 2 \", yhat.size())\n",
    "\n",
    "        return yhat\n",
    "\n",
    "ae_dense_layer_sizes_reversed = ae_dense_layer_sizes.copy()\n",
    "ae_dense_layer_sizes_reversed.reverse()\n",
    "\n",
    "decoder = Decoder(sequence_length, mel_filter_count, latent_dim, ae_rnn_layer_count, ae_rnn_layer_size, ae_dense_layer_sizes_reversed).to(device)\n",
    "\n",
    "print(decoder)\n",
    "\n",
    "if load_weights and decoder_weights_file:\n",
    "    decoder.load_state_dict(torch.load(decoder_weights_file, map_location=device))\n",
    "\n",
    "# test decoder\n",
    "audio_decoder_in = audio_encoder_out\n",
    "audio_decoder_out = decoder(audio_decoder_in)\n",
    "audio_features = audio_decoder_out.permute((0, 2, 1))\n",
    "\n",
    "audio_features = audio_features.squeeze(1)\n",
    "audio_batch = vocos.decode(audio_features)\n",
    "\n",
    "print(\"audio_decoder_in s \", audio_decoder_in.shape)\n",
    "print(\"audio_decoder_out s \", audio_decoder_out.shape)\n",
    "print(\"audio_features s \", audio_features.shape)\n",
    "print(\"audio_batch s \", audio_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c0b48-829d-47aa-8542-be4592c96f4c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32706cf1-367d-4b1e-adf1-5814b71ed2f3",
   "metadata": {},
   "source": [
    "## Create Beta Factor Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631aaafb-fa70-40c3-9b24-ad23002cdc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ae_beta_values():\n",
    "    \n",
    "    ae_beta_values = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        cycle_step = e % ae_beta_cycle_duration\n",
    "        \n",
    "        #print(\"cycle_step \", cycle_step)\n",
    "\n",
    "        if cycle_step < ae_beta_min_const_duration:\n",
    "            ae_beta_value = ae_min_beta\n",
    "            ae_beta_values.append(ae_beta_value)\n",
    "        elif cycle_step > ae_beta_cycle_duration - ae_beta_max_const_duration:\n",
    "            ae_beta_value = ae_max_beta\n",
    "            ae_beta_values.append(ae_beta_value)\n",
    "        else:\n",
    "            lin_step = cycle_step - ae_beta_min_const_duration\n",
    "            ae_beta_value = ae_min_beta + (ae_max_beta - ae_min_beta) * lin_step / (ae_beta_cycle_duration - ae_beta_min_const_duration - ae_beta_max_const_duration)\n",
    "            ae_beta_values.append(ae_beta_value)\n",
    "            \n",
    "    return ae_beta_values\n",
    "\n",
    "ae_beta_values = calc_ae_beta_values()\n",
    "\n",
    "plt.plot(ae_beta_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce906d-01b1-4160-8ee3-a3151516d059",
   "metadata": {},
   "source": [
    "## Create Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f2957-8300-4f20-9258-43e07afc56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=ae_learning_rate)\n",
    "ae_scheduler = torch.optim.lr_scheduler.StepLR(ae_optimizer, step_size=50, gamma=0.316)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae74d9d-f1fc-4dfd-9f33-bca14601b922",
   "metadata": {},
   "source": [
    "## Create Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1011a857-2d6c-40f1-9a6f-def914a7affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "cross_entropy = nn.BCELoss()\n",
    "\n",
    "def variational_loss(mu, std):\n",
    "    #returns the varialtional loss from arguments mean and standard deviation std\n",
    "    #see also: see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    #https://arxiv.org/abs/1312.6114\n",
    "    vl=-0.5*torch.mean(1+ 2*torch.log(std)-mu.pow(2) -(std.pow(2)))\n",
    "    return vl\n",
    "\n",
    "def ae_rec_loss(y, yhat):\n",
    "    \n",
    "    al = mse_loss(yhat, y)\n",
    "\n",
    "    return al\n",
    "\n",
    "# autoencoder loss function\n",
    "def ae_loss(y, yhat, mu, std):\n",
    "\n",
    "    # kld loss\n",
    "    _ae_kld_loss = variational_loss(mu, std)\n",
    "    \n",
    "    # ae rec loss\n",
    "    _ae_rec_loss = ae_rec_loss(y, yhat)\n",
    "    \n",
    "    _total_loss = 0.0\n",
    "    _total_loss += _ae_rec_loss * ae_rec_loss_scale\n",
    "    _total_loss += _ae_kld_loss * ae_beta\n",
    "    \n",
    "    return _total_loss, _ae_rec_loss, _ae_kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad61ed-824d-45e8-93d1-aa135968b006",
   "metadata": {},
   "source": [
    "## Create Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8131e06-e962-42c9-806d-0064ca249980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_train_step(target_features):\n",
    "    \n",
    "    #print(\"train step target_audio \", target_audio.shape)\n",
    "    audio_encoder_out_mu, audio_encoder_out_std = encoder(target_features)\n",
    "    \n",
    "    mu = audio_encoder_out_mu\n",
    "    std = torch.nn.functional.softplus(audio_encoder_out_std) + 1e-6\n",
    "    decoder_input = encoder.reparameterize(mu, std)\n",
    " \n",
    "    pred_features_norm = decoder(decoder_input)\n",
    "    \n",
    "    _ae_loss, _ae_rec_loss, _ae_kld_loss = ae_loss(target_features, pred_features_norm, mu, std) \n",
    "    \n",
    "    # Backpropagation\n",
    "    ae_optimizer.zero_grad()\n",
    "    _ae_loss.backward()\n",
    "    \n",
    "    #torch.nn.utils.clip_grad_norm(encoder.parameters(), 0.01)\n",
    "    #torch.nn.utils.clip_grad_norm(decoder.parameters(), 0.01)\n",
    "\n",
    "    ae_optimizer.step()\n",
    "    \n",
    "    return _ae_loss, _ae_rec_loss, _ae_kld_loss\n",
    "\n",
    "def train(dataloader, epochs):\n",
    "    \n",
    "    global ae_beta\n",
    "    \n",
    "    loss_history = {}\n",
    "    loss_history[\"ae train\"] = []\n",
    "    loss_history[\"ae rec\"] = []\n",
    "    loss_history[\"ae kld\"] = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        ae_beta = ae_beta_values[epoch]\n",
    "        \n",
    "        #print(\"ae_kld_loss_scale \", ae_kld_loss_scale)\n",
    "        \n",
    "        ae_train_loss_per_epoch = []\n",
    "        ae_rec_loss_per_epoch = []\n",
    "        ae_kld_loss_per_epoch = []\n",
    "        \n",
    "        for train_batch in dataloader:\n",
    "            train_batch = train_batch.to(device)\n",
    "            train_batch = vocos.feature_extractor(train_batch.unsqueeze(1))\n",
    "            train_batch = train_batch.squeeze(1).permute((0, 2, 1))\n",
    "            \n",
    "            _ae_loss, _ae_rec_loss, _ae_kld_loss = ae_train_step(train_batch)\n",
    "            \n",
    "            _ae_loss = _ae_loss.detach().cpu().numpy()\n",
    "            _ae_rec_loss = _ae_rec_loss.detach().cpu().numpy()\n",
    "            _ae_kld_loss = _ae_kld_loss.detach().cpu().numpy()\n",
    "            \n",
    "            #print(\"_ae_prior_loss \", _ae_prior_loss)\n",
    "            \n",
    "            ae_train_loss_per_epoch.append(_ae_loss)\n",
    "            ae_rec_loss_per_epoch.append(_ae_rec_loss)\n",
    "            ae_kld_loss_per_epoch.append(_ae_kld_loss)\n",
    "\n",
    "        ae_train_loss_per_epoch = np.mean(np.array(ae_train_loss_per_epoch))\n",
    "        ae_rec_loss_per_epoch = np.mean(np.array(ae_rec_loss_per_epoch))\n",
    "        ae_kld_loss_per_epoch = np.mean(np.array(ae_kld_loss_per_epoch))\n",
    "        \n",
    "        if epoch % model_save_interval == 0 and save_weights == True:\n",
    "            torch.save(encoder.state_dict(), \"results/weights/encoder_weights_epoch_{}\".format(epoch))\n",
    "            torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epoch))\n",
    "        \n",
    "        loss_history[\"ae train\"].append(ae_train_loss_per_epoch)\n",
    "        loss_history[\"ae rec\"].append(ae_rec_loss_per_epoch)\n",
    "        loss_history[\"ae kld\"].append(ae_kld_loss_per_epoch)\n",
    "        \n",
    "        print ('epoch {} : ae train: {:01.4f} rec {:01.4f} kld {:01.4f} time {:01.2f}'.format(epoch + 1, ae_train_loss_per_epoch, ae_rec_loss_per_epoch, ae_kld_loss_per_epoch, time.time()-start))\n",
    "    \n",
    "        ae_scheduler.step()\n",
    "        \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ee69d-e0f7-4a5f-b67e-479ce60826fb",
   "metadata": {},
   "source": [
    "## Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca1399-ce2d-4f55-98f5-5d7b172fbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = train(dataloader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5110d64-1de5-4176-82b3-22f1d9489974",
   "metadata": {},
   "source": [
    "## Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9783d0-c32c-4dc3-a6bc-af266e0c64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss_as_image(loss_history, image_file_name):\n",
    "    keys = list(loss_history.keys())\n",
    "    epochs = len(loss_history[keys[0]])\n",
    "    \n",
    "    for key in keys:\n",
    "        plt.plot(range(epochs), loss_history[key], label=key)\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(image_file_name)\n",
    "    plt.show()\n",
    "\n",
    "def save_loss_as_csv(loss_history, csv_file_name):\n",
    "    with open(csv_file_name, 'w') as csv_file:\n",
    "        csv_columns = list(loss_history.keys())\n",
    "        csv_row_count = len(loss_history[csv_columns[0]])\n",
    "        \n",
    "        \n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=',', lineterminator='\\n')\n",
    "        csv_writer.writeheader()\n",
    "    \n",
    "        for row in range(csv_row_count):\n",
    "        \n",
    "            csv_row = {}\n",
    "        \n",
    "            for key in loss_history.keys():\n",
    "                csv_row[key] = loss_history[key][row]\n",
    "\n",
    "            csv_writer.writerow(csv_row)\n",
    "\n",
    "save_loss_as_csv(loss_history, \"results/histories/history_{}.csv\".format(epochs))\n",
    "save_loss_as_image(loss_history, \"results/histories/history_{}.png\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c03f550-4aa9-43e2-8367-a427659ca85a",
   "metadata": {},
   "source": [
    "## Save Final Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ddc07-ab41-4f25-a38c-f0389dc1240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"results/weights/encoder_weights_epoch_{}\".format(epochs))\n",
    "torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24edda33-1efb-4442-8ec1-9a2f340b0efa",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f60540e-e4cc-4a05-89b4-5e754944f4df",
   "metadata": {},
   "source": [
    "## Audio Reconstruction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75d043-ac5f-4760-aa49-583c24638391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ref_audio_window(waveform_window, file_name):\n",
    "\n",
    "    torchaudio.save(\"{}\".format(file_name), waveform_window, audio_sample_rate)\n",
    "\n",
    "def create_voc_audio_window(waveform_window, file_name):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        audio_features = vocos.feature_extractor(waveform_window.to(device))\n",
    "        waveform_window_voc = vocos.decode(audio_features)\n",
    "    \n",
    "    torchaudio.save(\"{}\".format(file_name), waveform_window_voc.detach().cpu(), audio_sample_rate)\n",
    "\n",
    "def create_pred_audio_window(waveform_window, file_name):\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        audio_features = vocos.feature_extractor(waveform_window.to(device))\n",
    "        audio_encoder_in = audio_features.squeeze(1).permute((0, 2, 1))\n",
    "\n",
    "        audio_encoder_out_mu, audio_encoder_out_std = encoder(audio_encoder_in)\n",
    "        mu = audio_encoder_out_mu\n",
    "        std = torch.nn.functional.softplus(audio_encoder_out_std) + 1e-6\n",
    "        audio_encoder_out = encoder.reparameterize(mu, std)\n",
    "        audio_decoder_in = audio_encoder_out\n",
    "        audio_decoder_out = decoder(audio_decoder_in)\n",
    "        audio_features_pred = audio_decoder_out.permute((0, 2, 1))\n",
    "        audio_features_pred = audio_features_pred.squeeze(1)\n",
    "        audio_waveform_window_pred = vocos.decode(audio_features_pred)\n",
    "\n",
    "    torchaudio.save(\"{}\".format(file_name), audio_waveform_window_pred.detach().cpu(), audio_sample_rate)\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "def create_ref_audio(waveform, file_name):\n",
    "\n",
    "    torchaudio.save(\"{}\".format(file_name), waveform, audio_sample_rate)\n",
    "    \n",
    "    #print(\"waveform s \", waveform.shape)\n",
    "\n",
    "def create_voc_audio(waveform, file_name):\n",
    "    \n",
    "    waveform_length = waveform.shape[1]\n",
    "    audio_window_offset = audio_window_length // 2\n",
    "    audio_window_env = torch.hann_window(audio_window_length)\n",
    "    \n",
    "    audio_window_count = int(waveform_length - audio_window_length) // audio_window_offset\n",
    "    pred_audio_sequence = torch.zeros((waveform_length), dtype=torch.float32)\n",
    "    \n",
    "    #print(\"pred_audio_sequence s \", pred_audio_sequence.shape)\n",
    "    \n",
    "\n",
    "    for i in range(audio_window_count):\n",
    "        \n",
    "        window_start = i * audio_window_offset\n",
    "        window_end = window_start + audio_window_length\n",
    "        \n",
    "        waveform_window = waveform[:, window_start:window_end]\n",
    "        \n",
    "        #print(\"i \", i, \" target_audio s \", target_audio.shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            audio_features = vocos.feature_extractor(waveform_window.to(device))\n",
    "            waveform_window_voc = vocos.decode(audio_features)\n",
    "\n",
    "        #print(\"voc_audio s \", voc_audio.shape)\n",
    "        #print(\"grain_env s \", grain_env.shape)\n",
    "        \n",
    "        waveform_window_voc = waveform_window_voc.detach().cpu()\n",
    "\n",
    "        pred_audio_sequence[i*audio_window_offset:i*audio_window_offset + audio_window_length] += waveform_window_voc[0] * audio_window_env\n",
    "\n",
    "    torchaudio.save(\"{}\".format(file_name), torch.reshape(pred_audio_sequence, (1, -1)), audio_sample_rate)\n",
    "\n",
    "def create_pred_audio(waveform, file_name):\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    waveform_length = waveform.shape[1]\n",
    "    audio_window_offset = audio_window_length // 2\n",
    "    audio_window_env = torch.hann_window(audio_window_length)\n",
    "    \n",
    "    audio_window_count = int(waveform_length - audio_window_length) // audio_window_offset\n",
    "    pred_audio_sequence = torch.zeros((waveform_length), dtype=torch.float32)\n",
    "    \n",
    "    #print(\"pred_audio_sequence s \", pred_audio_sequence.shape)\n",
    "    \n",
    "    for i in range(audio_window_count):\n",
    "        \n",
    "        window_start = i * audio_window_offset\n",
    "        window_end = window_start + audio_window_length\n",
    "        \n",
    "        waveform_window = waveform[:, window_start:window_end]\n",
    "        \n",
    "        #print(\"i \", i, \" waveform_window s \", waveform_window.shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            audio_features = vocos.feature_extractor(waveform_window.to(device))\n",
    "            audio_encoder_in = audio_features.squeeze(1).permute((0, 2, 1))\n",
    "            \n",
    "            audio_encoder_out_mu, audio_encoder_out_std = encoder(audio_encoder_in)\n",
    "            mu = audio_encoder_out_mu\n",
    "            std = torch.nn.functional.softplus(audio_encoder_out_std) + 1e-6\n",
    "            \n",
    "            audio_encoder_out = encoder.reparameterize(mu, std)\n",
    "            \n",
    "            audio_decoder_in = audio_encoder_out\n",
    "            audio_decoder_out = decoder(audio_decoder_in)\n",
    "            audio_features_pred = audio_decoder_out.permute((0, 2, 1))\n",
    "            audio_features_pred = audio_features_pred.squeeze(1)\n",
    "            audio_waveform_window_pred = vocos.decode(audio_features_pred)\n",
    "\n",
    "        #print(\"voc_audio s \", voc_audio.shape)\n",
    "        #print(\"grain_env s \", grain_env.shape)\n",
    "        \n",
    "        audio_waveform_window_pred = audio_waveform_window_pred.detach().cpu()\n",
    "\n",
    "        pred_audio_sequence[i*audio_window_offset:i*audio_window_offset + audio_window_length] += audio_waveform_window_pred[0] * audio_window_env\n",
    "\n",
    "    torchaudio.save(\"{}\".format(file_name), torch.reshape(pred_audio_sequence, (1, -1)), audio_sample_rate)\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fe43b-16c3-43ba-9f50-cc45e1d7f167",
   "metadata": {},
   "source": [
    "## Perform Audio Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a84645d-17b9-406d-893f-78ff8b2405bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_file = audio_files[0]\n",
    "test_waveform_start_time = 50.0\n",
    "test_audio_start_times = [20, 120, 240]\n",
    "test_audio_duration = 20.0\n",
    "\n",
    "test_audio_file_gui = widgets.Dropdown(\n",
    "    options=[audio_file for audio_file in audio_files],\n",
    "    value=audio_files[0],  # default selected value\n",
    "    description='Audio File:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "test_waveform_start_time_gui = widgets.FloatText(value=test_waveform_start_time, description=\"Test Waveform Start Time (secs):\", style={'description_width': 'initial'})\n",
    "\n",
    "test_audio_start_times_gui = widgets.Textarea(\n",
    "    value=','.join(list(map(str, test_audio_start_times))),\n",
    "    placeholder='Test Audio Start Time (secs) separated by commas',\n",
    "    description='Test Audio Start Times:',\n",
    "    layout=widgets.Layout(width='50%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "test_audio_duration_gui = widgets.FloatText(value=test_audio_duration, description=\"Test Audio Duration (secs):\", style={'description_width': 'initial'})\n",
    "\n",
    "\n",
    "display(test_audio_file_gui)\n",
    "display(test_waveform_start_time_gui)\n",
    "display(test_audio_start_times_gui)\n",
    "display(test_audio_duration_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61daa60c-c21e-48b5-b1fd-c1ed334a7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_file = test_audio_file_gui.value\n",
    "test_waveform_start_time = test_waveform_start_time_gui.value\n",
    "test_audio_start_times  = [int(s) for s in re.split(r\"\\s*,\\s*\", test_audio_start_times_gui.value) if s.strip()]\n",
    "test_audio_duration = test_audio_duration_gui.value\n",
    "\n",
    "test_waveform, _ = torchaudio.load(audio_file_path + \"/\" + test_audio_file)\n",
    "test_waveform_sample_index = int(audio_sample_rate * test_waveform_start_time)\n",
    "test_waveform_window = test_waveform[:, test_waveform_sample_index:test_waveform_sample_index+audio_window_length]\n",
    "\n",
    "create_ref_audio_window(test_waveform_window, \"results/audio/audio_window_orig.wav\")\n",
    "create_voc_audio_window(test_waveform_window, \"results/audio/audio_window_voc.wav\")\n",
    "create_pred_audio_window(test_waveform_window, \"results/audio/audio_window_pred_epoch_{}.wav\".format(epochs))\n",
    "\n",
    "for test_audio_start_time in test_audio_start_times:\n",
    "    start_time_sample_index = int(test_audio_start_time * audio_sample_rate)\n",
    "    end_time_sample_index = start_time_sample_index + int(test_audio_duration * audio_sample_rate)\n",
    "\n",
    "    create_ref_audio(test_waveform[:, start_time_sample_index:end_time_sample_index], \"results/audio/audio_ref_{}-{}.wav\".format(test_audio_start_time, (test_audio_start_time + test_audio_duration)))\n",
    "    create_voc_audio(test_waveform[:, start_time_sample_index:end_time_sample_index], \"results/audio/audio_voc_{}-{}.wav\".format(test_audio_start_time, (test_audio_start_time + test_audio_duration)))\n",
    "    create_pred_audio(test_waveform[:, start_time_sample_index:end_time_sample_index], \"results/audio/audio_pred_{}-{}_epoch_{}.wav\".format(test_audio_start_time, (test_audio_start_time + test_audio_duration), epochs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab8b3f-89d7-4f6e-971c-967194a81c25",
   "metadata": {},
   "source": [
    "## Latent Space Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2421a9ad-7e9d-4b9b-beef-44fa44141c49",
   "metadata": {},
   "source": [
    "## Audio Encode and Decode Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1877a93-cbf4-4d4f-95f0-060db32acbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_audio(waveform):\n",
    "    \n",
    "    encoder.eval()\n",
    "    \n",
    "    waveform_length = waveform.shape[1]\n",
    "    audio_window_offset = audio_window_length // 2\n",
    "    audio_window_count = int(waveform_length - audio_window_length) // audio_window_offset\n",
    "    \n",
    "    latent_vectors = []\n",
    "\n",
    "    for i in range(audio_window_count):\n",
    "        \n",
    "        window_start = i * audio_window_offset\n",
    "        window_end = window_start + audio_window_length\n",
    "        \n",
    "        waveform_window = waveform[:, window_start:window_end]\n",
    "        \n",
    "        #print(\"i \", i, \" waveform_window s \", waveform_window.shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            audio_features = vocos.feature_extractor(waveform_window.to(device))\n",
    "            audio_encoder_in = audio_features.squeeze(1).permute((0, 2, 1))\n",
    "            \n",
    "            audio_encoder_out_mu, audio_encoder_out_std = encoder(audio_encoder_in)\n",
    "            mu = audio_encoder_out_mu\n",
    "            std = torch.nn.functional.softplus(audio_encoder_out_std) + 1e-6\n",
    "            \n",
    "            audio_encoder_out = encoder.reparameterize(mu, std)\n",
    "            \n",
    "        latent_vector = audio_encoder_out.squeeze(0)\n",
    "        latent_vector = latent_vector.detach().cpu().numpy()\n",
    "    \n",
    "        latent_vectors.append(latent_vector)\n",
    "    \n",
    "    encoder.train()\n",
    "        \n",
    "    return latent_vectors\n",
    "\n",
    "def decode_audio_encodings(encodings, file_name):\n",
    "    \n",
    "    decoder.eval()\n",
    "    \n",
    "    audio_window_offset = audio_window_length // 2\n",
    "    audio_window_env = torch.hann_window(audio_window_length)\n",
    "    \n",
    "    audio_window_count = len(encodings)\n",
    "    waveform_length = audio_window_count * audio_window_offset + audio_window_length\n",
    "    \n",
    "    pred_audio_sequence = torch.zeros((waveform_length), dtype=torch.float32)\n",
    "    \n",
    "    #print(\"pred_audio_sequence s \", pred_audio_sequence.shape)\n",
    "    \n",
    "    for i in range(audio_window_count):\n",
    "        \n",
    "        window_start = i * audio_window_offset\n",
    "        window_end = window_start + audio_window_length\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            audio_decoder_in = torch.Tensor(encodings[i]).unsqueeze(0).to(device)\n",
    "            audio_decoder_out = decoder(audio_decoder_in)\n",
    "            audio_features_pred = audio_decoder_out.permute((0, 2, 1))\n",
    "            audio_features_pred = audio_features_pred.squeeze(1)\n",
    "            audio_waveform_window_pred = vocos.decode(audio_features_pred)\n",
    "\n",
    "        #print(\"voc_audio s \", voc_audio.shape)\n",
    "        #print(\"grain_env s \", grain_env.shape)\n",
    "        \n",
    "        audio_waveform_window_pred = audio_waveform_window_pred.detach().cpu()\n",
    "\n",
    "        pred_audio_sequence[i*audio_window_offset:i*audio_window_offset + audio_window_length] += audio_waveform_window_pred[0] * audio_window_env\n",
    "\n",
    "    torchaudio.save(\"{}\".format(file_name), torch.reshape(pred_audio_sequence, (1, -1)), audio_sample_rate)\n",
    "\n",
    "    decoder.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914982d-8a29-4325-8f5e-a7a41aa091de",
   "metadata": {},
   "source": [
    "## Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82884919-7719-43c0-86a3-ea751c98360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_start_time = 20\n",
    "test_audio_duration = 20\n",
    "random_walk_step_scale = 0.1\n",
    "\n",
    "test_audio_start_time_gui = widgets.FloatText(value=test_audio_start_time, description=\"Test Audio Start Time (secs):\", style={'description_width': 'initial'})\n",
    "test_audio_duration_gui = widgets.FloatText(value=test_audio_duration, description=\"Test Audio Duration (secs):\", style={'description_width': 'initial'})\n",
    "random_walk_step_scale_gui = widgets.FloatText(value=random_walk_step_scale, description=\"Random Walk Step Scale:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(test_audio_start_time_gui)\n",
    "display(test_audio_duration_gui)\n",
    "display(random_walk_step_scale_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc7c2d-aab6-44b7-b8a9-ea39e6856404",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_start_time = test_audio_start_time_gui.value\n",
    "test_audio_duration = test_audio_duration_gui.value\n",
    "random_walk_step_scale = random_walk_step_scale_gui.value\n",
    "\n",
    "start_time_sample_index = int(test_audio_start_time * audio_sample_rate)\n",
    "end_time_sample_index = start_time_sample_index + int(test_audio_duration * audio_sample_rate)\n",
    "\n",
    "audio_window_offset = audio_window_length // 2\n",
    "\n",
    "latent_vectors = encode_audio(test_waveform[:, start_time_sample_index:start_time_sample_index + audio_window_length + audio_window_offset])\n",
    "audio_window_count = int(test_audio_duration * audio_sample_rate - audio_window_length) // audio_window_offset - 1\n",
    "\n",
    "for window_index in range(audio_window_count):\n",
    "    random_step = np.random.random((latent_dim)).astype(np.float32) * random_walk_step_scale\n",
    "    latent_vectors.append(latent_vectors[window_index] + random_step)\n",
    "\n",
    "decode_audio_encodings(latent_vectors, \"results/audio/randwalk_audio_epochs_{}_audio_{}-{}.wav\".format(epochs, test_audio_start_time, test_audio_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6836fe-a42f-4347-b545-f59da026d51f",
   "metadata": {},
   "source": [
    "## Sequence Offset Following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed171fc5-4c29-4f5a-8f0d-e7205eed9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_start_time = 20\n",
    "test_audio_duration = 20\n",
    "offset_oscil_freq = 4.0\n",
    "offset_oscil_scale = 1.0\n",
    "\n",
    "test_audio_start_time_gui = widgets.FloatText(value=test_audio_start_time, description=\"Test Audio Start Time (secs):\", style={'description_width': 'initial'})\n",
    "test_audio_duration_gui = widgets.FloatText(value=test_audio_duration, description=\"Test Audio Duration (secs):\", style={'description_width': 'initial'})\n",
    "offset_oscil_freq_gui = widgets.FloatText(value=offset_oscil_freq, description=\"Offset Oscil Frequency:\", style={'description_width': 'initial'})\n",
    "offset_oscil_scale_gui = widgets.FloatText(value=offset_oscil_scale, description=\"Offset Oscil Scale:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(test_audio_start_time_gui)\n",
    "display(test_audio_duration_gui)\n",
    "display(offset_oscil_freq_gui)\n",
    "display(offset_oscil_scale_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65db0f2-33f3-4b25-8f85-eef725142386",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_start_time = test_audio_start_time_gui.value\n",
    "test_audio_duration = test_audio_duration_gui.value\n",
    "offset_oscil_freq = offset_oscil_freq_gui.value\n",
    "offset_oscil_scale = offset_oscil_scale_gui.value\n",
    "\n",
    "start_time_sample_index = int(test_audio_start_time * audio_sample_rate)\n",
    "end_time_sample_index = start_time_sample_index + int(test_audio_duration * audio_sample_rate)\n",
    "\n",
    "latent_vectors = encode_audio(test_waveform[:, start_time_sample_index:end_time_sample_index])\n",
    "\n",
    "offset_encodings = []\n",
    "\n",
    "for index in range(len(latent_vectors)):\n",
    "    sin_value = np.sin(index / (len(latent_vectors) - 1) * np.pi * offset_oscil_freq)\n",
    "    offset = np.ones(shape=(latent_dim), dtype=np.float32) * sin_value * offset_oscil_scale\n",
    "    offset_encoding = latent_vectors[index] + offset\n",
    "    offset_encodings.append(offset_encoding)\n",
    "    \n",
    "decode_audio_encodings(offset_encodings, \"results/audio/offset_audio_epochs_{}_audio_{}-{}.wav\".format(epochs, test_audio_start_time, test_audio_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b4ee1-0d78-42dd-ae65-4a7565755457",
   "metadata": {},
   "source": [
    "## Sequence Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa5d8a-c48f-4633-bad4-ad995602cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_audio_start_time = 20\n",
    "test2_audio_start_time = 60\n",
    "test_audio_duration = 20\n",
    "\n",
    "test1_audio_start_time_gui = widgets.FloatText(value=test1_audio_start_time, description=\"Test1 Audio Start Time (secs):\", style={'description_width': 'initial'})\n",
    "test2_audio_start_time_gui = widgets.FloatText(value=test2_audio_start_time, description=\"Test2 Audio Start Time (secs):\", style={'description_width': 'initial'})\n",
    "test_audio_duration_gui = widgets.FloatText(value=test_audio_duration, description=\"Test Audio Duration (secs):\", style={'description_width': 'initial'})\n",
    "\n",
    "display(test1_audio_start_time_gui)\n",
    "display(test2_audio_start_time_gui)\n",
    "display(test_audio_duration_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7fd57d-caeb-4cb7-8d61-1e86b57ab964",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_audio_start_time = test1_audio_start_time_gui.value\n",
    "test2_audio_start_time = test2_audio_start_time_gui.value\n",
    "test_audio_duration = test_audio_duration_gui.value\n",
    "\n",
    "start1_time_sample_index = int(test1_audio_start_time * audio_sample_rate)\n",
    "end1_time_sample_index = start1_time_sample_index + int(test_audio_duration * audio_sample_rate)\n",
    "\n",
    "start2_time_sample_index = int(test2_audio_start_time * audio_sample_rate)\n",
    "end2_time_sample_index = start2_time_sample_index + int(test_audio_duration * audio_sample_rate)\n",
    "\n",
    "latent_vectors_1 = encode_audio(test_waveform[:, start1_time_sample_index:end1_time_sample_index])\n",
    "latent_vectors_2 = encode_audio(test_waveform[:, start2_time_sample_index:end2_time_sample_index])\n",
    "\n",
    "mix_encodings = []\n",
    "\n",
    "for index in range(len(latent_vectors_1)):\n",
    "    mix_factor = index / (len(latent_vectors_1) - 1)\n",
    "    mix_encoding = latent_vectors_1[index] * (1.0 - mix_factor) + latent_vectors_2[index] * mix_factor\n",
    "    mix_encodings.append(mix_encoding)\n",
    "\n",
    "decode_audio_encodings(mix_encodings, \"results/audio/mix_audio_epochs_{}_audio1_{}-{}_audio2_{}-{}.wav\".format(epochs, test1_audio_start_time, test1_audio_start_time + test_audio_duration, test2_audio_start_time, test2_audio_start_time + test_audio_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abb81e-eac4-43a9-a196-b2a510d12972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
