{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf0c862-18d5-4355-bd3b-d884d333c69c",
   "metadata": {},
   "source": [
    "# Audio Autoencoder (CNN Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b84300-6339-4f17-b3a8-e6cd1ae11eea",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea8041f-9c87-4c59-b621-e66be00cafae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "import torchaudio\n",
    "import simpleaudio as sa\n",
    "import numpy as np\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import os, time\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from vocos import Vocos\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a8d7b-b3a1-4bb6-adda-74229bbe52d5",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e081551d-ad45-4aae-86f9-5b2863d0ea87",
   "metadata": {},
   "source": [
    "## Compute Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad53149b-6deb-434e-bd33-7df4f42e0306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f652c9b-172c-4400-a2ee-c975e20a6299",
   "metadata": {},
   "source": [
    "## Audio Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b10ba9-b15c-45b7-a513-290eaac1869a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666f8a83047a469eae29d89882e0feea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../../../Data/Audio/', description='Audio File Path:', style=TextStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5edde0bf5e4d9494717b16503d0ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='Night_and_Day_by_Virginia_Woolf_48khz.wav', description='Audio Files:', layout=Layout(width='5…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97d031b91734fb8a6f520150cac36f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=48000, description='Audio Sample Rate:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a2b4e552be47718a0855dfe88834e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1, description='Audio Channel Count:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae87623f61364cd48b77174bb0793755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2048, description='Audio Window Length:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_file_path = \"../../../Data/Audio/Gutenberg/\"\n",
    "audio_files = [\"Night_and_Day_by_Virginia_Woolf_48khz.wav\"]\n",
    "audio_sample_rate = 48000 # numer of audio samples per sec\n",
    "audio_channel_count = 1\n",
    "audio_window_length = 2048 # this results in 9 mel spectra\n",
    "\n",
    "audio_file_path_gui = widgets.Text(value=audio_file_path, description=\"Audio File Path:\", style={'description_width': 'initial'}) \n",
    "\n",
    "audio_files_gui = widgets.Textarea(\n",
    "    value=','.join(audio_files),\n",
    "    placeholder='Enter file names separated by commas',\n",
    "    description='Audio Files:',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "audio_sample_rate_gui = widgets.IntText(value=audio_sample_rate, description=\"Audio Sample Rate:\", style={'description_width': 'initial'})\n",
    "audio_channel_count_gui = widgets.IntText(value=audio_channel_count, description=\"Audio Channel Count:\", style={'description_width': 'initial'})\n",
    "audio_window_length_gui = widgets.IntText(value=audio_window_length, description=\"Audio Window Length:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(audio_file_path_gui)\n",
    "display(audio_files_gui)\n",
    "display(audio_sample_rate_gui)\n",
    "display(audio_channel_count_gui)\n",
    "display(audio_window_length_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8205c5d7-71f8-4607-ac91-4a36b77c9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = audio_file_path_gui.value\n",
    "audio_files = re.split(r'\\s*,\\s*', audio_files_gui.value)\n",
    "audio_sample_rate = audio_sample_rate_gui.value\n",
    "audio_channel_count = audio_channel_count_gui.value\n",
    "audio_window_length = audio_window_length_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb003c7-f3bd-4cb0-acf4-e05fcef60e00",
   "metadata": {},
   "source": [
    "## Autoencoder Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b447e072-b9fa-4b7c-8c58-fb57235c7c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba7d723e957452196a4f5db2b1a7c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=32, description='Latent Dimension:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596b4a6662d04ccfab3cf60e49675a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='16,32,64,128', description='CNN Channel Counts:', layout=Layout(width='50%'), placeholder='Ent…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c57fb62163b41899c22a647c89add54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='5,3', description='CNN Kernel Size:', layout=Layout(width='50%'), placeholder='Enter kernel si…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c201002d474ab38bd5ba76435c8b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='512', description='Dense Layer Sizes:', layout=Layout(width='50%'), placeholder='Enter dense l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cbb8ac0210461194252a3c54d21c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Save Weights', style=CheckboxStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba19398b7d9c4efe88dd75620e775127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Load Weights', style=CheckboxStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368f68cc707347b5bf04e9a24bb24de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='results/weights/encoder_weights_epoch_400', description='Encoder Weights File:', style=TextStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef95741be3e4be0af7dea33ccd04be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='results/weights/decoder_weights_epoch_400', description='Decoder Weights File:', style=TextStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latent_dim = 32\n",
    "sequence_length = None # will be calculate automatically\n",
    "ae_cnn_channel_counts = [ 16, 32, 64, 128 ]\n",
    "ae_cnn_kernel_size = (5, 3)\n",
    "ae_dense_layer_sizes = [ 512 ]\n",
    "\n",
    "save_weights = True\n",
    "load_weights = False\n",
    "encoder_weights_file = \"results/weights/encoder_weights_epoch_400\"\n",
    "decoder_weights_file = \"results/weights/decoder_weights_epoch_400\"\n",
    "\n",
    "latent_dim_gui = widgets.IntText(value=latent_dim, description=\"Latent Dimension:\", style={'description_width': 'initial'})\n",
    "\n",
    "ae_cnn_channel_counts_gui = widgets.Textarea(\n",
    "    value=','.join(list(map(str, ae_cnn_channel_counts))),\n",
    "    placeholder='Enter channel sizes separated by commas',\n",
    "    description='CNN Channel Counts:',\n",
    "    layout=widgets.Layout(width='50%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "ae_cnn_kernel_size_gui = widgets.Textarea(\n",
    "    value=','.join(list(map(str, ae_cnn_kernel_size))),\n",
    "    placeholder='Enter kernel size separated by commas',\n",
    "    description='CNN Kernel Size:',\n",
    "    layout=widgets.Layout(width='50%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "ae_dense_layer_sizes_gui = widgets.Textarea(\n",
    "    value=','.join(list(map(str, ae_dense_layer_sizes))),\n",
    "    placeholder='Enter dense layer sizes separated by commas',\n",
    "    description='Dense Layer Sizes:',\n",
    "    layout=widgets.Layout(width='50%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "save_weights_gui = widgets.Checkbox(\n",
    "    value=save_weights,\n",
    "    description='Save Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "load_weights_gui = widgets.Checkbox(\n",
    "    value=load_weights,\n",
    "    description='Load Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "encoder_weights_file_gui = widgets.Text(value=encoder_weights_file, description=\"Encoder Weights File:\", style={'description_width': 'initial'}) \n",
    "decoder_weights_file_gui = widgets.Text(value=decoder_weights_file, description=\"Decoder Weights File:\", style={'description_width': 'initial'}) \n",
    "\n",
    "display(latent_dim_gui)\n",
    "display(ae_cnn_channel_counts_gui)\n",
    "display(ae_cnn_kernel_size_gui)\n",
    "display(ae_dense_layer_sizes_gui)\n",
    "display(save_weights_gui)\n",
    "display(load_weights_gui)\n",
    "display(encoder_weights_file_gui)\n",
    "display(decoder_weights_file_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03cf6272-0a55-41e6-be76-81dbe3a70994",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = latent_dim_gui.value\n",
    "ae_cnn_channel_counts  = [int(s) for s in re.split(r\"\\s*,\\s*\", ae_cnn_channel_counts_gui.value) if s.strip()]\n",
    "ae_cnn_kernel_size  = [int(s) for s in re.split(r\"\\s*,\\s*\", ae_cnn_kernel_size_gui.value) if s.strip()]\n",
    "ae_dense_layer_sizes  = [int(s) for s in re.split(r\"\\s*,\\s*\", ae_dense_layer_sizes_gui.value) if s.strip()]\n",
    "save_weights = save_weights_gui.value\n",
    "load_weights = load_weights_gui.value\n",
    "encoder_weights_file = encoder_weights_file_gui.value\n",
    "decoder_weights_file = decoder_weights_file_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534771a6-d3af-46b6-a18e-ec0305a35505",
   "metadata": {},
   "source": [
    "## Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bba3484-5f9c-4784-af4c-27f4faea61d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f916823d5844b44ba8b9a9b49588a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=100000, description='Dataset Size:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bac7e7e5f4445f9b93a0a5782464ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=32, description='Batch Size:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ef85889b3a4c90906630d7504fff55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='Autoencoder Learning Rate:', style=DescriptionStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfaedca757284b5d8b7335c0cb9cc7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=5.0, description='Autoencoder Reconstruction Loss Scale:', style=DescriptionStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc37ec17518f454daa689499543a3be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='Minimum Beta Factor:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a405daafff54c12aa405a3390196339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.1, description='Maximum Beta Factor:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1befaaa6814cbab85d63de6c424e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=100, description='Cycle Duration for Beta Factor:', style=DescriptionStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418e8e3b2cf04815a8b610402c38d81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=20, description='Duration for Minimum Beta Factor:', style=DescriptionStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9799e751a3c439681633fa9ae19ef04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=20, description='Duration for Maximum Beta Factor:', style=DescriptionStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f87d76ed1b45ea889c753ec0e1f7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=400, description='Number of Training Epochs:', style=DescriptionStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bce8ae6982645f3b4328fd45b44df29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=50, description='Model Save Interval:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_count = 100000\n",
    "batch_size = 32\n",
    "\n",
    "ae_learning_rate = 1e-4\n",
    "ae_rec_loss_scale = 5.0\n",
    "ae_beta = 0.0 # will be calculated\n",
    "ae_beta_cycle_duration = 100\n",
    "ae_beta_min_const_duration = 20\n",
    "ae_beta_max_const_duration = 20\n",
    "ae_min_beta = 0.0\n",
    "ae_max_beta = 0.1\n",
    "\n",
    "epochs = 400\n",
    "model_save_interval = 50\n",
    "save_history = True\n",
    "\n",
    "data_count_gui = widgets.IntText(value=data_count, description=\"Dataset Size:\", style={'description_width': 'initial'})\n",
    "batch_size_gui = widgets.IntText(value=batch_size, description=\"Batch Size:\", style={'description_width': 'initial'})\n",
    "ae_learning_rate_gui = widgets.FloatText(value=ae_learning_rate, description=\"Autoencoder Learning Rate:\", style={'description_width': 'initial'})\n",
    "ae_rec_loss_scale_gui = widgets.FloatText(value=ae_rec_loss_scale, description=\"Autoencoder Reconstruction Loss Scale:\", style={'description_width': 'initial'})\n",
    "ae_min_beta_gui = widgets.FloatText(value=ae_min_beta, description=\"Minimum Beta Factor:\", style={'description_width': 'initial'})\n",
    "ae_max_beta_gui = widgets.FloatText(value=ae_max_beta, description=\"Maximum Beta Factor:\", style={'description_width': 'initial'})\n",
    "ae_beta_cycle_duration_gui = widgets.IntText(value=ae_beta_cycle_duration, description=\"Cycle Duration for Beta Factor:\", style={'description_width': 'initial'})\n",
    "ae_beta_min_const_duration_gui = widgets.IntText(value=ae_beta_min_const_duration, description=\"Duration for Minimum Beta Factor:\", style={'description_width': 'initial'})\n",
    "ae_beta_max_const_duration_gui = widgets.IntText(value=ae_beta_min_const_duration, description=\"Duration for Maximum Beta Factor:\", style={'description_width': 'initial'})\n",
    "epochs_gui = widgets.IntText(value=epochs, description=\"Number of Training Epochs:\", style={'description_width': 'initial'})\n",
    "model_save_interval_gui = widgets.IntText(value=model_save_interval, description=\"Model Save Interval:\", style={'description_width': 'initial'})\n",
    "\n",
    "save_history_gui = widgets.Checkbox(\n",
    "    value=save_history,\n",
    "    description='Save Training History',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(data_count_gui)\n",
    "display(batch_size_gui)\n",
    "display(ae_learning_rate_gui)\n",
    "display(ae_rec_loss_scale_gui)\n",
    "display(ae_min_beta_gui)\n",
    "display(ae_max_beta_gui)\n",
    "display(ae_beta_cycle_duration_gui)\n",
    "display(ae_beta_min_const_duration_gui)\n",
    "display(ae_beta_max_const_duration_gui)\n",
    "display(epochs_gui)\n",
    "display(model_save_interval_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86e1fdfe-b6ad-440e-97d6-036a417e6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_count = data_count_gui.value\n",
    "batch_size = batch_size_gui.value\n",
    "ae_learning_rate = ae_learning_rate_gui.value\n",
    "ae_rec_loss_scale = ae_rec_loss_scale_gui.value\n",
    "ae_beta_cycle_duration = ae_beta_cycle_duration_gui.value\n",
    "ae_beta_min_const_duration = ae_beta_min_const_duration_gui.value\n",
    "ae_beta_max_const_duration = ae_beta_max_const_duration_gui.value\n",
    "ae_min_beta = ae_min_beta_gui.value\n",
    "ae_max_beta = ae_max_beta_gui.value\n",
    "epochs = epochs_gui.value\n",
    "model_save_interval = model_save_interval_gui.value\n",
    "save_history = save_history_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa14b8b-7a1d-47e0-929e-3d30ca3735b0",
   "metadata": {},
   "source": [
    "## Create Vocoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6fe13d-3dc5-43c5-9ddf-6ea294acbb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dbisig\\anaconda3\\envs\\CASAISound\\lib\\site-packages\\vocos\\pretrained.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "vocos = Vocos.from_pretrained(\"kittn/vocos-mel-48khz-alpha1\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09373480-fa2c-4192-b21d-efe96f31b075",
   "metadata": {},
   "source": [
    "## Determine Number of Mel Filters and Mel Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fd84bc3-477b-4d98-b9eb-3901481d1c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_window_length  2048  mel_count  9  mel_filter_count  128\n"
     ]
    }
   ],
   "source": [
    "vocoder_features = vocos.feature_extractor(torch.rand(size=(1, audio_window_length), dtype=torch.float32).to(device))\n",
    "mel_count = vocoder_features.shape[-1]\n",
    "mel_filter_count = vocoder_features.shape[1]\n",
    "sequence_length = mel_count\n",
    "\n",
    "print(\"audio_window_length \", audio_window_length, \" mel_count \", mel_count, \" mel_filter_count \", mel_filter_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d1940-be77-416c-a02e-565e7075e13a",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0431bfa-8227-448d-8556-9dbc4337595e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to open the input \"..\\..\\..\\Data\\Audio\\Night_and_Day_by_Virginia_Woolf_48khz.wav\" (No such file or directory).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 30\u001b[0m\n\u001b[0;32m     25\u001b[0m         audio_excerpt \u001b[38;5;241m=\u001b[39m audio_excerpt[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m audio_excerpt\n\u001b[1;32m---> 30\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mAudioDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_window_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m dataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_dataset)\n\u001b[0;32m     33\u001b[0m data_item \u001b[38;5;241m=\u001b[39m full_dataset[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m, in \u001b[0;36mAudioDataset.__init__\u001b[1;34m(self, audio_file_path, audio_files, audio_window_length, audio_data_count)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_waveforms \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m audio_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_files:\n\u001b[1;32m---> 11\u001b[0m     audio_waveform, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_file_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_waveforms\u001b[38;5;241m.\u001b[39mappend(audio_waveform)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CASAISound\\lib\\site-packages\\torchaudio\\_backend\\utils.py:205\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[1;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    204\u001b[0m backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CASAISound\\lib\\site-packages\\torchaudio\\_backend\\ffmpeg.py:297\u001b[0m, in \u001b[0;36mFFmpegBackend.load\u001b[1;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m    289\u001b[0m     uri: InputType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m     buffer_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m,\n\u001b[0;32m    296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CASAISound\\lib\\site-packages\\torchaudio\\_backend\\ffmpeg.py:88\u001b[0m, in \u001b[0;36mload_audio\u001b[1;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(src, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvorbis\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mogg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 88\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStreamReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m sample_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(s\u001b[38;5;241m.\u001b[39mget_src_stream_info(s\u001b[38;5;241m.\u001b[39mdefault_audio_stream)\u001b[38;5;241m.\u001b[39msample_rate)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m=\u001b[39m _get_load_filter(frame_offset, num_frames, convert)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CASAISound\\lib\\site-packages\\torio\\io\\_streaming_media_decoder.py:526\u001b[0m, in \u001b[0;36mStreamingMediaDecoder.__init__\u001b[1;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_be \u001b[38;5;241m=\u001b[39m ffmpeg_ext\u001b[38;5;241m.\u001b[39mStreamingMediaDecoderFileObj(src, \u001b[38;5;28mformat\u001b[39m, option, buffer_size)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 526\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_be \u001b[38;5;241m=\u001b[39m \u001b[43mffmpeg_ext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStreamingMediaDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_be\u001b[38;5;241m.\u001b[39mfind_best_audio_stream()\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_audio_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m i\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to open the input \"..\\..\\..\\Data\\Audio\\Night_and_Day_by_Virginia_Woolf_48khz.wav\" (No such file or directory)."
     ]
    }
   ],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_file_path, audio_files, audio_window_length, audio_data_count):\n",
    "        self.audio_file_path = audio_file_path\n",
    "        self.audio_files = audio_files\n",
    "        self.audio_window_length = audio_window_length\n",
    "        self.audio_data_count = audio_data_count\n",
    "        \n",
    "        self.audio_waveforms = []\n",
    "        \n",
    "        for audio_file in self.audio_files:\n",
    "            audio_waveform, _ = torchaudio.load(self.audio_file_path + \"/\" + audio_file)\n",
    "            self.audio_waveforms.append(audio_waveform)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.audio_data_count\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        audio_index = torch.randint(0, len(self.audio_waveforms), size=(1,))\n",
    "        audio_waveform = self.audio_waveforms[audio_index]\n",
    "        \n",
    "        audio_length = audio_waveform.shape[1]\n",
    "        audio_excerpt_start = torch.randint(0, audio_length - self.audio_window_length, size=(1,))\n",
    "        audio_excerpt = audio_waveform[:, audio_excerpt_start:audio_excerpt_start+audio_window_length]\n",
    "        audio_excerpt = audio_excerpt[0]\n",
    "        \n",
    "        return audio_excerpt\n",
    "\n",
    "\n",
    "full_dataset = AudioDataset(audio_file_path, audio_files, audio_window_length, data_count)\n",
    "dataset_size = len(full_dataset)\n",
    "\n",
    "data_item = full_dataset[0]\n",
    "\n",
    "print(\"data_item s \", data_item.shape)\n",
    "\n",
    "dataloader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "batch_x = next(iter(dataloader))\n",
    "\n",
    "print(\"batch_x s \", batch_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775389d9-1d3e-4f55-a824-8487798a74f8",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae531e8-4cec-442d-a30b-7c89436775ef",
   "metadata": {},
   "source": [
    "## Create Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764960dc-978f-4a22-9865-19c82fa2bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, mel_count, mel_filter_count, conv_channel_counts, conv_kernel_size, dense_layer_sizes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.mel_count = mel_count\n",
    "        self.mel_filter_count = mel_filter_count\n",
    "        self.conv_channel_counts = conv_channel_counts\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.dense_layer_sizes = dense_layer_sizes\n",
    "        \n",
    "        # create convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        stride = ((self.conv_kernel_size[0] - 1) // 2, (self.conv_kernel_size[1] - 1) // 2)\n",
    "        \n",
    "        #print(\"conv_kernel_size \", conv_kernel_size)\n",
    "        #print(\"stride \", stride)\n",
    "        \n",
    "        padding = stride\n",
    "        \n",
    "        self.conv_layers.append(nn.Conv2d(1, conv_channel_counts[0], self.conv_kernel_size, stride=stride, padding=padding))\n",
    "        self.conv_layers.append(nn.LeakyReLU(0.2))\n",
    "        self.conv_layers.append(nn.BatchNorm2d(conv_channel_counts[0]))\n",
    "        \n",
    "        conv_layer_count = len(conv_channel_counts)\n",
    "        for layer_index in range(1, conv_layer_count):\n",
    "            self.conv_layers.append(nn.Conv2d(conv_channel_counts[layer_index-1], conv_channel_counts[layer_index], self.conv_kernel_size, stride=stride, padding=padding))\n",
    "            self.conv_layers.append(nn.LeakyReLU(0.2))\n",
    "            self.conv_layers.append(nn.BatchNorm2d(conv_channel_counts[layer_index]))\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # create dense layers\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        \n",
    "        last_conv_layer_size_x = int(mel_filter_count // np.power(stride[0], len(conv_channel_counts)))\n",
    "        last_conv_layer_size_y = int(mel_count // np.power(stride[1], len(conv_channel_counts)))\n",
    "        \n",
    "        #print(\"last_conv_layer_size_x \", last_conv_layer_size_x)\n",
    "        #print(\"last_conv_layer_size_y \", last_conv_layer_size_y)\n",
    "        \n",
    "        preflattened_size = [conv_channel_counts[-1], last_conv_layer_size_x, last_conv_layer_size_y]\n",
    "        \n",
    "        #print(\"preflattened_size \", preflattened_size)\n",
    "        \n",
    "        dense_layer_input_size = conv_channel_counts[-1] * last_conv_layer_size_x * last_conv_layer_size_y\n",
    "        \n",
    "        #print(\"dense_layer_input_size \", dense_layer_input_size)\n",
    "        #print(\"self.dense_layer_sizes[0] \", self.dense_layer_sizes[0])\n",
    "        \n",
    "        self.dense_layers.append(nn.Linear(dense_layer_input_size, self.dense_layer_sizes[0]))\n",
    "        self.dense_layers.append(nn.ReLU())\n",
    "        \n",
    "        dense_layer_count = len(dense_layer_sizes)\n",
    "        for layer_index in range(1, dense_layer_count):\n",
    "            self.dense_layers.append(nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index]))\n",
    "            self.dense_layers.append(nn.ReLU())\n",
    "            \n",
    "        # create final dense layers\n",
    "        self.fc_mu = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n",
    "        self.fc_std = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"x0 s \", x.shape)\n",
    "        \n",
    "        for lI, layer in enumerate(self.conv_layers):\n",
    "            \n",
    "            #print(\"conv layer \", lI, \" x in \", x.shape)\n",
    "            \n",
    "            x = layer(x)\n",
    "            \n",
    "            #print(\"conv layer \", lI, \" x out \", x.shape)\n",
    "    \n",
    "        #print(\"x1 s \", x.shape)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        #print(\"x2 s \", x.shape)\n",
    "\n",
    "        for lI, layer in enumerate(self.dense_layers):\n",
    "            \n",
    "            #print(\"dense layer \", lI, \" x in \", x.shape)\n",
    "            \n",
    "            x = layer(x)\n",
    "            \n",
    "            #print(\"dense layer \", lI, \" x out \", x.shape)\n",
    "            \n",
    "        #print(\"x3 s \", x.shape)\n",
    "        \n",
    "        mu = self.fc_mu(x)\n",
    "        std = self.fc_std(x)\n",
    "        \n",
    "        #print(\"mu s \", mu.shape, \" lvar s \", std.shape)\n",
    "\n",
    "        return mu, std\n",
    "    \n",
    "    def reparameterize(self, mu, std):\n",
    "        z = mu + std*torch.randn_like(std)\n",
    "        return z\n",
    "\n",
    "encoder = Encoder(latent_dim, mel_count, mel_filter_count, ae_cnn_channel_counts, ae_cnn_kernel_size, ae_dense_layer_sizes).to(device)\n",
    "\n",
    "print(encoder)\n",
    "\n",
    "\n",
    "# test encoder\n",
    "audio_batch = next(iter(dataloader)).to(device)\n",
    "audio_batch_mels = vocos.feature_extractor(audio_batch.unsqueeze(1))\n",
    "audio_encoder_in = audio_batch_mels\n",
    "audio_encoder_out_mu, audio_encoder_out_std = encoder(audio_encoder_in)\n",
    "audio_encoder_out = encoder.reparameterize(audio_encoder_out_mu, audio_encoder_out_std)\n",
    "\n",
    "print(\"audio_batch s \", audio_batch.shape)\n",
    "print(\"audio_batch_mels s \", audio_batch_mels.shape)\n",
    "print(\"audio_encoder_in s \", audio_encoder_in.shape)\n",
    "print(\"audio_encoder_out_mu s \", audio_encoder_out_mu.shape)\n",
    "print(\"audio_encoder_out_std s \", audio_encoder_out_std.shape)\n",
    "print(\"audio_encoder_out s \", audio_encoder_out.shape)\n",
    "\n",
    "if load_weights and encoder_weights_file:\n",
    "    encoder.load_state_dict(torch.load(encoder_weights_file, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe293f-61c8-4d66-b0d6-79e336b3cac9",
   "metadata": {},
   "source": [
    "## Create Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2e46f-0a8e-4a80-90bd-35443fd59dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, mel_count, mel_filter_count, conv_channel_counts, conv_kernel_size, dense_layer_sizes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.mel_count = mel_count\n",
    "        self.mel_filter_count = mel_filter_count\n",
    "        self.conv_channel_counts = conv_channel_counts\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.dense_layer_sizes = dense_layer_sizes\n",
    "        \n",
    "        # create dense layers\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        \n",
    "        stride = ((self.conv_kernel_size[0] - 1) // 2, (self.conv_kernel_size[1] - 1) // 2)\n",
    "        \n",
    "        print(\"stride \", stride)\n",
    "                \n",
    "        self.dense_layers.append(nn.Linear(latent_dim, self.dense_layer_sizes[0]))\n",
    "        self.dense_layers.append(nn.ReLU())\n",
    "        \n",
    "        dense_layer_count = len(dense_layer_sizes)\n",
    "        for layer_index in range(1, dense_layer_count):\n",
    "            self.dense_layers.append(nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index]))\n",
    "            self.dense_layers.append(nn.ReLU())\n",
    "            \n",
    "        last_conv_layer_size_x = int(mel_filter_count // np.power(stride[0], len(conv_channel_counts)))\n",
    "        last_conv_layer_size_y = int(mel_count // np.power(stride[1], len(conv_channel_counts)))\n",
    "        \n",
    "        #print(\"last_conv_layer_size_x \", last_conv_layer_size_x)\n",
    "        #print(\"last_conv_layer_size_y \", last_conv_layer_size_y)\n",
    "        \n",
    "        preflattened_size = [conv_channel_counts[0], last_conv_layer_size_x, last_conv_layer_size_y]\n",
    "        \n",
    "        #print(\"preflattened_size \", preflattened_size)\n",
    "        \n",
    "        dense_layer_output_size = conv_channel_counts[0] * last_conv_layer_size_x * last_conv_layer_size_y\n",
    "        \n",
    "        #print(\"dense_layer_output_size \", dense_layer_output_size)\n",
    "\n",
    "        self.dense_layers.append(nn.Linear(self.dense_layer_sizes[-1], dense_layer_output_size))\n",
    "        self.dense_layers.append(nn.ReLU())\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=preflattened_size)\n",
    "        \n",
    "        # create convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        padding = stride\n",
    "        output_padding = (padding[0] - 1, padding[1] - 1) # does this universally work?\n",
    "        \n",
    "        conv_layer_count = len(conv_channel_counts)\n",
    "        for layer_index in range(1, conv_layer_count):\n",
    "            self.conv_layers.append(nn.BatchNorm2d(conv_channel_counts[layer_index-1]))\n",
    "            self.conv_layers.append(nn.ConvTranspose2d(conv_channel_counts[layer_index-1], conv_channel_counts[layer_index], self.conv_kernel_size, stride=stride, padding=padding, output_padding=output_padding))\n",
    "            self.conv_layers.append(nn.LeakyReLU(0.2))\n",
    "            \n",
    "        self.conv_layers.append(nn.BatchNorm2d(conv_channel_counts[-1]))\n",
    "        self.conv_layers.append(nn.ConvTranspose2d(conv_channel_counts[-1], 1, self.conv_kernel_size, stride=stride, padding=padding, output_padding=output_padding))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"x0 s \", x.shape)\n",
    "        \n",
    "        for lI, layer in enumerate(self.dense_layers):\n",
    "            \n",
    "            #print(\"dense layer \", lI, \" x in \", x.shape)\n",
    "            \n",
    "            x = layer(x)\n",
    "            \n",
    "            #print(\"dense layer \", lI, \" x out \", x.shape)\n",
    "            \n",
    "        #print(\"x1 s \", x.shape)\n",
    "        \n",
    "        x = self.unflatten(x)\n",
    "        \n",
    "        #print(\"x2 s \", x.shape)\n",
    "\n",
    "        for lI, layer in enumerate(self.conv_layers):\n",
    "            \n",
    "            #print(\"conv layer \", lI, \" x in \", x.shape)\n",
    "            \n",
    "            x = layer(x)\n",
    "            \n",
    "            #print(\"conv layer \", lI, \" x out \", x.shape)\n",
    "    \n",
    "        #print(\"x3 s \", x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "ae_cnn_channel_counts_reversed = ae_cnn_channel_counts.copy()\n",
    "ae_cnn_channel_counts_reversed.reverse()\n",
    "    \n",
    "ae_dense_layer_sizes_reversed = ae_dense_layer_sizes.copy()\n",
    "ae_dense_layer_sizes_reversed.reverse()\n",
    "\n",
    "decoder = Decoder(latent_dim, mel_count, mel_filter_count, ae_cnn_channel_counts_reversed, ae_cnn_kernel_size, ae_dense_layer_sizes_reversed).to(device)\n",
    "\n",
    "print(decoder)\n",
    "\n",
    "if load_weights and decoder_weights_file:\n",
    "    decoder.load_state_dict(torch.load(decoder_weights_file, map_location=device))\n",
    "\n",
    "# test decoder\n",
    "audio_decoder_in = audio_encoder_out\n",
    "audio_decoder_out = decoder(audio_decoder_in)\n",
    "audio_features = audio_decoder_out.squeeze(1)\n",
    "audio_batch = vocos.decode(audio_features)\n",
    "\n",
    "print(\"audio_decoder_in s \", audio_decoder_in.shape)\n",
    "print(\"audio_decoder_out s \", audio_decoder_out.shape)\n",
    "print(\"audio_features s \", audio_features.shape)\n",
    "print(\"audio_batch s \", audio_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a800654-6fc5-4217-ab6d-fe78905f083a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32706cf1-367d-4b1e-adf1-5814b71ed2f3",
   "metadata": {},
   "source": [
    "## Create Beta Factor Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631aaafb-fa70-40c3-9b24-ad23002cdc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ae_beta_values():\n",
    "    \n",
    "    ae_beta_values = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        cycle_step = e % ae_beta_cycle_duration\n",
    "        \n",
    "        #print(\"cycle_step \", cycle_step)\n",
    "\n",
    "        if cycle_step < ae_beta_min_const_duration:\n",
    "            ae_beta_value = ae_min_beta\n",
    "            ae_beta_values.append(ae_beta_value)\n",
    "        elif cycle_step > ae_beta_cycle_duration - ae_beta_max_const_duration:\n",
    "            ae_beta_value = ae_max_beta\n",
    "            ae_beta_values.append(ae_beta_value)\n",
    "        else:\n",
    "            lin_step = cycle_step - ae_beta_min_const_duration\n",
    "            ae_beta_value = ae_min_beta + (ae_max_beta - ae_min_beta) * lin_step / (ae_beta_cycle_duration - ae_beta_min_const_duration - ae_beta_max_const_duration)\n",
    "            ae_beta_values.append(ae_beta_value)\n",
    "            \n",
    "    return ae_beta_values\n",
    "\n",
    "ae_beta_values = calc_ae_beta_values()\n",
    "\n",
    "plt.plot(ae_beta_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce906d-01b1-4160-8ee3-a3151516d059",
   "metadata": {},
   "source": [
    "## Create Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f2957-8300-4f20-9258-43e07afc56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=ae_learning_rate)\n",
    "ae_scheduler = torch.optim.lr_scheduler.StepLR(ae_optimizer, step_size=50, gamma=0.316)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae74d9d-f1fc-4dfd-9f33-bca14601b922",
   "metadata": {},
   "source": [
    "## Create Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1011a857-2d6c-40f1-9a6f-def914a7affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "cross_entropy = nn.BCELoss()\n",
    "\n",
    "def variational_loss(mu, std):\n",
    "    #returns the varialtional loss from arguments mean and standard deviation std\n",
    "    #see also: see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    #https://arxiv.org/abs/1312.6114\n",
    "    vl=-0.5*torch.mean(1+ 2*torch.log(std)-mu.pow(2) -(std.pow(2)))\n",
    "    return vl\n",
    "\n",
    "def ae_rec_loss(y, yhat):\n",
    "    \n",
    "    al = mse_loss(yhat, y)\n",
    "\n",
    "    return al\n",
    "\n",
    "# autoencoder loss function\n",
    "def ae_loss(y, yhat, mu, std):\n",
    "\n",
    "    # kld loss\n",
    "    _ae_kld_loss = variational_loss(mu, std)\n",
    "    \n",
    "    # ae rec loss\n",
    "    _ae_rec_loss = ae_rec_loss(y, yhat)\n",
    "    \n",
    "    _total_loss = 0.0\n",
    "    _total_loss += _ae_rec_loss * ae_rec_loss_scale\n",
    "    _total_loss += _ae_kld_loss * ae_beta\n",
    "    \n",
    "    return _total_loss, _ae_rec_loss, _ae_kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad61ed-824d-45e8-93d1-aa135968b006",
   "metadata": {},
   "source": [
    "## Create Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8131e06-e962-42c9-806d-0064ca249980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_train_step(target_features):\n",
    "    \n",
    "    #print(\"train step target_audio \", target_audio.shape)\n",
    "    audio_encoder_out_mu, audio_encoder_out_std = encoder(target_features)\n",
    "    \n",
    "    mu = audio_encoder_out_mu\n",
    "    std = torch.nn.functional.softplus(audio_encoder_out_std) + 1e-6\n",
    "    decoder_input = encoder.reparameterize(mu, std)\n",
    " \n",
    "    pred_features_norm = decoder(decoder_input)\n",
    "    \n",
    "    _ae_loss, _ae_rec_loss, _ae_kld_loss = ae_loss(target_features, pred_features_norm, mu, std) \n",
    "    \n",
    "    # Backpropagation\n",
    "    ae_optimizer.zero_grad()\n",
    "    _ae_loss.backward()\n",
    "    \n",
    "    #torch.nn.utils.clip_grad_norm(encoder.parameters(), 0.01)\n",
    "    #torch.nn.utils.clip_grad_norm(decoder.parameters(), 0.01)\n",
    "\n",
    "    ae_optimizer.step()\n",
    "    \n",
    "    return _ae_loss, _ae_rec_loss, _ae_kld_loss\n",
    "\n",
    "def train(dataloader, epochs):\n",
    "    \n",
    "    global ae_beta\n",
    "    \n",
    "    loss_history = {}\n",
    "    loss_history[\"ae train\"] = []\n",
    "    loss_history[\"ae rec\"] = []\n",
    "    loss_history[\"ae kld\"] = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        ae_beta = ae_beta_values[epoch]\n",
    "        \n",
    "        #print(\"ae_kld_loss_scale \", ae_kld_loss_scale)\n",
    "        \n",
    "        ae_train_loss_per_epoch = []\n",
    "        ae_rec_loss_per_epoch = []\n",
    "        ae_kld_loss_per_epoch = []\n",
    "        \n",
    "        for train_batch in dataloader:\n",
    "            train_batch = train_batch.to(device)\n",
    "            train_batch = vocos.feature_extractor(train_batch.unsqueeze(1))\n",
    "            \n",
    "            _ae_loss, _ae_rec_loss, _ae_kld_loss = ae_train_step(train_batch)\n",
    "            \n",
    "            _ae_loss = _ae_loss.detach().cpu().numpy()\n",
    "            _ae_rec_loss = _ae_rec_loss.detach().cpu().numpy()\n",
    "            _ae_kld_loss = _ae_kld_loss.detach().cpu().numpy()\n",
    "            \n",
    "            #print(\"_ae_prior_loss \", _ae_prior_loss)\n",
    "            \n",
    "            ae_train_loss_per_epoch.append(_ae_loss)\n",
    "            ae_rec_loss_per_epoch.append(_ae_rec_loss)\n",
    "            ae_kld_loss_per_epoch.append(_ae_kld_loss)\n",
    "\n",
    "        ae_train_loss_per_epoch = np.mean(np.array(ae_train_loss_per_epoch))\n",
    "        ae_rec_loss_per_epoch = np.mean(np.array(ae_rec_loss_per_epoch))\n",
    "        ae_kld_loss_per_epoch = np.mean(np.array(ae_kld_loss_per_epoch))\n",
    "        \n",
    "        if epoch % model_save_interval == 0 and save_weights == True:\n",
    "            torch.save(encoder.state_dict(), \"results/weights/encoder_weights_epoch_{}\".format(epoch))\n",
    "            torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epoch))\n",
    "        \n",
    "        loss_history[\"ae train\"].append(ae_train_loss_per_epoch)\n",
    "        loss_history[\"ae rec\"].append(ae_rec_loss_per_epoch)\n",
    "        loss_history[\"ae kld\"].append(ae_kld_loss_per_epoch)\n",
    "        \n",
    "        print ('epoch {} : ae train: {:01.4f} rec {:01.4f} kld {:01.4f} time {:01.2f}'.format(epoch + 1, ae_train_loss_per_epoch, ae_rec_loss_per_epoch, ae_kld_loss_per_epoch, time.time()-start))\n",
    "    \n",
    "        ae_scheduler.step()\n",
    "        \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ee69d-e0f7-4a5f-b67e-479ce60826fb",
   "metadata": {},
   "source": [
    "## Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ca1399-ce2d-4f55-98f5-5d7b172fbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = train(dataloader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5110d64-1de5-4176-82b3-22f1d9489974",
   "metadata": {},
   "source": [
    "## Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9783d0-c32c-4dc3-a6bc-af266e0c64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss_as_image(loss_history, image_file_name):\n",
    "    keys = list(loss_history.keys())\n",
    "    epochs = len(loss_history[keys[0]])\n",
    "    \n",
    "    for key in keys:\n",
    "        plt.plot(range(epochs), loss_history[key], label=key)\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(image_file_name)\n",
    "    plt.show()\n",
    "\n",
    "def save_loss_as_csv(loss_history, csv_file_name):\n",
    "    with open(csv_file_name, 'w') as csv_file:\n",
    "        csv_columns = list(loss_history.keys())\n",
    "        csv_row_count = len(loss_history[csv_columns[0]])\n",
    "        \n",
    "        \n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=',', lineterminator='\\n')\n",
    "        csv_writer.writeheader()\n",
    "    \n",
    "        for row in range(csv_row_count):\n",
    "        \n",
    "            csv_row = {}\n",
    "        \n",
    "            for key in loss_history.keys():\n",
    "                csv_row[key] = loss_history[key][row]\n",
    "\n",
    "            csv_writer.writerow(csv_row)\n",
    "\n",
    "save_loss_as_csv(loss_history, \"results/histories/history_{}.csv\".format(epochs))\n",
    "save_loss_as_image(loss_history, \"results/histories/history_{}.png\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c03f550-4aa9-43e2-8367-a427659ca85a",
   "metadata": {},
   "source": [
    "## Save Final Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ddc07-ab41-4f25-a38c-f0389dc1240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"results/weights/encoder_weights_epoch_{}\".format(epochs))\n",
    "torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24edda33-1efb-4442-8ec1-9a2f340b0efa",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f60540e-e4cc-4a05-89b4-5e754944f4df",
   "metadata": {},
   "source": [
    "## Audio Reconstruction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75d043-ac5f-4760-aa49-583c24638391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ref_audio_window(waveform_window, file_name):\n",
    "\n",
    "    torchaudio.save(\"{}\".format(file_name), waveform_window, audio_sample_rate)\n",
    "\n",
    "def create_voc_audio_window(waveform_window, file_name):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        audio_features = vocos.feature_extractor(waveform_window.to(device))\n",
    "        waveform_window_voc = vocos.decode(audio_features)\n",
    "    \n",
    "    torchaudio.save(\"{}\".format(file_name), waveform_window_voc.detach().cpu(), audio_sample_rate)\n",
    "\n",
    "def create_pred_audio_window(waveform_window, file_name):\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        audio_features = vocos.feature_extractor(waveform_window.to(device))\n",
    "        audio_encoder_in = audio_features.unsqueeze(1)\n",
    "\n",
    "        audio_encoder_out_mu, audio_encoder_out_std = encoder(audio_encoder_in)\n",
    "        mu = audio_encoder_out_mu\n",
    "        std = torch.nn.functional.softplus(audio_encoder_out_std) + 1e-6\n",
    "        audio_encoder_out = encoder.reparameterize(mu, std)\n",
    "        audio_decoder_in = audio_encoder_out\n",
    "        audio_decoder_out = decoder(audio_decoder_in)\n",
    "        audio_features_pred = audio_decoder_out\n",
    "        audio_features_pred = audio_features_pred.squeeze(1)\n",
    "        audio_waveform_window_pred = vocos.decode(audio_features_pred)\n",
    "\n",
    "    torchaudio.save(\"{}\".format(file_name), audio_waveform_window_pred.detach().cpu(), audio_sample_rate)\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "def create_ref_audio(waveform, file_name):\n",
    "\n",
    "    torchaudio.save(\"{}\".format(file_name), waveform, audio_sample_rate)\n",
    "    \n",
    "    #print(\"waveform s \", waveform.shape)\n",
    "\n",
    "def create_voc_audio(waveform, file_name):\n",
    "    \n",
    "    waveform_length = waveform.shape[1]\n",
    "    audio_window_offset = audio_window_length // 2\n",
    "    audio_window_env = torch.hann_window(audio_window_length)\n",
    "    \n",
    "    audio_window_count = int(waveform_length - audio_window_length) // audio_window_offset\n",
    "    pred_audio_sequence = torch.zeros((waveform_length), dtype=torch.float32)\n",
    "    \n",
    "    #print(\"pred_audio_sequence s \", pred_audio_sequence.shape)\n",
    "\n",
    "    for i in range(audio_window_count):\n",
    "        \n",
    "        window_start = i * audio_window_offset\n",
    "        window_end = window_start + audio_window_length\n",
    "        \n",
    "        waveform_window = waveform[:, window_start:window_end]\n",
    "        \n",
    "        #print(\"i \", i, \" target_audio s \", target_audio.shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            audio_features = vocos.feature_extractor(waveform_window.to(device))\n",
    "            waveform_window_voc = vocos.decode(audio_features)\n",
    "\n",
    "        #print(\"voc_audio s \", voc_audio.shape)\n",
    "        #print(\"grain_env s \", grain_env.shape)\n",
    "        \n",
    "        waveform_window_voc = waveform_window_voc.detach().cpu()\n",
    "\n",
    "        pred_audio_sequence[i*audio_window_offset:i*audio_window_offset + audio_window_length] += waveform_window_voc[0] * audio_window_env\n",
    "\n",
    "    torchaudio.save(\"{}\".format(file_name), torch.reshape(pred_audio_sequence, (1, -1)), audio_sample_rate)\n",
    "\n",
    "def create_pred_audio(waveform, file_name):\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    waveform_length = waveform.shape[1]\n",
    "    audio_window_offset = audio_window_length // 2\n",
    "    audio_window_env = torch.hann_window(audio_window_length)\n",
    "    \n",
    "    audio_window_count = int(waveform_length - audio_window_length) // audio_window_offset\n",
    "    pred_audio_sequence = torch.zeros((waveform_length), dtype=torch.float32)\n",
    "    \n",
    "    #print(\"pred_audio_sequence s \", pred_audio_sequence.shape)\n",
    "    \n",
    "    for i in range(audio_window_count):\n",
    "        \n",
    "        window_start = i * audio_window_offset\n",
    "        window_end = window_start + audio_window_length\n",
    "        \n",
    "        waveform_window = waveform[:, window_start:window_end]\n",
    "        \n",
    "        #print(\"i \", i, \" waveform_window s \", waveform_window.shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            audio_features = vocos.feature_extractor(waveform_window.to(device))\n",
    "            audio_encoder_in = audio_features.unsqueeze(1)\n",
    "            \n",
    "            audio_encoder_out_mu, audio_encoder_out_std = encoder(audio_encoder_in)\n",
    "            mu = audio_encoder_out_mu\n",
    "            std = torch.nn.functional.softplus(audio_encoder_out_std) + 1e-6\n",
    "            \n",
    "            audio_encoder_out = encoder.reparameterize(mu, std)\n",
    "            \n",
    "            audio_decoder_in = audio_encoder_out\n",
    "            audio_decoder_out = decoder(audio_decoder_in)\n",
    "            audio_features_pred = audio_decoder_out\n",
    "            audio_features_pred = audio_features_pred.squeeze(1)\n",
    "            audio_waveform_window_pred = vocos.decode(audio_features_pred)\n",
    "\n",
    "        #print(\"voc_audio s \", voc_audio.shape)\n",
    "        #print(\"grain_env s \", grain_env.shape)\n",
    "        \n",
    "        audio_waveform_window_pred = audio_waveform_window_pred.detach().cpu()\n",
    "\n",
    "        pred_audio_sequence[i*audio_window_offset:i*audio_window_offset + audio_window_length] += audio_waveform_window_pred[0] * audio_window_env\n",
    "\n",
    "    torchaudio.save(\"{}\".format(file_name), torch.reshape(pred_audio_sequence, (1, -1)), audio_sample_rate)\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fe43b-16c3-43ba-9f50-cc45e1d7f167",
   "metadata": {},
   "source": [
    "## Perform Audio Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a84645d-17b9-406d-893f-78ff8b2405bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_file = audio_files[0]\n",
    "test_waveform_start_time = 50.0\n",
    "test_audio_start_times = [20, 120, 240]\n",
    "test_audio_duration = 20.0\n",
    "\n",
    "test_audio_file_gui = widgets.Dropdown(\n",
    "    options=[audio_file for audio_file in audio_files],\n",
    "    value=audio_files[0],  # default selected value\n",
    "    description='Audio File:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "test_waveform_start_time_gui = widgets.FloatText(value=test_waveform_start_time, description=\"Test Waveform Start Time (secs):\", style={'description_width': 'initial'})\n",
    "\n",
    "test_audio_start_times_gui = widgets.Textarea(\n",
    "    value=','.join(list(map(str, test_audio_start_times))),\n",
    "    placeholder='Test Audio Start Time (secs) separated by commas',\n",
    "    description='Test Audio Start Times:',\n",
    "    layout=widgets.Layout(width='50%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "test_audio_duration_gui = widgets.FloatText(value=test_audio_duration, description=\"Test Audio Duration (secs):\", style={'description_width': 'initial'})\n",
    "\n",
    "\n",
    "display(test_audio_file_gui)\n",
    "display(test_waveform_start_time_gui)\n",
    "display(test_audio_start_times_gui)\n",
    "display(test_audio_duration_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61daa60c-c21e-48b5-b1fd-c1ed334a7698",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_audio_file = test_audio_file_gui.value\n",
    "test_waveform_start_time = test_waveform_start_time_gui.value\n",
    "test_audio_start_times  = [int(s) for s in re.split(r\"\\s*,\\s*\", test_audio_start_times_gui.value) if s.strip()]\n",
    "test_audio_duration = test_audio_duration_gui.value\n",
    "\n",
    "test_waveform, _ = torchaudio.load(audio_file_path + \"/\" + test_audio_file)\n",
    "test_waveform_sample_index = int(audio_sample_rate * test_waveform_start_time)\n",
    "test_waveform_window = test_waveform[:, test_waveform_sample_index:test_waveform_sample_index+audio_window_length]\n",
    "\n",
    "create_ref_audio_window(test_waveform_window, \"results/audio/audio_window_orig.wav\")\n",
    "create_voc_audio_window(test_waveform_window, \"results/audio/audio_window_voc.wav\")\n",
    "create_pred_audio_window(test_waveform_window, \"results/audio/audio_window_pred_epoch_{}.wav\".format(epochs))\n",
    "\n",
    "for test_audio_start_time in test_audio_start_times:\n",
    "    start_time_sample_index = int(test_audio_start_time * audio_sample_rate)\n",
    "    end_time_sample_index = start_time_sample_index + int(test_audio_duration * audio_sample_rate)\n",
    "\n",
    "    create_ref_audio(test_waveform[:, start_time_sample_index:end_time_sample_index], \"results/audio/audio_ref_{}-{}.wav\".format(test_audio_start_time, (test_audio_start_time + test_audio_duration)))\n",
    "    create_voc_audio(test_waveform[:, start_time_sample_index:end_time_sample_index], \"results/audio/audio_voc_{}-{}.wav\".format(test_audio_start_time, (test_audio_start_time + test_audio_duration)))\n",
    "    create_pred_audio(test_waveform[:, start_time_sample_index:end_time_sample_index], \"results/audio/audio_pred_{}-{}_epoch_{}.wav\".format(test_audio_start_time, (test_audio_start_time + test_audio_duration), epochs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab8b3f-89d7-4f6e-971c-967194a81c25",
   "metadata": {},
   "source": [
    "## Latent Space Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2421a9ad-7e9d-4b9b-beef-44fa44141c49",
   "metadata": {},
   "source": [
    "## Audio Encode and Decode Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1877a93-cbf4-4d4f-95f0-060db32acbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_audio(waveform):\n",
    "    \n",
    "    encoder.eval()\n",
    "    \n",
    "    waveform_length = waveform.shape[1]\n",
    "    audio_window_offset = audio_window_length // 2\n",
    "    audio_window_count = int(waveform_length - audio_window_length) // audio_window_offset\n",
    "    \n",
    "    latent_vectors = []\n",
    "\n",
    "    for i in range(audio_window_count):\n",
    "        \n",
    "        window_start = i * audio_window_offset\n",
    "        window_end = window_start + audio_window_length\n",
    "        \n",
    "        waveform_window = waveform[:, window_start:window_end]\n",
    "        \n",
    "        #print(\"i \", i, \" waveform_window s \", waveform_window.shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            audio_features = vocos.feature_extractor(waveform_window.to(device))\n",
    "            audio_encoder_in = audio_features.unsqueeze(1)\n",
    "            \n",
    "            audio_encoder_out_mu, audio_encoder_out_std = encoder(audio_encoder_in)\n",
    "            mu = audio_encoder_out_mu\n",
    "            std = torch.nn.functional.softplus(audio_encoder_out_std) + 1e-6\n",
    "            \n",
    "            audio_encoder_out = encoder.reparameterize(mu, std)\n",
    "            \n",
    "        latent_vector = audio_encoder_out.squeeze(0)\n",
    "        latent_vector = latent_vector.detach().cpu().numpy()\n",
    "    \n",
    "        latent_vectors.append(latent_vector)\n",
    "    \n",
    "    encoder.train()\n",
    "        \n",
    "    return latent_vectors\n",
    "\n",
    "def decode_audio_encodings(encodings, file_name):\n",
    "    \n",
    "    decoder.eval()\n",
    "    \n",
    "    audio_window_offset = audio_window_length // 2\n",
    "    audio_window_env = torch.hann_window(audio_window_length)\n",
    "    \n",
    "    audio_window_count = len(encodings)\n",
    "    waveform_length = audio_window_count * audio_window_offset + audio_window_length\n",
    "    \n",
    "    pred_audio_sequence = torch.zeros((waveform_length), dtype=torch.float32)\n",
    "    \n",
    "    #print(\"pred_audio_sequence s \", pred_audio_sequence.shape)\n",
    "    \n",
    "    for i in range(audio_window_count):\n",
    "        \n",
    "        window_start = i * audio_window_offset\n",
    "        window_end = window_start + audio_window_length\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            audio_decoder_in = torch.Tensor(encodings[i]).unsqueeze(0).to(device)\n",
    "            audio_decoder_out = decoder(audio_decoder_in)\n",
    "            audio_features_pred = audio_decoder_out\n",
    "            audio_features_pred = audio_features_pred.squeeze(1)\n",
    "            audio_waveform_window_pred = vocos.decode(audio_features_pred)\n",
    "\n",
    "        #print(\"voc_audio s \", voc_audio.shape)\n",
    "        #print(\"grain_env s \", grain_env.shape)\n",
    "        \n",
    "        audio_waveform_window_pred = audio_waveform_window_pred.detach().cpu()\n",
    "\n",
    "        pred_audio_sequence[i*audio_window_offset:i*audio_window_offset + audio_window_length] += audio_waveform_window_pred[0] * audio_window_env\n",
    "\n",
    "    torchaudio.save(\"{}\".format(file_name), torch.reshape(pred_audio_sequence, (1, -1)), audio_sample_rate)\n",
    "\n",
    "    decoder.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914982d-8a29-4325-8f5e-a7a41aa091de",
   "metadata": {},
   "source": [
    "## Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82884919-7719-43c0-86a3-ea751c98360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_start_time = 20\n",
    "test_audio_duration = 20\n",
    "random_walk_step_scale = 0.1\n",
    "\n",
    "test_audio_start_time_gui = widgets.FloatText(value=test_audio_start_time, description=\"Test Audio Start Time (secs):\", style={'description_width': 'initial'})\n",
    "test_audio_duration_gui = widgets.FloatText(value=test_audio_duration, description=\"Test Audio Duration (secs):\", style={'description_width': 'initial'})\n",
    "random_walk_step_scale_gui = widgets.FloatText(value=random_walk_step_scale, description=\"Random Walk Step Scale:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(test_audio_start_time_gui)\n",
    "display(test_audio_duration_gui)\n",
    "display(random_walk_step_scale_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc7c2d-aab6-44b7-b8a9-ea39e6856404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_audio_start_time = test_audio_start_time_gui.value\n",
    "test_audio_duration = test_audio_duration_gui.value\n",
    "random_walk_step_scale = random_walk_step_scale_gui.value\n",
    "\n",
    "start_time_sample_index = int(test_audio_start_time * audio_sample_rate)\n",
    "end_time_sample_index = start_time_sample_index + int(test_audio_duration * audio_sample_rate)\n",
    "\n",
    "audio_window_offset = audio_window_length // 2\n",
    "\n",
    "latent_vectors = encode_audio(test_waveform[:, start_time_sample_index:start_time_sample_index + audio_window_length + audio_window_offset])\n",
    "audio_window_count = int(test_audio_duration * audio_sample_rate - audio_window_length) // audio_window_offset - 1\n",
    "\n",
    "for window_index in range(audio_window_count):\n",
    "    random_step = np.random.random((latent_dim)).astype(np.float32) * random_walk_step_scale\n",
    "    latent_vectors.append(latent_vectors[window_index] + random_step)\n",
    "\n",
    "decode_audio_encodings(latent_vectors, \"results/audio/randwalk_audio_epochs_{}_audio_{}-{}.wav\".format(epochs, test_audio_start_time, test_audio_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2beab66-2d52-4dd7-8588-00e6bdc57967",
   "metadata": {},
   "source": [
    "## Sequence Offset Following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed171fc5-4c29-4f5a-8f0d-e7205eed9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_start_time = 20\n",
    "test_audio_duration = 20\n",
    "offset_oscil_freq = 4.0\n",
    "offset_oscil_scale = 1.0\n",
    "\n",
    "test_audio_start_time_gui = widgets.FloatText(value=test_audio_start_time, description=\"Test Audio Start Time (secs):\", style={'description_width': 'initial'})\n",
    "test_audio_duration_gui = widgets.FloatText(value=test_audio_duration, description=\"Test Audio Duration (secs):\", style={'description_width': 'initial'})\n",
    "offset_oscil_freq_gui = widgets.FloatText(value=offset_oscil_freq, description=\"Offset Oscil Frequency:\", style={'description_width': 'initial'})\n",
    "offset_oscil_scale_gui = widgets.FloatText(value=offset_oscil_scale, description=\"Offset Oscil Scale:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(test_audio_start_time_gui)\n",
    "display(test_audio_duration_gui)\n",
    "display(offset_oscil_freq_gui)\n",
    "display(offset_oscil_scale_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65db0f2-33f3-4b25-8f85-eef725142386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_audio_start_time = test_audio_start_time_gui.value\n",
    "test_audio_duration = test_audio_duration_gui.value\n",
    "offset_oscil_freq = offset_oscil_freq_gui.value\n",
    "offset_oscil_scale = offset_oscil_scale_gui.value\n",
    "\n",
    "start_time_sample_index = int(test_audio_start_time * audio_sample_rate)\n",
    "end_time_sample_index = start_time_sample_index + int(test_audio_duration * audio_sample_rate)\n",
    "\n",
    "latent_vectors = encode_audio(test_waveform[:, start_time_sample_index:end_time_sample_index])\n",
    "\n",
    "offset_encodings = []\n",
    "\n",
    "for index in range(len(latent_vectors)):\n",
    "    sin_value = np.sin(index / (len(latent_vectors) - 1) * np.pi * offset_oscil_freq)\n",
    "    offset = np.ones(shape=(latent_dim), dtype=np.float32) * sin_value * offset_oscil_scale\n",
    "    offset_encoding = latent_vectors[index] + offset\n",
    "    offset_encodings.append(offset_encoding)\n",
    "    \n",
    "decode_audio_encodings(offset_encodings, \"results/audio/offset_audio_epochs_{}_audio_{}-{}.wav\".format(epochs, test_audio_start_time, test_audio_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5878e-0adf-460b-bb2a-43ba699d68bc",
   "metadata": {},
   "source": [
    "## Sequence Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa5d8a-c48f-4633-bad4-ad995602cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_audio_start_time = 20\n",
    "test2_audio_start_time = 60\n",
    "test_audio_duration = 20\n",
    "\n",
    "test1_audio_start_time_gui = widgets.FloatText(value=test1_audio_start_time, description=\"Test1 Audio Start Time (secs):\", style={'description_width': 'initial'})\n",
    "test2_audio_start_time_gui = widgets.FloatText(value=test2_audio_start_time, description=\"Test2 Audio Start Time (secs):\", style={'description_width': 'initial'})\n",
    "test_audio_duration_gui = widgets.FloatText(value=test_audio_duration, description=\"Test Audio Duration (secs):\", style={'description_width': 'initial'})\n",
    "\n",
    "display(test1_audio_start_time_gui)\n",
    "display(test2_audio_start_time_gui)\n",
    "display(test_audio_duration_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7fd57d-caeb-4cb7-8d61-1e86b57ab964",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_audio_start_time = test1_audio_start_time_gui.value\n",
    "test2_audio_start_time = test2_audio_start_time_gui.value\n",
    "test_audio_duration = test_audio_duration_gui.value\n",
    "\n",
    "start1_time_sample_index = int(test1_audio_start_time * audio_sample_rate)\n",
    "end1_time_sample_index = start1_time_sample_index + int(test_audio_duration * audio_sample_rate)\n",
    "\n",
    "start2_time_sample_index = int(test2_audio_start_time * audio_sample_rate)\n",
    "end2_time_sample_index = start2_time_sample_index + int(test_audio_duration * audio_sample_rate)\n",
    "\n",
    "latent_vectors_1 = encode_audio(test_waveform[:, start1_time_sample_index:end1_time_sample_index])\n",
    "latent_vectors_2 = encode_audio(test_waveform[:, start2_time_sample_index:end2_time_sample_index])\n",
    "\n",
    "mix_encodings = []\n",
    "\n",
    "for index in range(len(latent_vectors_1)):\n",
    "    mix_factor = index / (len(latent_vectors_1) - 1)\n",
    "    mix_encoding = latent_vectors_1[index] * (1.0 - mix_factor) + latent_vectors_2[index] * mix_factor\n",
    "    mix_encodings.append(mix_encoding)\n",
    "\n",
    "decode_audio_encodings(mix_encodings, \"results/audio/mix_audio_epochs_{}_audio1_{}-{}_audio2_{}-{}.wav\".format(epochs, test1_audio_start_time, test1_audio_start_time + test_audio_duration, test2_audio_start_time, test2_audio_start_time + test_audio_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abb81e-eac4-43a9-a196-b2a510d12972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
