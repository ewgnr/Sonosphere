{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fdfce1e-b7a8-4fff-829d-a9b86f669110",
   "metadata": {},
   "source": [
    "## Audio Autoencoder (CNN Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee34929-09b7-4134-9b3d-6adbcc0c86d8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848ca67-5fd9-451c-b69f-dc45b8fd1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import threading\n",
    "import queue\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.functional import highpass_biquad\n",
    "import sounddevice as sd\n",
    "\n",
    "from vocos import Vocos\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from PyQt5 import QtWidgets, QtCore\n",
    "import pyqtgraph as pg\n",
    "import sys\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384747ea-7ddf-4ca4-95a9-6c21e3d77266",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12e649-5fed-4f85-8ceb-a74f340fcfd2",
   "metadata": {},
   "source": [
    "### Device Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c210a2-8feb-48ae-b6e9-71cd5beb84c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device.upper()} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dda1ea-c0df-498c-bd39-0cfd464253d6",
   "metadata": {},
   "source": [
    "### Audio Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10138d-9e02-435d-af81-329d144bca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"../../../Data/Audio/Gutenberg/Night_and_Day_by_Virginia_Woolf_48khz.wav\"\n",
    "audio_sample_rate = 48000\n",
    "audio_channel_count = 1\n",
    "audio_window_length = 2048\n",
    "audio_output_device = 8 # windows: 7, macOS: 2\n",
    "max_audio_queue_length = 32\n",
    "\n",
    "audio_file_gui = widgets.Text(value=audio_file, description=\"Audio File:\", style={'description_width': 'initial'}) \n",
    "audio_sample_rate_gui = widgets.IntText(value=audio_sample_rate, description=\"Audio Sample Rate:\", style={'description_width': 'initial'})\n",
    "audio_channel_count_gui = widgets.IntText(value=audio_channel_count, description=\"Audio Channel Count:\", style={'description_width': 'initial'})\n",
    "audio_window_length_gui = widgets.IntText(value=audio_window_length, description=\"Audio Window Length:\", style={'description_width': 'initial'})\n",
    "audio_output_device_gui = widgets.IntText(value=audio_output_device, description=\"Audio Output Device:\", style={'description_width': 'initial'})\n",
    "max_audio_queue_length_gui = widgets.IntText(value=max_audio_queue_length, description=\"Max Audio Queue Length:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(audio_file_gui)\n",
    "display(audio_sample_rate_gui)\n",
    "display(audio_channel_count_gui)\n",
    "display(audio_window_length_gui)\n",
    "print(sd.query_devices())\n",
    "display(audio_output_device_gui)\n",
    "display(max_audio_queue_length_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745dbcea-438b-464e-9393-06c5ad018784",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = audio_file_gui.value\n",
    "audio_sample_rate = audio_sample_rate_gui.value\n",
    "audio_channel_count = audio_channel_count_gui.value\n",
    "audio_window_length = audio_window_length_gui.value\n",
    "audio_output_device = audio_output_device_gui.value\n",
    "max_audio_queue_length = max_audio_queue_length_gui.value\n",
    "\n",
    "gen_buffer_size = audio_window_length\n",
    "window_size = gen_buffer_size\n",
    "window_offset = window_size // 2\n",
    "play_buffer_size = window_size * 16\n",
    "playback_latency = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d0f22-a5b2-447d-b631-4ca95bd63a41",
   "metadata": {},
   "source": [
    "### Autoencoder Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8be659-6b91-4d3c-95cf-fa02b9fd4ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_latent_dim = 32\n",
    "ae_conv_channel_counts = [ 16, 32, 64, 128 ]\n",
    "ae_conv_kernel_size = (5, 3)\n",
    "ae_dense_layer_sizes = [ 512 ]\n",
    "ae_encoder_weights_file = \"../../../Data/Models/AudioTransformation/CNN/Gutenberg_Ephraim/weights/encoder_weights_epoch_400\"\n",
    "ae_decoder_weights_file = \"../../../Data/Models/AudioTransformation/CNN/Gutenberg_Ephraim/weights/decoder_weights_epoch_400\"\n",
    "\n",
    "ae_latent_dim_gui = widgets.IntText(value=ae_latent_dim, description=\"Latent Dimension:\", style={'description_width': 'initial'})\n",
    "\n",
    "ae_conv_channel_counts_gui = widgets.Textarea(\n",
    "    value=','.join(list(map(str, ae_conv_channel_counts))),\n",
    "    placeholder='Enter convolutional channel counts separated by commas',\n",
    "    description='Convolutional Channel Counts:',\n",
    "    layout=widgets.Layout(width='50%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "ae_conv_kernel_size_gui = widgets.Textarea(\n",
    "    value=','.join(list(map(str, ae_conv_kernel_size))),\n",
    "    placeholder='Enter convolutional kernel size separated by commas',\n",
    "    description='Convolutional Kernel Size:',\n",
    "    layout=widgets.Layout(width='50%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "ae_dense_layer_sizes_gui = widgets.Textarea(\n",
    "    value=','.join(list(map(str, ae_dense_layer_sizes))),\n",
    "    placeholder='Enter dense layer sizes separated by commas',\n",
    "    description='Dense Layer Sizes:',\n",
    "    layout=widgets.Layout(width='50%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "ae_encoder_weights_file_gui = widgets.Text(value=ae_encoder_weights_file, description=\"Encoder Weights File:\", style={'description_width': 'initial'}) \n",
    "ae_decoder_weights_file_gui = widgets.Text(value=ae_decoder_weights_file, description=\"Decoder Weights File:\", style={'description_width': 'initial'}) \n",
    "\n",
    "display(ae_latent_dim_gui)\n",
    "display(ae_conv_channel_counts_gui)\n",
    "display(ae_conv_kernel_size_gui)\n",
    "display(ae_dense_layer_sizes_gui)\n",
    "display(ae_encoder_weights_file_gui)\n",
    "display(ae_decoder_weights_file_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c69571-c0f1-4c6a-8416-97f00b111fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_latent_dim = ae_latent_dim_gui.value\n",
    "ae_conv_channel_counts  = [int(s) for s in re.split(r\"\\s*,\\s*\", ae_conv_channel_counts_gui.value) if s.strip()]\n",
    "ae_conv_kernel_size  = tuple([int(s) for s in re.split(r\"\\s*,\\s*\", ae_conv_kernel_size_gui.value) if s.strip()])\n",
    "ae_dense_layer_sizes  = [int(s) for s in re.split(r\"\\s*,\\s*\", ae_dense_layer_sizes_gui.value) if s.strip()]\n",
    "ae_encoder_weights_file = ae_encoder_weights_file_gui.value\n",
    "ae_decoder_weights_file = ae_decoder_weights_file_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b8e8d-cb1f-42aa-865a-cae288149dca",
   "metadata": {},
   "source": [
    "### 2D Projection Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b63842-d9af-41ad-aa10-c0f34961fee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'widgets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m projected_latents_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[1;32m----> 3\u001b[0m projected_latents_count_gui \u001b[38;5;241m=\u001b[39m \u001b[43mwidgets\u001b[49m\u001b[38;5;241m.\u001b[39mIntText(value\u001b[38;5;241m=\u001b[39mprojected_latents_count, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProjected Latents Count:\u001b[39m\u001b[38;5;124m\"\u001b[39m, style\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription_width\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      5\u001b[0m display(projected_latents_count_gui)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'widgets' is not defined"
     ]
    }
   ],
   "source": [
    "projected_latents_count = 2000\n",
    "\n",
    "projected_latents_count_gui = widgets.IntText(value=projected_latents_count, description=\"Projected Latents Count:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(projected_latents_count_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707845d-a1fd-44fc-8f8a-e9dd64d6caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_latents_count = projected_latents_count_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e962f9-71b0-4048-ba35-9918bc4c34c8",
   "metadata": {},
   "source": [
    "### Nearest Neighbors Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72218a45-5dbf-43d3-963a-d640b352fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 4\n",
    "\n",
    "n_neighbors_gui = widgets.IntText(value=n_neighbors, description=\"Nearest Neighbors Count:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(n_neighbors_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6a713-2d8e-422e-a490-09c73de8e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = n_neighbors_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4503d54-0c14-4685-8306-8f417b25ff7d",
   "metadata": {},
   "source": [
    "## Load Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b7a29-0dfd-4ea0-8693-fd2596eaa7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(audio_file), f\"Audio file not found: {audio_file}\"\n",
    "\n",
    "audio_waveform, _ = torchaudio.load(audio_file)\n",
    "audio_source_samples = audio_waveform[0].to(device)\n",
    "audio_source_frame_index = 0\n",
    "hann_window = torch.from_numpy(np.hanning(window_size)).float().to(device) # Move to device once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0e336-23c7-4fb2-b03d-03c5eae9aeb6",
   "metadata": {},
   "source": [
    "## Audio Excerpt Generation for 2D Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92db718-8a89-4b79-a68c-334c4796c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_excerpt_frame_offset = 10000\n",
    "\n",
    "audio_excerpt_frame_offset_gui = widgets.IntText(value=audio_excerpt_frame_offset, description=\"Audio Excerpt Frame Offset:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(audio_excerpt_frame_offset_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab8939-3c92-4111-812f-60e2bc7a1b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_excerpt_frame_offset = audio_waveform.shape[1] // projected_latents_count\n",
    "\n",
    "audio_excerpt_start_frame = 0\n",
    "audio_excerpt_end_frame = audio_waveform.shape[1]\n",
    "audio_excerpts = []\n",
    "for fI in range(audio_excerpt_start_frame, audio_excerpt_end_frame - gen_buffer_size, audio_excerpt_frame_offset):\n",
    "    audio_excerpt = audio_waveform[0, fI:fI + gen_buffer_size]\n",
    "    audio_excerpts.append(audio_excerpt)\n",
    "audio_excerpts = torch.stack(audio_excerpts, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f62b36-9e55-487d-9c14-4b37cd12da86",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd51382e-a437-47d6-82d2-20510b1bb170",
   "metadata": {},
   "source": [
    "## Create Vocoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b41c1f8-a8f9-4d88-a790-b9ff0b8d6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocos = Vocos.from_pretrained(\"kittn/vocos-mel-48khz-alpha1\").to(device)\n",
    "vocos.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb3cea7-5b44-49b0-a2cd-c35d29ed670d",
   "metadata": {},
   "source": [
    "## Determine Number of Mel Filters and Mel Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26527f-e2c2-4f14-a1a1-09433b0bd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dummy_features = vocos.feature_extractor(torch.rand(size=(1, gen_buffer_size), device=device))\n",
    "    mel_count = dummy_features.shape[-1]\n",
    "    mel_filters = dummy_features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1838208-98a4-4db5-b8f2-96ce145a73fd",
   "metadata": {},
   "source": [
    "## Create Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f26de-7720-42a1-bb64-cdc665a6c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, mel_count, mel_filter_count, conv_channel_counts, conv_kernel_size, dense_layer_sizes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.mel_count = mel_count\n",
    "        self.mel_filter_count = mel_filter_count\n",
    "        self.conv_channel_counts = conv_channel_counts\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.dense_layer_sizes = dense_layer_sizes\n",
    "        \n",
    "        # create convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        stride = ((self.conv_kernel_size[0] - 1) // 2, (self.conv_kernel_size[1] - 1) // 2)\n",
    "\n",
    "        padding = stride\n",
    "        \n",
    "        self.conv_layers.append(nn.Conv2d(1, conv_channel_counts[0], self.conv_kernel_size, stride=stride, padding=padding))\n",
    "        self.conv_layers.append(nn.LeakyReLU(0.2))\n",
    "        self.conv_layers.append(nn.BatchNorm2d(conv_channel_counts[0]))\n",
    "        \n",
    "        conv_layer_count = len(conv_channel_counts)\n",
    "        for layer_index in range(1, conv_layer_count):\n",
    "            self.conv_layers.append(nn.Conv2d(conv_channel_counts[layer_index-1], conv_channel_counts[layer_index], self.conv_kernel_size, stride=stride, padding=padding))\n",
    "            self.conv_layers.append(nn.LeakyReLU(0.2))\n",
    "            self.conv_layers.append(nn.BatchNorm2d(conv_channel_counts[layer_index]))\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # create dense layers\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        \n",
    "        last_conv_layer_size_x = int(mel_filter_count // np.power(stride[0], len(conv_channel_counts)))\n",
    "        last_conv_layer_size_y = int(mel_count // np.power(stride[1], len(conv_channel_counts)))\n",
    "\n",
    "        preflattened_size = [conv_channel_counts[-1], last_conv_layer_size_x, last_conv_layer_size_y]\n",
    "        \n",
    "        dense_layer_input_size = conv_channel_counts[-1] * last_conv_layer_size_x * last_conv_layer_size_y\n",
    "\n",
    "        self.dense_layers.append(nn.Linear(dense_layer_input_size, self.dense_layer_sizes[0]))\n",
    "        self.dense_layers.append(nn.ReLU())\n",
    "        \n",
    "        dense_layer_count = len(dense_layer_sizes)\n",
    "        for layer_index in range(1, dense_layer_count):\n",
    "            self.dense_layers.append(nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index]))\n",
    "            self.dense_layers.append(nn.ReLU())\n",
    "            \n",
    "        # create final dense layers\n",
    "        self.fc_mu = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n",
    "        self.fc_std = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for lI, layer in enumerate(self.conv_layers):\n",
    "            x = layer(x)\n",
    "        x = self.flatten(x)\n",
    "        for lI, layer in enumerate(self.dense_layers):\n",
    "            x = layer(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        std = self.fc_std(x)\n",
    "        \n",
    "        return mu, std\n",
    "    \n",
    "    @staticmethod\n",
    "    def reparameterize(mu, std):\n",
    "        z = mu + std*torch.randn_like(std)\n",
    "        return z\n",
    "\n",
    "encoder = Encoder(ae_latent_dim, mel_count, mel_filters, ae_conv_channel_counts, ae_conv_kernel_size, ae_dense_layer_sizes).to(device)\n",
    "encoder.load_state_dict(torch.load(ae_encoder_weights_file, map_location=device))\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aa4739-f038-41c5-ac3d-17d37c64d0ac",
   "metadata": {},
   "source": [
    "## Create Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7236f2-1366-434c-97fa-e9e05779ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, mel_count, mel_filters, conv_channel_counts, conv_kernel_size, dense_layer_sizes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.mel_count = mel_count\n",
    "        self.mel_filters = mel_filters\n",
    "        self.conv_channel_counts = conv_channel_counts\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.dense_layer_sizes = dense_layer_sizes\n",
    "        \n",
    "        # create dense layers\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        \n",
    "        stride = ((self.conv_kernel_size[0] - 1) // 2, (self.conv_kernel_size[1] - 1) // 2)\n",
    "\n",
    "        self.dense_layers.append(nn.Linear(latent_dim, self.dense_layer_sizes[0]))\n",
    "        self.dense_layers.append(nn.ReLU())\n",
    "        \n",
    "        dense_layer_count = len(dense_layer_sizes)\n",
    "        for layer_index in range(1, dense_layer_count):\n",
    "            self.dense_layers.append(nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index]))\n",
    "            self.dense_layers.append(nn.ReLU())\n",
    "            \n",
    "        last_conv_layer_size_x = int(mel_filters // np.power(stride[0], len(conv_channel_counts)))\n",
    "        last_conv_layer_size_y = int(mel_count // np.power(stride[1], len(conv_channel_counts)))\n",
    "\n",
    "        preflattened_size = [conv_channel_counts[0], last_conv_layer_size_x, last_conv_layer_size_y]\n",
    "\n",
    "        dense_layer_output_size = conv_channel_counts[0] * last_conv_layer_size_x * last_conv_layer_size_y\n",
    " \n",
    "        self.dense_layers.append(nn.Linear(self.dense_layer_sizes[-1], dense_layer_output_size))\n",
    "        self.dense_layers.append(nn.ReLU())\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=preflattened_size)\n",
    "        \n",
    "        # create convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        padding = stride\n",
    "        output_padding = (padding[0] - 1, padding[1] - 1) # does this universally work?\n",
    "        \n",
    "        conv_layer_count = len(conv_channel_counts)\n",
    "        for layer_index in range(1, conv_layer_count):\n",
    "            self.conv_layers.append(nn.BatchNorm2d(conv_channel_counts[layer_index-1]))\n",
    "            self.conv_layers.append(nn.ConvTranspose2d(conv_channel_counts[layer_index-1], conv_channel_counts[layer_index], self.conv_kernel_size, stride=stride, padding=padding, output_padding=output_padding))\n",
    "            self.conv_layers.append(nn.LeakyReLU(0.2))\n",
    "            \n",
    "        self.conv_layers.append(nn.BatchNorm2d(conv_channel_counts[-1]))\n",
    "        self.conv_layers.append(nn.ConvTranspose2d(conv_channel_counts[-1], 1, self.conv_kernel_size, stride=stride, padding=padding, output_padding=output_padding))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for lI, layer in enumerate(self.dense_layers):\n",
    "            x = layer(x)\n",
    "        x = self.unflatten(x)\n",
    "        for lI, layer in enumerate(self.conv_layers):\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "decoder = Decoder(ae_latent_dim, mel_count, mel_filters, list(reversed(ae_conv_channel_counts)), ae_conv_kernel_size, list(reversed(ae_dense_layer_sizes))).to(device)\n",
    "decoder.load_state_dict(torch.load(ae_decoder_weights_file, map_location=device))\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d50076-a0d3-4bd9-8511-fbff4ecad306",
   "metadata": {},
   "source": [
    "## Generate latent encodings and 2D projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8b008-c0d4-48c3-a69b-5a75598b13d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio_encodings = []\n",
    "    for eI in range(0, audio_excerpts.shape[0] - batch_size, batch_size):\n",
    "        batch = audio_excerpts[eI:eI+batch_size].to(device)\n",
    "        mels = vocos.feature_extractor(batch)\n",
    "        audio_encoder_in = mels.unsqueeze(1)\n",
    "        mu, std = encoder(audio_encoder_in)\n",
    "        std = torch.nn.functional.softplus(std) + 1e-6\n",
    "        encoded_batch = Encoder.reparameterize(mu, std).detach().cpu()\n",
    "        audio_encodings.append(encoded_batch)\n",
    "    audio_encodings = torch.cat(audio_encodings, dim=0).numpy()\n",
    "tsne = TSNE(n_components=2, max_iter=5000, verbose=1)\n",
    "Z_tsne = tsne.fit_transform(audio_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b07bf3e-b325-420f-90e4-d855a001c593",
   "metadata": {},
   "source": [
    "## Display 2D Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdebdda2-4e78-47b4-9f38-89d5be3a6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_tsne_x = Z_tsne[:,0]\n",
    "Z_tsne_y = Z_tsne[:,1]\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Z_tsne_x, Z_tsne_y, s=0.1, c=\"grey\", alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd49f6-a9ec-4b98-b011-a006841a346c",
   "metadata": {},
   "source": [
    "## K-Nearest-Neighbors Search based on Mouse Clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09171f-0170-4805-aa14-e0ee605226f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto', metric='euclidean')\n",
    "knn.fit(Z_tsne)\n",
    "\n",
    "def calc_distance_based_averaged_encoding(point2D):\n",
    "    \"\"\"Returns distance-weighted averaged encoding for a given 2D point.\"\"\"\n",
    "    _, indices = knn.kneighbors(point2D)\n",
    "    nearest_positions = Z_tsne[indices[0]]\n",
    "    nearest_encodings = audio_encodings[indices[0]]\n",
    "    nearest_2D_distances = np.linalg.norm(nearest_positions - point2D, axis=1)\n",
    "    max_2D_distance = np.max(nearest_2D_distances)\n",
    "    norm_nearest_2D_distances = nearest_2D_distances / max_2D_distance\n",
    "    weights = (1.0 - norm_nearest_2D_distances)\n",
    "    return np.average(nearest_encodings, weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a236cbb-57ba-49b2-9333-deefcdcf08a5",
   "metadata": {},
   "source": [
    "## Real-Time Audio Callback Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece5169-810a-418a-bf1b-c318ab649154",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_audio_encodings = []\n",
    "inter_audio_encoding_index = 0\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_audio(waveform):\n",
    "    waveform = waveform.unsqueeze(0).to(device)\n",
    "    mels = vocos.feature_extractor(waveform)\n",
    "    mu, std = encoder(mels.unsqueeze(1))\n",
    "    std = torch.nn.functional.softplus(std) + 1e-6\n",
    "    return Encoder.reparameterize(mu, std)\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_audio(latent):\n",
    "    mel_pred = decoder(latent).squeeze(1)\n",
    "    waveform = vocos.decode(mel_pred).reshape(1, -1)\n",
    "    return waveform\n",
    "\n",
    "@torch.no_grad()\n",
    "def synthesize_audio():\n",
    "    \"\"\"Decode the next latent encoding from the current interactive list.\"\"\"\n",
    "    global inter_audio_encoding_index\n",
    "    if len(inter_audio_encodings) == 0:\n",
    "        return torch.zeros(gen_buffer_size)\n",
    "    inter_audio_encoding_index += 1\n",
    "    if inter_audio_encoding_index >= len(inter_audio_encodings):\n",
    "        inter_audio_encoding_index = 0\n",
    "    latent = inter_audio_encodings[inter_audio_encoding_index]\n",
    "    gen_waveform = decode_audio(latent).reshape(-1)\n",
    "    return gen_waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db698f9-af3d-4feb-931a-d918639bd645",
   "metadata": {},
   "source": [
    "## Audio Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10fb3a2-3ccc-4d2e-922f-2910c976fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_queue = queue.Queue(maxsize=max_audio_queue_length)\n",
    "export_audio_buffer = []\n",
    "last_chunk = np.zeros(window_size, dtype=np.float32)\n",
    "\n",
    "def producer_thread():\n",
    "    \"\"\"Continuously generates audio and fills the output buffer queue.\"\"\"\n",
    "    while True:\n",
    "        if not audio_queue.full():\n",
    "            gen_waveform = synthesize_audio()\n",
    "            audio_queue.put(gen_waveform.cpu().numpy())\n",
    "        else:\n",
    "            sd.sleep(10)\n",
    "\n",
    "def audio_callback(out_data, frames, time_info, status):\n",
    "    \"\"\"sounddevice stream callback function.\"\"\"\n",
    "    global last_chunk\n",
    "    output = np.zeros((frames, audio_channel_count), dtype=np.float32)\n",
    "    cursor = 0\n",
    "    overlap_len = window_size // 2\n",
    "    output[cursor:cursor+overlap_len, 0] += last_chunk[overlap_len:]\n",
    "    samples_needed = frames\n",
    "    while samples_needed > 0:\n",
    "        try:\n",
    "            chunk = audio_queue.get_nowait()\n",
    "            chunk = (chunk * hann_window.cpu().numpy())\n",
    "        except queue.Empty:\n",
    "            chunk = np.zeros(window_size, dtype=np.float32)\n",
    "        chunk_copy_size = output[cursor:cursor+window_size, 0].shape[0]\n",
    "        output[cursor:cursor+chunk_copy_size, 0] += chunk[:chunk_copy_size]\n",
    "        cursor += window_size // 2\n",
    "        samples_needed = frames - cursor\n",
    "        last_chunk[:] = chunk[:]\n",
    "    out_data[:] = output\n",
    "\n",
    "def run_audio_stream():\n",
    "    \"\"\"Main thread for launching audio producer and output stream.\"\"\"\n",
    "    threading.Thread(target=producer_thread, daemon=True).start()\n",
    "    sd.sleep(2000)\n",
    "    with sd.OutputStream(\n",
    "        samplerate=audio_sample_rate,\n",
    "        device=audio_output_device,\n",
    "        channels=audio_channel_count,\n",
    "        callback=audio_callback,\n",
    "        blocksize=play_buffer_size,\n",
    "        latency=playback_latency\n",
    "    ):\n",
    "        print(\"Streaming audio with FIFO queue... press Ctrl+C to stop.\")\n",
    "        try:\n",
    "            while True:\n",
    "                sd.sleep(1000)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nStopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39cd20-62d3-49d3-a197-55108412cf37",
   "metadata": {},
   "source": [
    "## Scatter Plot Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14df2a-5aac-4dc6-8546-1a43bb1d8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScatterPlotApp(QtWidgets.QMainWindow):\n",
    "    \"\"\"\n",
    "    Interactive scatter plot GUI for latent encodings.\n",
    "    - Left button press/drag: Add points and encodings.\n",
    "    - Right button press/drag: Remove points and corresponding encodings near cursor.\n",
    "    - Middle button: pan plot.\n",
    "    - 'C' key: clear all points/encodings.\n",
    "    \"\"\"\n",
    "    def __init__(self, points2D, inter_audio_encodings):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"2D Scatter Plot with Click and Drag\")\n",
    "        # Container and layout for pyqtgraph widget\n",
    "        container_widget = QtWidgets.QWidget()\n",
    "        layout = QtWidgets.QVBoxLayout(container_widget)\n",
    "        self.graph_widget = pg.PlotWidget()\n",
    "        layout.addWidget(self.graph_widget)\n",
    "        self.setCentralWidget(container_widget)\n",
    "\n",
    "        # Main scatter item (fixed background points)\n",
    "        self.scatter = pg.ScatterPlotItem(\n",
    "            x=points2D[:, 0],\n",
    "            y=points2D[:, 1],\n",
    "            pen=pg.mkPen(None),\n",
    "            brush=pg.mkBrush(100, 100, 255, 120),\n",
    "            size=10\n",
    "        )\n",
    "        self.graph_widget.addItem(self.scatter)\n",
    "\n",
    "        # Interactive points and encodings (red points are user-selected)\n",
    "        self.click_points = []\n",
    "        self.click_scatter = pg.ScatterPlotItem(size=12, brush=pg.mkBrush(255, 0, 0, 200))\n",
    "        self.graph_widget.addItem(self.click_scatter)\n",
    "        \n",
    "        # Audio playback point\n",
    "        self.play_point = []\n",
    "        self.play_scatter = pg.ScatterPlotItem(size=12, brush=pg.mkBrush(0, 255, 0, 200))\n",
    "        self.graph_widget.addItem(self.play_scatter)\n",
    "\n",
    "        # Interaction state\n",
    "        self.left_button_pressed = False\n",
    "        self.middle_button_pressed = False\n",
    "        self.right_button_pressed = False\n",
    "        self.last_mouse_pos = None\n",
    "\n",
    "        # Timer for continuous add (while dragging left button)\n",
    "        self.timer = QtCore.QTimer()\n",
    "        self.timer.setInterval(100)  # ms\n",
    "        self.timer.timeout.connect(self.continuous_add_point)\n",
    "        \n",
    "        # Timer for updating play point\n",
    "        self.playpoint_timer = QtCore.QTimer()\n",
    "        self.playpoint_timer.setInterval(25)  # ms\n",
    "        self.playpoint_timer.timeout.connect(self.update_play_point)\n",
    "        self.playpoint_timer.start()\n",
    "\n",
    "        # Register event filter for mouse events\n",
    "        self.graph_widget.scene().installEventFilter(self)\n",
    "        self.click_encodings = inter_audio_encodings\n",
    "\n",
    "    def addInteractiveEncoding(self, mouse_point):\n",
    "        \"\"\"Add an encoding for the clicked position.\"\"\"\n",
    "        encoding = calc_distance_based_averaged_encoding(np.array([[mouse_point.x(), mouse_point.y()]]))\n",
    "        encoding = torch.from_numpy(encoding).unsqueeze(0).to(torch.float32).to(device)\n",
    "        self.click_encodings.append(encoding)\n",
    "\n",
    "    def removeInteractiveEncodingNear(self, mouse_point, radius=0.5):\n",
    "        \"\"\"Remove points and encodings within a radius of the mouse cursor.\"\"\"\n",
    "        to_remove_indices = []\n",
    "        mp_x, mp_y = mouse_point.x(), mouse_point.y()\n",
    "        for i, p in enumerate(self.click_points):\n",
    "            dx = p['pos'][0] - mp_x\n",
    "            dy = p['pos'][1] - mp_y\n",
    "            dist = (dx*dx + dy*dy)**0.5\n",
    "            if dist < radius:\n",
    "                to_remove_indices.append(i)\n",
    "        # Remove from lists in reverse order for safety\n",
    "        for idx in reversed(to_remove_indices):\n",
    "            self.click_points.pop(idx)\n",
    "            self.click_encodings.pop(idx)\n",
    "        # Update plot\n",
    "        self.click_scatter.setData(\n",
    "            [p['pos'][0] for p in self.click_points],\n",
    "            [p['pos'][1] for p in self.click_points]\n",
    "        )\n",
    "\n",
    "    def clearInteractiveEncodings(self):\n",
    "        \"\"\"Clear all interactive points and encodings.\"\"\"\n",
    "        self.click_points.clear()\n",
    "        self.click_scatter.setData([], [])\n",
    "        self.click_encodings.clear()\n",
    "\n",
    "    def eventFilter(self, source, event):\n",
    "        \"\"\"Handle mouse events for the plot.\"\"\"\n",
    "        if source == self.graph_widget.scene():\n",
    "            if event.type() == QtCore.QEvent.GraphicsSceneMousePress:\n",
    "                if event.button() == QtCore.Qt.LeftButton:\n",
    "                    self.left_button_pressed = True\n",
    "                    self.last_mouse_pos = event.scenePos()\n",
    "                    self.add_point_at(event.scenePos())\n",
    "                    self.timer.start()\n",
    "                    return True\n",
    "                elif event.button() == QtCore.Qt.MiddleButton:\n",
    "                    self.middle_button_pressed = True\n",
    "                    self.last_mouse_pos = event.scenePos()\n",
    "                    return True\n",
    "                elif event.button() == QtCore.Qt.RightButton:\n",
    "                    self.right_button_pressed = True\n",
    "                    self.last_mouse_pos = event.scenePos()\n",
    "                    return True\n",
    "            elif event.type() == QtCore.QEvent.GraphicsSceneMouseRelease:\n",
    "                if event.button() == QtCore.Qt.LeftButton:\n",
    "                    self.left_button_pressed = False\n",
    "                    self.timer.stop()\n",
    "                    return True\n",
    "                elif event.button() == QtCore.Qt.MiddleButton:\n",
    "                    self.middle_button_pressed = False\n",
    "                    return True\n",
    "                elif event.button() == QtCore.Qt.RightButton:\n",
    "                    self.right_button_pressed = False\n",
    "                    return True\n",
    "            elif event.type() == QtCore.QEvent.GraphicsSceneMouseMove:\n",
    "                if self.left_button_pressed:\n",
    "                    self.last_mouse_pos = event.scenePos()\n",
    "                    return True\n",
    "                elif self.middle_button_pressed:\n",
    "                    if self.last_mouse_pos is not None:\n",
    "                        diff = event.scenePos() - self.last_mouse_pos\n",
    "                        vb = self.graph_widget.plotItem.vb\n",
    "                        vb.translateBy(x=-diff.x(), y=diff.y())\n",
    "                        self.last_mouse_pos = event.scenePos()\n",
    "                    return True\n",
    "                elif self.right_button_pressed:\n",
    "                    # Remove points near mouse position on drag\n",
    "                    vb = self.graph_widget.plotItem.vb\n",
    "                    mouse_point = vb.mapSceneToView(event.scenePos())\n",
    "                    self.removeInteractiveEncodingNear(mouse_point)\n",
    "                    self.last_mouse_pos = event.scenePos()\n",
    "                    return True\n",
    "        return super().eventFilter(source, event)\n",
    "\n",
    "    def keyPressEvent(self, event):\n",
    "        \"\"\"Clear points/encodings on 'C' key.\"\"\"\n",
    "        if event.key() == QtCore.Qt.Key_C:\n",
    "            self.clearInteractiveEncodings()\n",
    "        super().keyPressEvent(event)\n",
    "\n",
    "    def add_point_at(self, scene_pos):\n",
    "        \"\"\"Add a scatter point and encoding at mouse position.\"\"\"\n",
    "        if self.graph_widget.sceneBoundingRect().contains(scene_pos):\n",
    "            vb = self.graph_widget.plotItem.vb\n",
    "            mouse_point = vb.mapSceneToView(scene_pos)\n",
    "            self.addInteractiveEncoding(mouse_point)\n",
    "            x, y = mouse_point.x(), mouse_point.y()\n",
    "            self.click_points.append({'pos': (x, y)})\n",
    "            self.click_scatter.setData(\n",
    "                [p['pos'][0] for p in self.click_points],\n",
    "                [p['pos'][1] for p in self.click_points]\n",
    "            )\n",
    "\n",
    "    def continuous_add_point(self):\n",
    "        \"\"\"Timer-driven continuous interactive addition while dragging.\"\"\"\n",
    "        if self.left_button_pressed and self.last_mouse_pos is not None:\n",
    "            self.add_point_at(self.last_mouse_pos)\n",
    "            \n",
    "    def update_play_point(self):\n",
    "        \n",
    "        global inter_audio_encoding_index\n",
    "        \n",
    "        if inter_audio_encoding_index >= len(self.click_points):\n",
    "            return\n",
    "        \n",
    "        play_p = self.click_points[inter_audio_encoding_index]\n",
    "\n",
    "        self.play_scatter.setData( [ play_p['pos'][0] ], [ play_p['pos'][1] ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53939bdf-d469-40d9-910b-a3c3e9b1ded6",
   "metadata": {},
   "source": [
    "## Run Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4120ee8-2ef2-4ffd-a670-c8155a0f9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run audio stream in separate thread for GUI/audio concurrency\n",
    "audio_thread = threading.Thread(target=run_audio_stream, daemon=True)\n",
    "audio_thread.start()\n",
    "app = QtWidgets.QApplication(sys.argv)\n",
    "main = ScatterPlotApp(Z_tsne, inter_audio_encodings)\n",
    "main.show()\n",
    "sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e09aa-292a-4dd1-922b-d22073acea2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
