{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89f64e4-544b-4ace-a668-5044cfe9afca",
   "metadata": {},
   "source": [
    "# Audio Autoencoder (CNN Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b894044-f44b-40ca-bafb-7eea7031a862",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac774dbe-01f1-4a78-837b-736a5e96b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import threading\n",
    "import queue\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import torchaudio\n",
    "from torchaudio.functional import highpass_biquad\n",
    "\n",
    "from vocos import Vocos\n",
    "\n",
    "import sounddevice as sd\n",
    "\n",
    "from pythonosc import dispatcher\n",
    "from pythonosc import osc_server\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15815af-03c5-40d1-991d-05afccf4e589",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b97784-fc81-4f33-bbf8-5f90ce9c7445",
   "metadata": {},
   "source": [
    "## Device Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c257d5-a50c-48c7-af22-b883db17895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device.upper()} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286a167-e8c3-4129-bd36-a3b7b0f2b7af",
   "metadata": {},
   "source": [
    "## Audio Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126aaad-51c2-42a4-9421-f82506565626",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_1 = \"../../../Data/Audio/Gutenberg/Night_and_Day_by_Virginia_Woolf_48khz.wav\"\n",
    "audio_file_2 = \"../../../Data/Audio/Ephraim/RareBird_Mono_48.wav\"\n",
    "audio_sample_rate = 48000\n",
    "audio_channel_count = 1\n",
    "audio_buffer_size = 2048\n",
    "audio_output_device = 8 # windows: 7, macOs: 2\n",
    "max_audio_queue_length = 32\n",
    "\n",
    "audio_file_1_gui = widgets.Text(value=audio_file_1, description=\"Audio File 1:\", style={'description_width': 'initial'}) \n",
    "audio_file_2_gui = widgets.Text(value=audio_file_2, description=\"Audio File 2:\", style={'description_width': 'initial'}) \n",
    "audio_sample_rate_gui = widgets.IntText(value=audio_sample_rate, description=\"Audio Sample Rate:\", style={'description_width': 'initial'})\n",
    "audio_channel_count_gui = widgets.IntText(value=audio_channel_count, description=\"Audio Channel Count:\", style={'description_width': 'initial'})\n",
    "audio_buffer_size_gui = widgets.IntText(value=audio_buffer_size, description=\"Audio Buffer Size:\", style={'description_width': 'initial'})\n",
    "audio_output_device_gui = widgets.IntText(value=audio_output_device, description=\"Audio Output Device:\", style={'description_width': 'initial'})\n",
    "max_audio_queue_length_gui = widgets.IntText(value=max_audio_queue_length, description=\"Maximum Audio Queue Length:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(audio_file_1_gui)\n",
    "display(audio_file_2_gui)\n",
    "display(audio_sample_rate_gui)\n",
    "display(audio_channel_count_gui)\n",
    "display(audio_buffer_size_gui)\n",
    "print(sd.query_devices())\n",
    "display(audio_output_device_gui)\n",
    "display(max_audio_queue_length_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f2353-8c53-4dd5-992a-336fb7ee4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_1 = audio_file_1_gui.value\n",
    "audio_file_2 = audio_file_2_gui.value\n",
    "audio_sample_rate = audio_sample_rate_gui.value\n",
    "audio_channel_count = audio_channel_count_gui.value\n",
    "audio_buffer_size = audio_buffer_size_gui.value\n",
    "audio_output_device = audio_output_device_gui.value\n",
    "max_audio_queue_length = max_audio_queue_length_gui.value\n",
    "\n",
    "# automated settings\n",
    "gen_buffer_size = audio_buffer_size\n",
    "window_size = gen_buffer_size\n",
    "window_offset = window_size // 2\n",
    "play_buffer_size = window_size * 16\n",
    "playback_latency = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5c3f4-4eda-43d7-8094-5d9ec738260a",
   "metadata": {},
   "source": [
    "## Autoencoder Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11c0d9-26f1-428f-99d1-2f2c20c4bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_latent_dim = 32\n",
    "ae_conv_channel_counts = [ 16, 32, 64, 128 ]\n",
    "ae_conv_kernel_size = (5, 3)\n",
    "ae_dense_layer_sizes = [ 512 ]\n",
    "ae_encoder_weights_file = \"../../../Data/Models/AudioTransformation/CNN/Gutenberg_Ephraim/weights/encoder_weights_epoch_400\"\n",
    "ae_decoder_weights_file = \"../../../Data/Models/AudioTransformation/CNN/Gutenberg_Ephraim/weights/decoder_weights_epoch_400\"\n",
    "\n",
    "ae_latent_dim_gui = widgets.IntText(value=ae_latent_dim, description=\"Latent Dimension:\", style={'description_width': 'initial'})\n",
    "\n",
    "ae_conv_channel_counts_gui = widgets.Textarea(\n",
    "    value=','.join(list(map(str, ae_conv_channel_counts))),\n",
    "    placeholder='Enter convolutional channel counts separated by commas',\n",
    "    description='Convolutional Channel Counts:',\n",
    "    layout=widgets.Layout(width='50%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "ae_conv_kernel_size_gui = widgets.Textarea(\n",
    "    value=','.join(list(map(str, ae_conv_kernel_size))),\n",
    "    placeholder='Enter convolutional kernel size separated by commas',\n",
    "    description='Convolutional Kernel Size:',\n",
    "    layout=widgets.Layout(width='50%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "ae_dense_layer_sizes_gui = widgets.Textarea(\n",
    "    value=','.join(list(map(str, ae_dense_layer_sizes))),\n",
    "    placeholder='Enter dense layer sizes separated by commas',\n",
    "    description='Dense Layer Sizes:',\n",
    "    layout=widgets.Layout(width='50%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "ae_encoder_weights_file_gui = widgets.Text(value=ae_encoder_weights_file, description=\"Encoder Weights File:\", style={'description_width': 'initial'}) \n",
    "ae_decoder_weights_file_gui = widgets.Text(value=ae_decoder_weights_file, description=\"Decoder Weights File:\", style={'description_width': 'initial'}) \n",
    "\n",
    "display(ae_latent_dim_gui)\n",
    "display(ae_conv_channel_counts_gui)\n",
    "display(ae_conv_kernel_size_gui)\n",
    "display(ae_dense_layer_sizes_gui)\n",
    "display(ae_encoder_weights_file_gui)\n",
    "display(ae_decoder_weights_file_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2321d-c110-423b-bfb9-1c8efd713b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_latent_dim = ae_latent_dim_gui.value\n",
    "ae_conv_channel_counts  = [int(s) for s in re.split(r\"\\s*,\\s*\", ae_conv_channel_counts_gui.value) if s.strip()]\n",
    "ae_conv_kernel_size  = tuple([int(s) for s in re.split(r\"\\s*,\\s*\", ae_conv_kernel_size_gui.value) if s.strip()])\n",
    "ae_dense_layer_sizes  = [int(s) for s in re.split(r\"\\s*,\\s*\", ae_dense_layer_sizes_gui.value) if s.strip()]\n",
    "ae_encoder_weights_file = ae_encoder_weights_file_gui.value\n",
    "ae_decoder_weights_file = ae_decoder_weights_file_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9edb58f-c4ab-433f-8f1b-d89fb3dd37ae",
   "metadata": {},
   "source": [
    "## OSC Control Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd131d0-7cec-4856-8c56-39b60c888068",
   "metadata": {},
   "outputs": [],
   "source": [
    "osc_receive_ip = \"0.0.0.0\"\n",
    "osc_receive_port = 9005\n",
    "\n",
    "osc_receive_ip_gui = widgets.Text(value=osc_receive_ip, description=\"OSC Receive IP:\", style={'description_width': 'initial'}) \n",
    "osc_receive_port_gui = widgets.IntText(value=osc_receive_port, description=\"OSC Receive Port:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(osc_receive_ip_gui)\n",
    "display(osc_receive_port_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135454b-d943-4b2d-9a8c-90e1a5e87e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "osc_receive_ip = osc_receive_ip_gui.value\n",
    "osc_receive_port = osc_receive_port_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa3582d-9e79-4f5f-aff9-e1b617b30515",
   "metadata": {},
   "source": [
    "## Load Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787668a-64ff-408c-95d0-4726b2ea67cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(audio_file_1), f\"Audio file 1 not found: {audio_file_1}\"\n",
    "assert os.path.exists(audio_file_2), f\"Audio file 2 not found: {audio_file_2}\"\n",
    "\n",
    "audio_waveform_1, _ = torchaudio.load(audio_file_1)\n",
    "audio_source_samples_1 = audio_waveform_1[0].to(device)\n",
    "audio_source_frame_index_1 = 0\n",
    "\n",
    "audio_waveform_2, _ = torchaudio.load(audio_file_2)\n",
    "audio_source_samples_2 = audio_waveform_2[0].to(device)\n",
    "audio_source_frame_index_2 = 0\n",
    "\n",
    "hann_window = torch.from_numpy(np.hanning(window_size)).float().to(device) # Move to device once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e30f4-3039-4620-9826-7db2081c2c4c",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e536b94-e5a0-4887-9001-b7740658aa55",
   "metadata": {},
   "source": [
    "## Load Vocos Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c6765d-6bf3-4a39-8c73-d702a69ea778",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocos = Vocos.from_pretrained(\"kittn/vocos-mel-48khz-alpha1\").to(device)\n",
    "vocos.eval()\n",
    "with torch.no_grad():\n",
    "    dummy_features = vocos.feature_extractor(torch.rand(size=(1, gen_buffer_size), device=device))\n",
    "    mel_count = dummy_features.shape[-1]\n",
    "    mel_filters = dummy_features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3346cbb-22bd-4df2-a290-5eadffc981cd",
   "metadata": {},
   "source": [
    "## Create VAE Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a220dd4-8509-4d85-8cfe-91915ed912c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, mel_count, mel_filter_count, conv_channel_counts, conv_kernel_size, dense_layer_sizes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.mel_count = mel_count\n",
    "        self.mel_filter_count = mel_filter_count\n",
    "        self.conv_channel_counts = conv_channel_counts\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.dense_layer_sizes = dense_layer_sizes\n",
    "        \n",
    "        # create convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        stride = ((self.conv_kernel_size[0] - 1) // 2, (self.conv_kernel_size[1] - 1) // 2)\n",
    "\n",
    "        padding = stride\n",
    "        \n",
    "        self.conv_layers.append(nn.Conv2d(1, conv_channel_counts[0], self.conv_kernel_size, stride=stride, padding=padding))\n",
    "        self.conv_layers.append(nn.LeakyReLU(0.2))\n",
    "        self.conv_layers.append(nn.BatchNorm2d(conv_channel_counts[0]))\n",
    "        \n",
    "        conv_layer_count = len(conv_channel_counts)\n",
    "        for layer_index in range(1, conv_layer_count):\n",
    "            self.conv_layers.append(nn.Conv2d(conv_channel_counts[layer_index-1], conv_channel_counts[layer_index], self.conv_kernel_size, stride=stride, padding=padding))\n",
    "            self.conv_layers.append(nn.LeakyReLU(0.2))\n",
    "            self.conv_layers.append(nn.BatchNorm2d(conv_channel_counts[layer_index]))\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # create dense layers\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        \n",
    "        last_conv_layer_size_x = int(mel_filter_count // np.power(stride[0], len(conv_channel_counts)))\n",
    "        last_conv_layer_size_y = int(mel_count // np.power(stride[1], len(conv_channel_counts)))\n",
    "\n",
    "        preflattened_size = [conv_channel_counts[-1], last_conv_layer_size_x, last_conv_layer_size_y]\n",
    "        \n",
    "        dense_layer_input_size = conv_channel_counts[-1] * last_conv_layer_size_x * last_conv_layer_size_y\n",
    "\n",
    "        self.dense_layers.append(nn.Linear(dense_layer_input_size, self.dense_layer_sizes[0]))\n",
    "        self.dense_layers.append(nn.ReLU())\n",
    "        \n",
    "        dense_layer_count = len(dense_layer_sizes)\n",
    "        for layer_index in range(1, dense_layer_count):\n",
    "            self.dense_layers.append(nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index]))\n",
    "            self.dense_layers.append(nn.ReLU())\n",
    "            \n",
    "        # create final dense layers\n",
    "        self.fc_mu = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n",
    "        self.fc_std = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for lI, layer in enumerate(self.conv_layers):\n",
    "            x = layer(x)\n",
    "        x = self.flatten(x)\n",
    "        for lI, layer in enumerate(self.dense_layers):\n",
    "            x = layer(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        std = self.fc_std(x)\n",
    "        \n",
    "        return mu, std\n",
    "    \n",
    "    def reparameterize(self, mu, std):\n",
    "        z = mu + std*torch.randn_like(std)\n",
    "        return z\n",
    "\n",
    "encoder = Encoder(ae_latent_dim, mel_count, mel_filters, ae_conv_channel_counts, ae_conv_kernel_size, ae_dense_layer_sizes).to(device)\n",
    "encoder.load_state_dict(torch.load(ae_encoder_weights_file, map_location=device))\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ed1e1-cabd-4e03-8689-aafe3ed1350a",
   "metadata": {},
   "source": [
    "## Create VAE Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2197fd04-a5ce-4d99-9c15-d4ee2cfe25be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, mel_count, mel_filters, conv_channel_counts, conv_kernel_size, dense_layer_sizes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.mel_count = mel_count\n",
    "        self.mel_filters = mel_filters\n",
    "        self.conv_channel_counts = conv_channel_counts\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.dense_layer_sizes = dense_layer_sizes\n",
    "        \n",
    "        # create dense layers\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        \n",
    "        stride = ((self.conv_kernel_size[0] - 1) // 2, (self.conv_kernel_size[1] - 1) // 2)\n",
    "\n",
    "        self.dense_layers.append(nn.Linear(latent_dim, self.dense_layer_sizes[0]))\n",
    "        self.dense_layers.append(nn.ReLU())\n",
    "        \n",
    "        dense_layer_count = len(dense_layer_sizes)\n",
    "        for layer_index in range(1, dense_layer_count):\n",
    "            self.dense_layers.append(nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index]))\n",
    "            self.dense_layers.append(nn.ReLU())\n",
    "            \n",
    "        last_conv_layer_size_x = int(mel_filters // np.power(stride[0], len(conv_channel_counts)))\n",
    "        last_conv_layer_size_y = int(mel_count // np.power(stride[1], len(conv_channel_counts)))\n",
    "\n",
    "        preflattened_size = [conv_channel_counts[0], last_conv_layer_size_x, last_conv_layer_size_y]\n",
    "\n",
    "        dense_layer_output_size = conv_channel_counts[0] * last_conv_layer_size_x * last_conv_layer_size_y\n",
    " \n",
    "        self.dense_layers.append(nn.Linear(self.dense_layer_sizes[-1], dense_layer_output_size))\n",
    "        self.dense_layers.append(nn.ReLU())\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=preflattened_size)\n",
    "        \n",
    "        # create convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        padding = stride\n",
    "        output_padding = (padding[0] - 1, padding[1] - 1) # does this universally work?\n",
    "        \n",
    "        conv_layer_count = len(conv_channel_counts)\n",
    "        for layer_index in range(1, conv_layer_count):\n",
    "            self.conv_layers.append(nn.BatchNorm2d(conv_channel_counts[layer_index-1]))\n",
    "            self.conv_layers.append(nn.ConvTranspose2d(conv_channel_counts[layer_index-1], conv_channel_counts[layer_index], self.conv_kernel_size, stride=stride, padding=padding, output_padding=output_padding))\n",
    "            self.conv_layers.append(nn.LeakyReLU(0.2))\n",
    "            \n",
    "        self.conv_layers.append(nn.BatchNorm2d(conv_channel_counts[-1]))\n",
    "        self.conv_layers.append(nn.ConvTranspose2d(conv_channel_counts[-1], 1, self.conv_kernel_size, stride=stride, padding=padding, output_padding=output_padding))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for lI, layer in enumerate(self.dense_layers):\n",
    "            x = layer(x)\n",
    "        x = self.unflatten(x)\n",
    "        for lI, layer in enumerate(self.conv_layers):\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "decoder = Decoder(ae_latent_dim, mel_count, mel_filters, list(reversed(ae_conv_channel_counts)), ae_conv_kernel_size, list(reversed(ae_dense_layer_sizes))).to(device)\n",
    "decoder.load_state_dict(torch.load(ae_decoder_weights_file, map_location=device))\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee12b4ea-9da8-46d7-b496-4b81ef62347e",
   "metadata": {},
   "source": [
    "## Audio Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c5facb-edf6-41f7-a47c-e1004315688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_source_start_frame_index_1 = 0\n",
    "audio_source_start_frame_index_2 = 0\n",
    "audio_source_end_frame_index_1 = audio_source_samples_1.shape[0]\n",
    "audio_source_end_frame_index_2 = audio_source_samples_2.shape[0]\n",
    "\n",
    "audio_encoding_mix_factors = torch.zeros((ae_latent_dim), dtype=torch.float32).to(device)\n",
    "audio_encoding_offset_factors = torch.zeros((ae_latent_dim), dtype=torch.float32).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_audio(waveform):\n",
    "    waveform = waveform.unsqueeze(0).to(device)\n",
    "    mels = vocos.feature_extractor(waveform)\n",
    "    audio_encoder_in = mels.unsqueeze(1)\n",
    "    mu, std = encoder(audio_encoder_in)\n",
    "    std = torch.nn.functional.softplus(std) + 1e-6\n",
    "    return Encoder.reparameterize(mu, std)\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_audio(latent):\n",
    "    mel_pred = decoder(latent).squeeze(1)\n",
    "    mel_pred = mel_pred.squeeze(1)\n",
    "    waveform = vocos.decode(mel_pred).reshape(1, -1)\n",
    "    return waveform\n",
    "\n",
    "@torch.no_grad()\n",
    "def synthesize_audio():\n",
    "    \n",
    "    global audio_source_frame_index_1, audio_source_frame_index_2\n",
    "\n",
    "    waveform_excerpt_1 = audio_source_samples_1[audio_source_frame_index_1:audio_source_frame_index_1 + window_size]\n",
    "    waveform_excerpt_2 = audio_source_samples_2[audio_source_frame_index_2:audio_source_frame_index_2 + window_size]\n",
    "    \n",
    "    waveform_excerpt_1 = waveform_excerpt_1.unsqueeze(0).to(device)\n",
    "    waveform_excerpt_2 = waveform_excerpt_2.unsqueeze(0).to(device)\n",
    "    \n",
    "    mels_excerpt_1 = vocos.feature_extractor(waveform_excerpt_1)\n",
    "    mels_excerpt_2 = vocos.feature_extractor(waveform_excerpt_2)\n",
    "\n",
    "    encoder_in_1 = mels_excerpt_1.unsqueeze(1)\n",
    "    encoder_in_2 = mels_excerpt_2.unsqueeze(1)\n",
    "\n",
    "    mu_1, std_1 = encoder(encoder_in_1)\n",
    "    mu_2, std_2 = encoder(encoder_in_2)\n",
    "\n",
    "    std_1 = torch.nn.functional.softplus(std_1) + 1e-6\n",
    "    std_2 = torch.nn.functional.softplus(std_2) + 1e-6\n",
    "    \n",
    "    encoding_1 = encoder.reparameterize(mu_1, std_1)\n",
    "    encoding_2 = encoder.reparameterize(mu_2, std_2)\n",
    "\n",
    "    decoder_in = encoding_1 * (1.0 - audio_encoding_mix_factors) + encoding_2 * audio_encoding_mix_factors\n",
    "    decoder_in = decoder_in + audio_encoding_offset_factors\n",
    "\n",
    "    gen_mels_excerpt = decoder(decoder_in)\n",
    " \n",
    "    gen_mels_excerpt = gen_mels_excerpt.squeeze(1)\n",
    "    gen_waveform_excerpt = vocos.decode(gen_mels_excerpt.detach()).reshape(-1)\n",
    "    \n",
    "    audio_source_frame_index_1 = audio_source_frame_index_1 + window_offset\n",
    "    if audio_source_frame_index_1 >= audio_source_end_frame_index_1 - window_size:\n",
    "        audio_source_frame_index_1 = audio_source_start_frame_index_1\n",
    "    \n",
    "    audio_source_frame_index_2 = audio_source_frame_index_2 + window_offset\n",
    "    if audio_source_frame_index_2 >= audio_source_end_frame_index_2 - window_size:\n",
    "        audio_source_frame_index_2 = audio_source_start_frame_index_2\n",
    "\n",
    "    return gen_waveform_excerpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf8ac54-3324-4dbf-9833-654d5a4ac338",
   "metadata": {},
   "source": [
    "## Audio Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb45875-57e3-4e6a-b630-0a28e2aa9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_queue = queue.Queue(maxsize=max_audio_queue_length)\n",
    "export_audio_buffer = []\n",
    "last_chunk = np.zeros(window_size, dtype=np.float32)\n",
    "\n",
    "def producer_thread():\n",
    "    \"\"\"Continuously generates audio and fills the output buffer queue.\"\"\"\n",
    "    while True:\n",
    "        if not audio_queue.full():\n",
    "            gen_waveform = synthesize_audio()\n",
    "            audio_queue.put(gen_waveform.cpu().numpy())\n",
    "        else:\n",
    "            sd.sleep(10)\n",
    "\n",
    "def audio_callback(out_data, frames, time_info, status):\n",
    "    \"\"\"sounddevice stream callback function.\"\"\"\n",
    "    global last_chunk\n",
    "    output = np.zeros((frames, audio_channel_count), dtype=np.float32)\n",
    "    cursor = 0\n",
    "    overlap_len = window_size // 2\n",
    "    output[cursor:cursor+overlap_len, 0] += last_chunk[overlap_len:]\n",
    "    samples_needed = frames\n",
    "    while samples_needed > 0:\n",
    "        try:\n",
    "            chunk = audio_queue.get_nowait()\n",
    "            chunk = (chunk * hann_window.cpu().numpy())\n",
    "        except queue.Empty:\n",
    "            chunk = np.zeros(window_size, dtype=np.float32)\n",
    "        chunk_copy_size = output[cursor:cursor+window_size, 0].shape[0]\n",
    "        output[cursor:cursor+chunk_copy_size, 0] += chunk[:chunk_copy_size]\n",
    "        cursor += window_size // 2\n",
    "        samples_needed = frames - cursor\n",
    "        last_chunk[:] = chunk[:]\n",
    "    out_data[:] = output\n",
    "\n",
    "def run_audio_stream():\n",
    "    \"\"\"Main thread for launching audio producer and output stream.\"\"\"\n",
    "    threading.Thread(target=producer_thread, daemon=True).start()\n",
    "    sd.sleep(2000)\n",
    "    with sd.OutputStream(\n",
    "        samplerate=audio_sample_rate,\n",
    "        device=audio_output_device,\n",
    "        channels=audio_channel_count,\n",
    "        callback=audio_callback,\n",
    "        blocksize=play_buffer_size,\n",
    "        latency=playback_latency\n",
    "    ):\n",
    "        print(\"Streaming audio with FIFO queue... press Ctrl+C to stop.\")\n",
    "        try:\n",
    "            while True:\n",
    "                sd.sleep(1000)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nStopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379bbfb-0850-439d-8057-bf55f9c5e004",
   "metadata": {},
   "source": [
    "## OSC Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340dd39-5057-4a74-ac9a-98c55d0b09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def osc_setIndex1(address, *args):\n",
    "    global audio_source_start_frame_index_1\n",
    "    \n",
    "    audio_source_start_frame_index_1 = args[0]\n",
    "    \n",
    "def osc_setIndex2(address, *args):\n",
    "    global audio_source_start_frame_index_2\n",
    "    \n",
    "    audio_source_start_frame_index_2 = args[0]\n",
    "     \n",
    "def osc_setRange1(address, *args):\n",
    "    global audio_source_start_frame_index_1, audio_source_end_frame_index_1\n",
    "    \n",
    "    audio_source_start_frame_index_1 = args[0]\n",
    "    audio_source_end_frame_index_1 = args[1]\n",
    "\n",
    "def osc_setRange2(address, *args):\n",
    "    global audio_source_start_frame_index_2, audio_source_end_frame_index_2\n",
    "    \n",
    "    audio_source_start_frame_index_2 = args[0]\n",
    "    audio_source_end_frame_index_2 = args[1]\n",
    "\n",
    "def osc_setEncodingMix(address, *args):\n",
    "    \n",
    "    global audio_encoding_mix_factors\n",
    "    \n",
    "    arg_count = len(args)\n",
    "    \n",
    "    for i in range(min(arg_count, ae_latent_dim)):\n",
    "        audio_encoding_mix_factors[i] = args[i]\n",
    "\n",
    "def osc_setEncodingOffset(address, *args):\n",
    "    \n",
    "    global audio_encoding_offset_factors\n",
    "\n",
    "    arg_count = len(args)\n",
    "        \n",
    "    for i in range(min(arg_count, ae_latent_dim)):\n",
    "        audio_encoding_offset_factors[i] = args[i]\n",
    "\n",
    "osc_handler = dispatcher.Dispatcher()\n",
    "osc_handler.map(\"/audio/sampleindex1\", osc_setIndex1)\n",
    "osc_handler.map(\"/audio/sampleindex2\", osc_setIndex2)\n",
    "osc_handler.map(\"/audio/samplerange1\", osc_setRange1)\n",
    "osc_handler.map(\"/audio/samplerange2\", osc_setRange2)\n",
    "osc_handler.map(\"/synth/encodingmix\", osc_setEncodingMix)\n",
    "osc_handler.map(\"/synth/encodingoffset\", osc_setEncodingOffset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4208bae-930a-4c34-a5ca-07f5733348d7",
   "metadata": {},
   "source": [
    "## Start OSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b1c37d-850b-4849-a271-c44f932b67a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "osc_server = osc_server.ThreadingOSCUDPServer((osc_receive_ip, osc_receive_port), osc_handler)\n",
    "\n",
    "def osc_start_receive():\n",
    "    osc_server.serve_forever()\n",
    "\n",
    "osc_thread = threading.Thread(target=osc_start_receive)\n",
    "osc_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12091d1e-56f0-44c2-9ec6-39324a851768",
   "metadata": {},
   "source": [
    "## Start Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0369a2-3557-442d-a67a-1fa0c88e77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_thread = threading.Thread(target=run_audio_stream, daemon=True)\n",
    "audio_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8995029-ff13-4fc7-a6fd-a5ecb4bbc1eb",
   "metadata": {},
   "source": [
    "## Interactive Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30228bf7-5a4e-49ce-9a97-ba5a645b122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_source_frame_index_1_gui = widgets.FloatSlider(\n",
    "    value=audio_source_frame_index_1 / audio_sample_rate,\n",
    "    min=0.0,\n",
    "    max=audio_source_samples_1.shape[0] / audio_sample_rate,\n",
    "    step=0.1,\n",
    "    description='Audio Source 1 Current Play Position (Secs):',\n",
    "    readout=True,  # displays current value\n",
    "    orientation='horizontal',  # can also be 'vertical'\n",
    "    readout_format='.1f',\n",
    "    layout=widgets.Layout(width='90%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "audio_source_frame_index_2_gui = widgets.FloatSlider(\n",
    "    value=audio_source_frame_index_2 / audio_sample_rate,\n",
    "    min=0.0,\n",
    "    max=audio_source_samples_2.shape[0] / audio_sample_rate,\n",
    "    step=0.1,\n",
    "    description='Audio Source 2 Current Play Position (Secs):',\n",
    "    readout=True,  # displays current value\n",
    "    orientation='horizontal',  # can also be 'vertical'\n",
    "    readout_format='.1f',\n",
    "    layout=widgets.Layout(width='90%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "audio_source_frame_range_1_gui = widgets.FloatRangeSlider(\n",
    "    value=[0.0, audio_source_samples_1.shape[0]  / audio_sample_rate ],\n",
    "    min=0.0,\n",
    "    max=audio_source_samples_1.shape[0]  / audio_sample_rate ,\n",
    "    step=1,\n",
    "    description='Audio Source 1 Play Range (Secs):',\n",
    "    readout=True,\n",
    "    orientation='horizontal',  # can also be 'vertical'\n",
    "    readout_format='.1f',\n",
    "    layout=widgets.Layout(width='90%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "audio_source_frame_range_2_gui = widgets.FloatRangeSlider(\n",
    "    value=[0.0, audio_source_samples_2.shape[0]  / audio_sample_rate ],\n",
    "    min=0.0,\n",
    "    max=audio_source_samples_2.shape[0]  / audio_sample_rate ,\n",
    "    step=1,\n",
    "    description='Audio Source 2 Play Range (Secs):',\n",
    "    readout=True,\n",
    "    orientation='horizontal',  # can also be 'vertical'\n",
    "    readout_format='.1f',\n",
    "    layout=widgets.Layout(width='90%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "audio_encoding_mix_factors_gui = widgets.FloatSlider(\n",
    "    value=0.0,\n",
    "    min=-4.0,\n",
    "    max=4.0,\n",
    "    step=0.01,\n",
    "    description='Mix Factors All:',\n",
    "    readout_format='.1f',\n",
    "    layout=widgets.Layout(width='90%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "audio_encoding_offset_factors_gui = widgets.FloatSlider(\n",
    "    value=0.0,\n",
    "    min=-4.0,\n",
    "    max=4.0,\n",
    "    step=0.01,\n",
    "    description='Offset Factors All:',\n",
    "    readout_format='.1f',\n",
    "    layout=widgets.Layout(width='90%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "audio_encoding_mix_factor_gui = []\n",
    "audio_encoding_offset_factor_gui = []\n",
    "\n",
    "for ld in range(ae_latent_dim):\n",
    "\n",
    "    mix_factor_gui = widgets.FloatSlider(\n",
    "        value=0.0,\n",
    "        min=-4.0,\n",
    "        max=4.0,\n",
    "        step=0.01,\n",
    "        description='Mix Factor Dim {}:'.format(ld),\n",
    "        readout_format='.1f',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    offset_factor_gui = widgets.FloatSlider(\n",
    "        value=0.0,\n",
    "        min=-4.0,\n",
    "        max=4.0,\n",
    "        step=0.01,\n",
    "        description='Offset Factor Dim {}:'.format(ld),\n",
    "        readout_format='.1f',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    audio_encoding_mix_factor_gui.append(mix_factor_gui)\n",
    "    audio_encoding_offset_factor_gui.append(offset_factor_gui)\n",
    "\n",
    "display(audio_source_frame_index_1_gui)\n",
    "display(audio_source_frame_index_2_gui)\n",
    "display(audio_source_frame_range_1_gui)\n",
    "display(audio_source_frame_range_2_gui)\n",
    "display(audio_encoding_mix_factors_gui)\n",
    "\n",
    "for ld in range(ae_latent_dim):\n",
    "    display(audio_encoding_mix_factor_gui[ld])\n",
    "\n",
    "display(audio_encoding_offset_factors_gui)\n",
    "\n",
    "for ld in range(ae_latent_dim):\n",
    "    display(audio_encoding_offset_factor_gui[ld])\n",
    "\n",
    "def on_audio_source_frame_index_1_change(value):\n",
    "    global audio_source_frame_index_1\n",
    "    \n",
    "    audio_source_frame_index_1 = int(value['new'] * audio_sample_rate)\n",
    "\n",
    "def on_audio_source_frame_index_2_change(value):\n",
    "    global audio_source_frame_index_2\n",
    "    \n",
    "    audio_source_frame_index_2 = int(value['new'] * audio_sample_rate)\n",
    "\n",
    "def on_audio_source_frame_range_1_change(value):\n",
    "    global audio_source_start_frame_index_1, audio_source_end_frame_index_1\n",
    "    \n",
    "    audio_source_start_frame_index_1 = int(value['new'][0] * audio_sample_rate)\n",
    "    audio_source_end_frame_index_1 = int(value['new'][1] * audio_sample_rate)\n",
    "\n",
    "def on_audio_source_frame_range_2_change(value):\n",
    "    global audio_source_start_frame_index_2, audio_source_end_frame_index_2\n",
    "    \n",
    "    audio_source_start_frame_index_2 = int(value['new'][0] * audio_sample_rate)\n",
    "    audio_source_end_frame_index_2 = int(value['new'][1] * audio_sample_rate)\n",
    "\n",
    "def on_audio_encoding_mix_factors_change(value):\n",
    "    global audio_encoding_mix_factors\n",
    "\n",
    "    mix_factor = value['new']\n",
    "\n",
    "    for ld in range(ae_latent_dim):\n",
    "        audio_encoding_mix_factors[ld] = mix_factor\n",
    "        audio_encoding_mix_factor_gui[ld].value = mix_factor\n",
    "\n",
    "def on_audio_encoding_offset_factors_change(value):\n",
    "    global audio_encoding_offset_factors\n",
    "\n",
    "    offset_factor = value['new']\n",
    "\n",
    "    for ld in range(ae_latent_dim):\n",
    "        audio_encoding_offset_factors[ld] = offset_factor\n",
    "        audio_encoding_offset_factor_gui[ld].value = offset_factor\n",
    "\n",
    "def on_audio_encoding_mix_factor_change(value):\n",
    "    global audio_encoding_mix_factors\n",
    "    \n",
    "    encoding_mix_gui = value['owner']\n",
    "    ld = audio_encoding_mix_factor_gui.index(encoding_mix_gui)\n",
    "    audio_encoding_mix_factors[ld] = value['new']\n",
    "\n",
    "def on_audio_encoding_offset_factor_change(value):\n",
    "    global audio_encoding_offset_factors\n",
    "    \n",
    "    encoding_offset_gui = value['owner']\n",
    "    ld = audio_encoding_offset_factor_gui.index(encoding_offset_gui)\n",
    "    audio_encoding_offset_factors[ld] = value['new']\n",
    "\n",
    "audio_source_frame_index_1_gui.observe(on_audio_source_frame_index_1_change, names='value')\n",
    "audio_source_frame_index_2_gui.observe(on_audio_source_frame_index_2_change, names='value')\n",
    "audio_source_frame_range_1_gui.observe(on_audio_source_frame_range_1_change, names='value')\n",
    "audio_source_frame_range_2_gui.observe(on_audio_source_frame_range_2_change, names='value')\n",
    "\n",
    "audio_encoding_mix_factors_gui.observe(on_audio_encoding_mix_factors_change, names='value')\n",
    "audio_encoding_offset_factors_gui.observe(on_audio_encoding_offset_factors_change, names='value')\n",
    "\n",
    "for ld in range(ae_latent_dim):\n",
    "    audio_encoding_mix_factor_gui[ld].observe(on_audio_encoding_mix_factor_change, names='value')\n",
    "    audio_encoding_offset_factor_gui[ld].observe(on_audio_encoding_offset_factor_change, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d53ec-3368-4f26-a900-98b73b5ce89e",
   "metadata": {},
   "source": [
    "## Stop Real-Time Audio and OSC Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc11de1-96ae-414e-9b12-bfc3c604e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio_thread.stop()\n",
    "#osc_control.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
