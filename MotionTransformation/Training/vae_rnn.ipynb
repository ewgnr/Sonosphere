{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518e6ec5-dbbf-4165-9af6-c35d7fb3d5a2",
   "metadata": {},
   "source": [
    "# Motion Autoencoder (RNN Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e7cc2-916f-478f-b448-caa24ef1934b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213da82d-98aa-4da0-88b4-b3961b187ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "import os, sys, time, subprocess\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "from common import utils\n",
    "from common import bvh_tools as bvh\n",
    "from common import fbx_tools as fbx\n",
    "from common import mocap_tools as mocap\n",
    "from common.quaternion import qmul, qrot, qnormalize_np, slerp, qfix\n",
    "from common.pose_renderer import PoseRenderer\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61e671-cfad-4dbc-967c-444a2fdf83ab",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e23de3-8cfc-463f-b000-671339cac508",
   "metadata": {},
   "source": [
    "## Compute Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026792f3-5c46-4c53-b45c-509f64b96601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f79ce-26c1-4206-87f8-fa178e66585e",
   "metadata": {},
   "source": [
    "## Mocap Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c22673a-4c06-48a4-9a16-155b7c1d8db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cef4fe2ef24ad780db17434ea305c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=1.0, description='Mocap Position Scale:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d38b71e45654c27a69fc8279ba43d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=50, description='Mocap FPS:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56a67daa2e0477b8658fc65b63c8ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Mocap Files:', index=(0,), layout=Layout(width='400px'), options=('Daniel_ChineseR…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdedadcd2f3541a2b929b3b680c9c6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Mocap Loss Weight File:', layout=Layout(width='400px'), options=('AIC_config.json', 'Cam…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mocap_file_path = \"../../../Data/Mocap/\"\n",
    "mocap_files = [\"Daniel_ChineseRoom_Take1_50fps.fbx\"]\n",
    "mocap_pos_scale = 1.0\n",
    "mocap_fps = 50\n",
    "mocap_loss_file_path = \"data/configs/\"\n",
    "mocap_loss_weights_file = None\n",
    "\n",
    "mocap_pos_scale_gui = widgets.FloatText(mocap_pos_scale, description=\"Mocap Position Scale:\", style={'description_width': 'initial'})\n",
    "mocap_fps_gui = widgets.IntText(mocap_fps, description=\"Mocap FPS:\", style={'description_width': 'initial'})\n",
    "\n",
    "mocap_files_all = [f for f in os.listdir(mocap_file_path) if os.path.isfile(os.path.join(mocap_file_path, f))]\n",
    "#print(mocap_files_all)\n",
    "\n",
    "mocap_loss_file_all = [f for f in os.listdir(mocap_loss_file_path) if os.path.isfile(os.path.join(mocap_loss_file_path, f))]\n",
    "#print(mocap_loss_file_all)\n",
    "\n",
    "mocap_files_gui = widgets.SelectMultiple(\n",
    "    options=mocap_files_all,\n",
    "    value=mocap_files,  # default: first option selected; can be empty\n",
    "    description='Mocap Files:',\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "mocap_loss_weights_gui = widgets.Dropdown(\n",
    "    options=mocap_loss_file_all,\n",
    "    value=mocap_loss_weights_file,  # default: first option selected; can be empty\n",
    "    description='Mocap Loss Weight File:',\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(mocap_pos_scale_gui)\n",
    "display(mocap_fps_gui)\n",
    "display(mocap_files_gui)\n",
    "display(mocap_loss_weights_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2526c5-7a0b-4834-afd1-655cc84a4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "mocap_pos_scale = mocap_pos_scale_gui.value\n",
    "mocap_fps = mocap_fps_gui.value\n",
    "mocap_files = list(mocap_files_gui.value)\n",
    "mocap_loss_weights_file = mocap_loss_weights_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bda5c-3f52-4316-8abc-ac484de62175",
   "metadata": {},
   "source": [
    "## Autoencoder Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "932f4197-faad-4baa-8011-23a0b9181855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a724daa192fe436dbce891e159925154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=32, description='Latent Dimension:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c05fcb024c45d085224bfdceab8268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='LSTM Layer Count:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41aa1063ef9a465b9fc457b444da7c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=512, description='LSTM Layer Size:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c287bdd83abe4f97b55f899344ad3560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='512', description='Dense Layer Sizes:', layout=Layout(width='50%'), placeholder='Enter dense l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d94cd50e5d445d9a5c4914305235a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Save Weights', style=CheckboxStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfc4bf142c3466e9d564e8f5212d030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Load Weights', style=CheckboxStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffcbc84e64b44f1b1185c72d302c0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='results/weights/encoder_weights_epoch_600', description='Encoder Weights File:', style=TextStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ad6ead166743d385b74587c9921c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='results/weights/decoder_weights_epoch_600', description='Decoder Weights File:', style=TextStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latent_dim = 32\n",
    "sequence_length = 64\n",
    "ae_rnn_layer_count = 2\n",
    "ae_rnn_layer_size = 512\n",
    "ae_dense_layer_sizes = [ 512 ]\n",
    "\n",
    "save_models = False\n",
    "save_tscript = False\n",
    "save_weights = True\n",
    "\n",
    "# load model weights\n",
    "load_weights = False\n",
    "encoder_weights_file = \"results/weights/encoder_weights_epoch_600\"\n",
    "decoder_weights_file = \"results/weights/decoder_weights_epoch_600\"\n",
    "\n",
    "latent_dim_gui = widgets.IntText(value=latent_dim, description=\"Latent Dimension:\", style={'description_width': 'initial'})\n",
    "sequence_length_gui = widgets.IntText(value=sequence_length, description=\"Sequence Length (Frames):\", style={'description_width': 'initial'})\n",
    "ae_rnn_layer_count_gui = widgets.IntText(value=ae_rnn_layer_count, description=\"LSTM Layer Count:\", style={'description_width': 'initial'})\n",
    "ae_rnn_layer_size_gui = widgets.IntText(value=ae_rnn_layer_size, description=\"LSTM Layer Size:\", style={'description_width': 'initial'})\n",
    "\n",
    "ae_dense_layer_sizes_gui = widgets.Textarea(\n",
    "    value=','.join(list(map(str, ae_dense_layer_sizes))),\n",
    "    placeholder='Enter dense layer sizes separated by commas',\n",
    "    description='Dense Layer Sizes:',\n",
    "    layout=widgets.Layout(width='50%'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "save_weights_gui = widgets.Checkbox(\n",
    "    value=save_weights,\n",
    "    description='Save Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "load_weights_gui = widgets.Checkbox(\n",
    "    value=load_weights,\n",
    "    description='Load Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "encoder_weights_file_gui = widgets.Text(value=encoder_weights_file, description=\"Encoder Weights File:\", style={'description_width': 'initial'}) \n",
    "decoder_weights_file_gui = widgets.Text(value=decoder_weights_file, description=\"Decoder Weights File:\", style={'description_width': 'initial'}) \n",
    "\n",
    "display(latent_dim_gui)\n",
    "display(ae_rnn_layer_count_gui)\n",
    "display(ae_rnn_layer_size_gui)\n",
    "display(ae_dense_layer_sizes_gui)\n",
    "display(save_weights_gui)\n",
    "display(load_weights_gui)\n",
    "display(encoder_weights_file_gui)\n",
    "display(decoder_weights_file_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81604d65-ce59-4905-a52f-ea0f541d7e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = latent_dim_gui.value\n",
    "ae_rnn_layer_count = ae_rnn_layer_count_gui.value\n",
    "ae_rnn_layer_size = ae_rnn_layer_size_gui.value\n",
    "ae_dense_layer_sizes  = [int(s) for s in re.split(r\"\\s*,\\s*\", ae_dense_layer_sizes_gui.value) if s.strip()]\n",
    "save_weights = save_weights_gui.value\n",
    "load_weights = load_weights_gui.value\n",
    "encoder_weights_file = encoder_weights_file_gui.value\n",
    "decoder_weights_file = decoder_weights_file_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e837a4-2518-47e8-83a3-fd7e71859ef8",
   "metadata": {},
   "source": [
    "## Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c6ed054-c2bf-4e33-8fe8-60126f1f5512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51da77503eb4aba8f006ca8cfb9d1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Sequence Offset (Frames):', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8fd41d7258a468f8e4efc5bdb9375d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=16, description='Batch Size:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4496726a09e41459e6eb709b6789d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.2, description='Test Percentage:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98830c222fa540e58606510369da070e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0001, description='Autoencoder Learning Rate:', style=DescriptionStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da3b6bed1de4ad1b375945c2ee16d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.1, description='Normalization Loss Scale:', style=DescriptionStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5b13ceac3b417cbd7726000103a4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.1, description='Position Loss Scale:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fd6e6dec7b4bf7bc6af67a6fd09b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=1.0, description='Quaternion Loss Scale:', style=DescriptionStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0298d6cc2de7412ca1cbae2b86d28872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.0, description='Minimum KLD Scale:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311e58db6c724e00932f44ac363d9ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.1, description='Maximum KLD Scale:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e7729a7ee34508bf2767d2b01d9f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=100, description='Cycle Duration for KLD Scale:', style=DescriptionStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9781e0498384cf493398b6731fbf45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=20, description='Duration for Minimum KLD Scale:', style=DescriptionStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1595f1fa55c24be5953ef98f39058503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=20, description='Duration for Maximum KLD Scale:', style=DescriptionStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa161569edb248e584c666ae4c7f6e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=600, description='Number of Training Epochs:', style=DescriptionStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d238a90177d48c0a361840d8ec3339e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=50, description='Model Save Interval:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequence_offset = 2 # when creating sequence excerpts, each excerpt is offset from the previous one by this value\n",
    "batch_size = 16\n",
    "test_percentage  = 0.2\n",
    "ae_learning_rate = 1e-4\n",
    "ae_norm_loss_scale = 0.1\n",
    "ae_pos_loss_scale = 0.1\n",
    "ae_quat_loss_scale = 1.0\n",
    "ae_kld_loss_scale = 0.0 # will be calculated\n",
    "min_kld_scale = 0.0\n",
    "max_kld_scale = 0.1\n",
    "kld_scale_cycle_duration = 100\n",
    "kld_scale_min_const_duration = 20\n",
    "kld_scale_max_const_duration = 20\n",
    "epochs = 600\n",
    "model_save_interval = 50\n",
    "save_history = True\n",
    "\n",
    "sequence_offset_gui = widgets.IntText(value=sequence_offset, description=\"Sequence Offset (Frames):\", style={'description_width': 'initial'})\n",
    "batch_size_gui = widgets.IntText(value=batch_size, description=\"Batch Size:\", style={'description_width': 'initial'})\n",
    "test_percentage_gui = widgets.FloatText(value=test_percentage, description=\"Test Percentage:\", style={'description_width': 'initial'})\n",
    "ae_learning_rate_gui = widgets.FloatText(value=ae_learning_rate, description=\"Autoencoder Learning Rate:\", style={'description_width': 'initial'})\n",
    "ae_norm_loss_scale_gui = widgets.FloatText(value=ae_norm_loss_scale, description=\"Normalization Loss Scale:\", style={'description_width': 'initial'})\n",
    "ae_pos_loss_scale_gui = widgets.FloatText(value=ae_pos_loss_scale, description=\"Position Loss Scale:\", style={'description_width': 'initial'})\n",
    "ae_quat_loss_scale_gui = widgets.FloatText(value=ae_quat_loss_scale, description=\"Quaternion Loss Scale:\", style={'description_width': 'initial'})\n",
    "min_kld_scale_gui = widgets.FloatText(value=min_kld_scale, description=\"Minimum KLD Scale:\", style={'description_width': 'initial'})\n",
    "max_kld_scale_gui = widgets.FloatText(value=max_kld_scale, description=\"Maximum KLD Scale:\", style={'description_width': 'initial'})\n",
    "kld_scale_cycle_duration_gui = widgets.IntText(value=kld_scale_cycle_duration, description=\"Cycle Duration for KLD Scale:\", style={'description_width': 'initial'})\n",
    "kld_scale_min_const_duration_gui = widgets.IntText(value=kld_scale_min_const_duration, description=\"Duration for Minimum KLD Scale:\", style={'description_width': 'initial'})\n",
    "kld_scale_max_const_duration_gui = widgets.IntText(value=kld_scale_max_const_duration, description=\"Duration for Maximum KLD Scale:\", style={'description_width': 'initial'})\n",
    "epochs_gui = widgets.IntText(value=epochs, description=\"Number of Training Epochs:\", style={'description_width': 'initial'})\n",
    "model_save_interval_gui = widgets.IntText(value=model_save_interval, description=\"Model Save Interval:\", style={'description_width': 'initial'})\n",
    "\n",
    "save_history_gui = widgets.Checkbox(\n",
    "    value=save_history,\n",
    "    description='Save Training History',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(sequence_offset_gui)\n",
    "display(batch_size_gui)\n",
    "display(test_percentage_gui)\n",
    "display(ae_learning_rate_gui)\n",
    "display(ae_norm_loss_scale_gui)\n",
    "display(ae_pos_loss_scale_gui)\n",
    "display(ae_quat_loss_scale_gui)\n",
    "display(min_kld_scale_gui)\n",
    "display(max_kld_scale_gui)\n",
    "display(kld_scale_cycle_duration_gui)\n",
    "display(kld_scale_min_const_duration_gui)\n",
    "display(kld_scale_max_const_duration_gui)\n",
    "display(epochs_gui)\n",
    "display(model_save_interval_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6abe8b8c-6ad3-481f-a763-32b4f5df7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_offset = sequence_offset_gui.value\n",
    "batch_size = batch_size_gui.value\n",
    "test_percentage = test_percentage_gui.value\n",
    "ae_learning_rate = ae_learning_rate_gui.value\n",
    "ae_norm_loss_scale = ae_norm_loss_scale_gui.value\n",
    "ae_pos_loss_scale = ae_pos_loss_scale_gui.value\n",
    "ae_quat_loss_scale = ae_quat_loss_scale_gui.value\n",
    "min_kld_scale = min_kld_scale_gui.value\n",
    "max_kld_scale = max_kld_scale_gui.value\n",
    "kld_scale_cycle_duration = kld_scale_cycle_duration_gui.value\n",
    "kld_scale_min_const_duration = kld_scale_min_const_duration_gui.value\n",
    "kld_scale_max_const_duration = kld_scale_max_const_duration_gui.value\n",
    "epochs = epochs_gui.value\n",
    "model_save_interval = model_save_interval_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5db1b2-5f20-418c-aae3-eaa59fafcd10",
   "metadata": {},
   "source": [
    "## Visualization Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae9cdcf-fb1f-463a-a976-d8395ecd8aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ab558118c8456bae1496c6d9a6cbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=90.0, description='View Elevation:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbeae8ec61e741289a32c9595ab82134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=-90.0, description='View Azimuth:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79fa4fbfbca4c8195c70fb7703c86fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=1.0, description='View Line Width:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e110c80b884cb8bf0422c4ada8dc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=4.0, description='View Size:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_ele = 90.0\n",
    "view_azi = -90.0\n",
    "view_line_width = 1.0\n",
    "view_size = 4.0\n",
    "\n",
    "view_ele_gui = widgets.FloatText(value=view_ele, description=\"View Elevation:\", style={'description_width': 'initial'})\n",
    "view_azi_gui = widgets.FloatText(value=view_azi, description=\"View Azimuth:\", style={'description_width': 'initial'})\n",
    "view_line_width_gui = widgets.FloatText(value=view_line_width, description=\"View Line Width:\", style={'description_width': 'initial'})\n",
    "view_size_gui = widgets.FloatText(value=view_size, description=\"View Size:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(view_ele_gui)\n",
    "display(view_azi_gui)\n",
    "display(view_line_width_gui)\n",
    "display(view_size_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc10783-6cf5-4553-aff6-13d66352d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_ele = view_ele_gui.value\n",
    "view_azi = view_azi_gui.value\n",
    "view_line_width = view_line_width_gui.value\n",
    "view_size = view_size_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aec44c7-d461-46ab-ba9b-b5713d47053f",
   "metadata": {},
   "source": [
    "## Load Mocap Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8807b00e-c89f-48c6-b6b5-7d6a33543147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process file  Daniel_ChineseRoom_Take1_50fps.fbx\n"
     ]
    }
   ],
   "source": [
    "bvh_tools = bvh.BVH_Tools()\n",
    "fbx_tools = fbx.FBX_Tools()\n",
    "mocap_tools = mocap.Mocap_Tools()\n",
    "\n",
    "all_mocap_data = []\n",
    "\n",
    "for mocap_file in mocap_files:\n",
    "    \n",
    "    print(\"process file \", mocap_file)\n",
    "    \n",
    "    if mocap_file.endswith(\".bvh\") or mocap_file.endswith(\".BVH\"):\n",
    "        bvh_data = bvh_tools.load(mocap_file_path + \"/\" + mocap_file)\n",
    "        mocap_data = mocap_tools.bvh_to_mocap(bvh_data)\n",
    "    elif mocap_file.endswith(\".fbx\") or mocap_file.endswith(\".FBX\"):\n",
    "        fbx_data = fbx_tools.load(mocap_file_path + \"/\" + mocap_file)\n",
    "        mocap_data = mocap_tools.fbx_to_mocap(fbx_data)[0] # first skeleton only\n",
    "        \n",
    "    mocap_data[\"skeleton\"][\"offsets\"] *= mocap_pos_scale\n",
    "    mocap_data[\"motion\"][\"pos_local\"] *= mocap_pos_scale\n",
    "    \n",
    "    # set x and z offset of root joint to zero\n",
    "    mocap_data[\"skeleton\"][\"offsets\"][0, 0] = 0.0 \n",
    "    mocap_data[\"skeleton\"][\"offsets\"][0, 2] = 0.0 \n",
    "    \n",
    "    if mocap_file.endswith(\".bvh\") or mocap_file.endswith(\".BVH\"):\n",
    "        mocap_data[\"motion\"][\"rot_local\"] = mocap_tools.euler_to_quat_bvh(mocap_data[\"motion\"][\"rot_local_euler\"], mocap_data[\"rot_sequence\"])\n",
    "    elif mocap_file.endswith(\".fbx\") or mocap_file.endswith(\".FBX\"):\n",
    "        mocap_data[\"motion\"][\"rot_local\"] = mocap_tools.euler_to_quat(mocap_data[\"motion\"][\"rot_local_euler\"], mocap_data[\"rot_sequence\"])\n",
    "\n",
    "    all_mocap_data.append(mocap_data)\n",
    "\n",
    "# retrieve mocap properties\n",
    "\n",
    "mocap_data = all_mocap_data[0]\n",
    "joint_count = mocap_data[\"motion\"][\"rot_local\"].shape[1]\n",
    "joint_dim = mocap_data[\"motion\"][\"rot_local\"].shape[2]\n",
    "pose_dim = joint_count * joint_dim\n",
    "\n",
    "offsets = mocap_data[\"skeleton\"][\"offsets\"].astype(np.float32)\n",
    "parents = mocap_data[\"skeleton\"][\"parents\"]\n",
    "children = mocap_data[\"skeleton\"][\"children\"]\n",
    "\n",
    "# create edge list\n",
    "def get_edge_list(children):\n",
    "    edge_list = []\n",
    "\n",
    "    for parent_joint_index in range(len(children)):\n",
    "        for child_joint_index in children[parent_joint_index]:\n",
    "            edge_list.append([parent_joint_index, child_joint_index])\n",
    "    \n",
    "    return edge_list\n",
    "\n",
    "edge_list = get_edge_list(children)\n",
    "\n",
    "# set joint loss weigths \n",
    "\n",
    "if mocap_loss_weights_file is not None:\n",
    "    with open(mocap_loss_weights_file) as f:\n",
    "        joint_loss_weights = json.load(f)\n",
    "        joint_loss_weights = joint_loss_weights[\"joint_loss_weights\"]\n",
    "else:\n",
    "    joint_loss_weights = [1.0]\n",
    "    joint_loss_weights *= joint_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f032ace6-7447-4073-9e91-64cac4917a93",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efa5b921-4711-4382-ab1e-b2698831d435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_item s  (64, 96)\n",
      "X_batch s  torch.Size([16, 64, 96])\n"
     ]
    }
   ],
   "source": [
    "pose_sequence_excerpts = []\n",
    "\n",
    "for mocap_data in all_mocap_data:\n",
    "    pose_sequence = mocap_data[\"motion\"][\"rot_local\"]\n",
    "    pose_sequence = np.reshape(pose_sequence, (-1, pose_dim))\n",
    "    \n",
    "    frame_range_start = 0\n",
    "    frame_range_end = pose_sequence.shape[0]\n",
    "    \n",
    "    for seq_excerpt_start in np.arange(frame_range_start, frame_range_end - sequence_length, sequence_offset):\n",
    "        #print(\"valid: start \", frame_range_start, \" end \", frame_range_end, \" exc: start \", seq_excerpt_start, \" end \", (seq_excerpt_start + sequence_length) )\n",
    "        pose_sequence_excerpt =  pose_sequence[seq_excerpt_start:seq_excerpt_start + sequence_length]\n",
    "        pose_sequence_excerpts.append(pose_sequence_excerpt)\n",
    "    \n",
    "pose_sequence_excerpts = np.array(pose_sequence_excerpts, dtype=np.float32)\n",
    "\n",
    "# create dataset\n",
    "\n",
    "sequence_excerpts_count = pose_sequence_excerpts.shape[0]\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequence_excerpts):\n",
    "        self.sequence_excerpts = sequence_excerpts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.sequence_excerpts.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequence_excerpts[idx, ...]\n",
    "        \n",
    "\n",
    "full_dataset = SequenceDataset(pose_sequence_excerpts)\n",
    "\n",
    "X_item = full_dataset[0]\n",
    "print(\"X_item s \", X_item.shape)\n",
    "\n",
    "dataset_size = len(full_dataset)\n",
    "\n",
    "test_size = int(test_percentage * dataset_size)\n",
    "train_size = dataset_size - test_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X_batch = next(iter(train_dataloader))\n",
    "print(\"X_batch s \", X_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f58747-4023-44a5-8f47-6d517e418c51",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16fb1de-2469-4d11-ba55-1459da11118c",
   "metadata": {},
   "source": [
    "## Create Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a873297-9e80-435f-92f4-dcbe9d124d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (rnn_layers): Sequential(\n",
      "    (encoder_rnn_0): LSTM(96, 512, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (dense_layers): Sequential(\n",
      "    (encoder_dense_0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (encoder_dense_relu_0): ReLU()\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=512, out_features=32, bias=True)\n",
      "  (fc_std): Linear(in_features=512, out_features=32, bias=True)\n",
      ")\n",
      "motion_encoder_in s  torch.Size([16, 64, 96])\n",
      "motion_encoder_out_mu s  torch.Size([16, 32])\n",
      "motion_encoder_out_std s  torch.Size([16, 32])\n",
      "motion_encoder_out s  torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, sequence_length, pose_dim, latent_dim, rnn_layer_count, rnn_layer_size, dense_layer_sizes):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.pose_dim = pose_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.rnn_layer_count = rnn_layer_count\n",
    "        self.rnn_layer_size = rnn_layer_size \n",
    "        self.dense_layer_sizes = dense_layer_sizes\n",
    "    \n",
    "        # create recurrent layers\n",
    "        rnn_layers = []\n",
    "        rnn_layers.append((\"encoder_rnn_0\", nn.LSTM(self.pose_dim, self.rnn_layer_size, self.rnn_layer_count, batch_first=True)))\n",
    "        \n",
    "        self.rnn_layers = nn.Sequential(OrderedDict(rnn_layers))\n",
    "        \n",
    "        # create dense layers\n",
    "        \n",
    "        dense_layers = []\n",
    "        \n",
    "        dense_layers.append((\"encoder_dense_0\", nn.Linear(self.rnn_layer_size, self.dense_layer_sizes[0])))\n",
    "        dense_layers.append((\"encoder_dense_relu_0\", nn.ReLU()))\n",
    "        \n",
    "        dense_layer_count = len(self.dense_layer_sizes)\n",
    "        for layer_index in range(1, dense_layer_count):\n",
    "            dense_layers.append((\"encoder_dense_{}\".format(layer_index), nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index])))\n",
    "            dense_layers.append((\"encoder_dense_relu_{}\".format(layer_index), nn.ReLU()))\n",
    "            \n",
    "        self.dense_layers = nn.Sequential(OrderedDict(dense_layers))\n",
    "        \n",
    "        # create final dense layers\n",
    "            \n",
    "        self.fc_mu = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n",
    "        self.fc_std = nn.Linear(self.dense_layer_sizes[-1], self.latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"x 1 \", x.shape)\n",
    "        \n",
    "        x, (_, _) = self.rnn_layers(x)\n",
    "        \n",
    "        #print(\"x 2 \", x.shape)\n",
    "        \n",
    "        x = x[:, -1, :] # only last time step \n",
    "        \n",
    "        #print(\"x 3 \", x.shape)\n",
    "        \n",
    "        x = self.dense_layers(x)\n",
    "        \n",
    "        #print(\"x 3 \", x.shape)\n",
    "        \n",
    "        mu = self.fc_mu(x)\n",
    "        std = self.fc_std(x)\n",
    "        \n",
    "        #print(\"mu s \", mu.shape, \" lvar s \", log_var.shape)\n",
    "    \n",
    "        return mu, std\n",
    "\n",
    "    def reparameterize(self, mu, std):\n",
    "        z = mu + std*torch.randn_like(std)\n",
    "        return z\n",
    "    \n",
    "encoder = Encoder(sequence_length, pose_dim, latent_dim, ae_rnn_layer_count, ae_rnn_layer_size, ae_dense_layer_sizes).to(device)\n",
    "\n",
    "print(encoder)\n",
    "\n",
    "if load_weights and encoder_weights_file:\n",
    "    encoder.load_state_dict(torch.load(encoder_weights_file, map_location=device))\n",
    "\n",
    "# test encoder\n",
    "motion_encoder_in = next(iter(train_dataloader)).to(device)\n",
    "motion_encoder_out_mu, motion_encoder_out_std = encoder(motion_encoder_in)\n",
    "motion_encoder_out = encoder.reparameterize(motion_encoder_out_mu, motion_encoder_out_std)\n",
    "\n",
    "print(\"motion_encoder_in s \", motion_encoder_in.shape)\n",
    "print(\"motion_encoder_out_mu s \", motion_encoder_out_std.shape)\n",
    "print(\"motion_encoder_out_std s \", motion_encoder_out_std.shape)\n",
    "print(\"motion_encoder_out s \", motion_encoder_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af41559-6a5f-46cd-8904-187a74e7bf17",
   "metadata": {},
   "source": [
    "## Create Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d4525e2-093a-4bf4-8bbd-e46354805dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (dense_layers): Sequential(\n",
      "    (decoder_dense_0): Linear(in_features=32, out_features=512, bias=True)\n",
      "    (decoder_relu_0): ReLU()\n",
      "  )\n",
      "  (rnn_layers): Sequential(\n",
      "    (decoder_rnn_0): LSTM(512, 512, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (final_layers): Sequential(\n",
      "    (decoder_dense_1): Linear(in_features=512, out_features=96, bias=True)\n",
      "  )\n",
      ")\n",
      "motion_decoder_in s  torch.Size([16, 32])\n",
      "motion_decoder_out s  torch.Size([16, 64, 96])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, sequence_length, pose_dim, latent_dim, rnn_layer_count, rnn_layer_size, dense_layer_sizes):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.pose_dim = pose_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.rnn_layer_size = rnn_layer_size\n",
    "        self.rnn_layer_count = rnn_layer_count\n",
    "        self.dense_layer_sizes = dense_layer_sizes\n",
    "\n",
    "        # create dense layers\n",
    "        dense_layers = []\n",
    "        \n",
    "        dense_layers.append((\"decoder_dense_0\", nn.Linear(latent_dim, self.dense_layer_sizes[0])))\n",
    "        dense_layers.append((\"decoder_relu_0\", nn.ReLU()))\n",
    "\n",
    "        dense_layer_count = len(self.dense_layer_sizes)\n",
    "        for layer_index in range(1, dense_layer_count):\n",
    "            dense_layers.append((\"decoder_dense_{}\".format(layer_index), nn.Linear(self.dense_layer_sizes[layer_index-1], self.dense_layer_sizes[layer_index])))\n",
    "            dense_layers.append((\"decoder_dense_relu_{}\".format(layer_index), nn.ReLU()))\n",
    " \n",
    "        self.dense_layers = nn.Sequential(OrderedDict(dense_layers))\n",
    "        \n",
    "        # create rnn layers\n",
    "        rnn_layers = []\n",
    "\n",
    "        rnn_layers.append((\"decoder_rnn_0\", nn.LSTM(self.dense_layer_sizes[-1], self.rnn_layer_size, self.rnn_layer_count, batch_first=True)))\n",
    "        \n",
    "        self.rnn_layers = nn.Sequential(OrderedDict(rnn_layers))\n",
    "        \n",
    "        # final output dense layer\n",
    "        final_layers = []\n",
    "        \n",
    "        final_layers.append((\"decoder_dense_{}\".format(dense_layer_count), nn.Linear(self.rnn_layer_size, self.pose_dim)))\n",
    "        \n",
    "        self.final_layers = nn.Sequential(OrderedDict(final_layers))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"x 1 \", x.size())\n",
    "        \n",
    "        # dense layers\n",
    "        x = self.dense_layers(x)\n",
    "        #print(\"x 2 \", x.size())\n",
    "        \n",
    "        # repeat vector\n",
    "        x = torch.unsqueeze(x, dim=1)\n",
    "        x = x.repeat(1, sequence_length, 1)\n",
    "        #print(\"x 3 \", x.size())\n",
    "        \n",
    "        # rnn layers\n",
    "        x, (_, _) = self.rnn_layers(x)\n",
    "        #print(\"x 4 \", x.size())\n",
    "        \n",
    "        # final time distributed dense layer\n",
    "        x_reshaped = x.contiguous().view(-1, self.rnn_layer_size)  # (batch_size * sequence, input_size)\n",
    "        #print(\"x 5 \", x_reshaped.size())\n",
    "        \n",
    "        yhat = self.final_layers(x_reshaped)\n",
    "        #print(\"yhat 1 \", yhat.size())\n",
    "        \n",
    "        yhat = yhat.contiguous().view(-1, self.sequence_length, self.pose_dim)\n",
    "        #print(\"yhat 2 \", yhat.size())\n",
    "\n",
    "        return yhat\n",
    "\n",
    "ae_dense_layer_sizes_reversed = ae_dense_layer_sizes.copy()\n",
    "ae_dense_layer_sizes_reversed.reverse()\n",
    "\n",
    "decoder = Decoder(sequence_length, pose_dim, latent_dim, ae_rnn_layer_count, ae_rnn_layer_size, ae_dense_layer_sizes_reversed).to(device)\n",
    "\n",
    "print(decoder)\n",
    "\n",
    "if load_weights and decoder_weights_file:\n",
    "    decoder.load_state_dict(torch.load(decoder_weights_file, map_location=device))\n",
    "\n",
    "# test decoder\n",
    "motion_decoder_in = motion_encoder_out\n",
    "motion_decoder_out = decoder(motion_decoder_in)\n",
    "\n",
    "print(\"motion_decoder_in s \", motion_decoder_in.shape)\n",
    "print(\"motion_decoder_out s \", motion_decoder_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0929b-1c27-4608-a717-b0a038ed3e98",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fbae8a-5452-44d7-aa8d-3a26254cf577",
   "metadata": {},
   "source": [
    "## Create Beta Factor Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2c925-81ad-4127-b2d6-14adb5df9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_kld_scales():\n",
    "    \n",
    "    kld_scales = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        cycle_step = e % kld_scale_cycle_duration\n",
    "        \n",
    "        #print(\"cycle_step \", cycle_step)\n",
    "\n",
    "\n",
    "        if cycle_step < kld_scale_min_const_duration:\n",
    "            kld_scale = min_kld_scale\n",
    "            kld_scales.append(kld_scale)\n",
    "        elif cycle_step > kld_scale_cycle_duration - kld_scale_max_const_duration:\n",
    "            kld_scale = max_kld_scale\n",
    "            kld_scales.append(kld_scale)\n",
    "        else:\n",
    "            lin_step = cycle_step - kld_scale_min_const_duration\n",
    "            kld_scale = min_kld_scale + (max_kld_scale - min_kld_scale) * lin_step / (kld_scale_cycle_duration - kld_scale_min_const_duration - kld_scale_max_const_duration)\n",
    "            kld_scales.append(kld_scale)\n",
    "            \n",
    "    return kld_scales\n",
    "\n",
    "kld_scales = calc_kld_scales()\n",
    "\n",
    "plt.plot(kld_scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98e67e-229f-424a-8c80-55be36debc33",
   "metadata": {},
   "source": [
    "## Create Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9585112-d043-4955-99f7-21d36db03f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=ae_learning_rate)\n",
    "ae_scheduler = torch.optim.lr_scheduler.StepLR(ae_optimizer, step_size=100, gamma=0.316) # reduce the learning every 100 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33236f84-4299-4964-9b27-27c22962e61c",
   "metadata": {},
   "source": [
    "## Create Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1369850e-a2f7-4f21-9c3f-7b193a0c282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "cross_entropy = nn.BCELoss()\n",
    "\n",
    "# joint loss weights\n",
    "joint_loss_weights = torch.tensor(joint_loss_weights, dtype=torch.float32)\n",
    "joint_loss_weights = joint_loss_weights.reshape(1, 1, -1).to(device)\n",
    "\n",
    "# KL Divergence\n",
    "\n",
    "def variational_loss(mu, std):\n",
    "    #returns the variational loss from arguments mean and standard deviation std\n",
    "    #see also: see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    #https://arxiv.org/abs/1312.6114\n",
    "    vl=-0.5*torch.mean(1+ 2*torch.log(std)-mu.pow(2) -(std.pow(2)))\n",
    "    return vl\n",
    "   \n",
    "def variational_loss2(mu, std):\n",
    "    #returns the variational loss from arguments mean and standard deviation std\n",
    "    #alternative: mean squared distance from ideal mu=0 and std=1:\n",
    "    vl=torch.mean(mu.pow(2)+(1-std).pow(2))\n",
    "    return vl\n",
    "\n",
    "def ae_norm_loss(yhat):\n",
    "    \n",
    "    _yhat = yhat.view(-1, 4)\n",
    "    _norm = torch.norm(_yhat, dim=1)\n",
    "    _diff = (_norm - 1.0) ** 2\n",
    "    _loss = torch.mean(_diff)\n",
    "    return _loss\n",
    "\n",
    "def forward_kinematics(rotations, root_positions):\n",
    "    \"\"\"\n",
    "    Perform forward kinematics using the given trajectory and local rotations.\n",
    "    Arguments (where N = batch size, L = sequence length, J = number of joints):\n",
    "     -- rotations: (N, L, J, 4) tensor of unit quaternions describing the local rotations of each joint.\n",
    "     -- root_positions: (N, L, 3) tensor describing the root joint positions.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(rotations.shape) == 4\n",
    "    assert rotations.shape[-1] == 4\n",
    "    \n",
    "    toffsets = torch.tensor(offsets).to(device)\n",
    "    \n",
    "    positions_world = []\n",
    "    rotations_world = []\n",
    "\n",
    "    expanded_offsets = toffsets.expand(rotations.shape[0], rotations.shape[1], offsets.shape[0], offsets.shape[1])\n",
    "\n",
    "    # Parallelize along the batch and time dimensions\n",
    "    for jI in range(offsets.shape[0]):\n",
    "        if parents[jI] == -1:\n",
    "            positions_world.append(root_positions)\n",
    "            rotations_world.append(rotations[:, :, 0])\n",
    "        else:\n",
    "            positions_world.append(qrot(rotations_world[parents[jI]], expanded_offsets[:, :, jI]) \\\n",
    "                                   + positions_world[parents[jI]])\n",
    "            if len(children[jI]) > 0:\n",
    "                rotations_world.append(qmul(rotations_world[parents[jI]], rotations[:, :, jI]))\n",
    "            else:\n",
    "                # This joint is a terminal node -> it would be useless to compute the transformation\n",
    "                rotations_world.append(None)\n",
    "\n",
    "    return torch.stack(positions_world, dim=3).permute(0, 1, 3, 2)\n",
    "\n",
    "def ae_pos_loss(y, yhat):\n",
    "    # y and yhat shapes: batch_size, seq_length, pose_dim\n",
    "\n",
    "    # normalize tensors\n",
    "    _yhat = yhat.view(-1, 4)\n",
    "\n",
    "    _yhat_norm = nn.functional.normalize(_yhat, p=2, dim=1)\n",
    "    _y_rot = y.view((y.shape[0], y.shape[1], -1, 4))\n",
    "    _yhat_rot = _yhat.view((y.shape[0], y.shape[1], -1, 4))\n",
    "\n",
    "    zero_trajectory = torch.zeros((y.shape[0], y.shape[1], 3), dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "    _y_pos = forward_kinematics(_y_rot, zero_trajectory)\n",
    "    _yhat_pos = forward_kinematics(_yhat_rot, zero_trajectory)\n",
    "\n",
    "    _pos_diff = torch.norm((_y_pos - _yhat_pos), dim=3)\n",
    "    \n",
    "    #print(\"_pos_diff s \", _pos_diff.shape)\n",
    "    \n",
    "    _pos_diff_weighted = _pos_diff * joint_loss_weights\n",
    "    \n",
    "    _loss = torch.mean(_pos_diff_weighted)\n",
    "\n",
    "    return _loss\n",
    "\n",
    "def ae_quat_loss(y, yhat):\n",
    "    # y and yhat shapes: batch_size, seq_length, pose_dim\n",
    "    \n",
    "    # normalize quaternion\n",
    "    \n",
    "    _y = y.view((-1, 4))\n",
    "    _yhat = yhat.view((-1, 4))\n",
    "\n",
    "    _yhat_norm = nn.functional.normalize(_yhat, p=2, dim=1)\n",
    "    \n",
    "    # inverse of quaternion: https://www.mathworks.com/help/aeroblks/quaternioninverse.html\n",
    "    _yhat_inv = _yhat_norm * torch.tensor([[1.0, -1.0, -1.0, -1.0]], dtype=torch.float32).to(device)\n",
    "\n",
    "    # calculate difference quaternion\n",
    "    _diff = qmul(_yhat_inv, _y)\n",
    "    # length of complex part\n",
    "    _len = torch.norm(_diff[:, 1:], dim=1)\n",
    "    # atan2\n",
    "    _atan = torch.atan2(_len, _diff[:, 0])\n",
    "    # abs\n",
    "    _abs = torch.abs(_atan)\n",
    "    \n",
    "    _abs = _abs.reshape(-1, sequence_length, joint_count)\n",
    "    \n",
    "    _abs_weighted = _abs * joint_loss_weights\n",
    "    \n",
    "    #print(\"_abs s \", _abs.shape)\n",
    "    \n",
    "    _loss = torch.mean(_abs_weighted)   \n",
    "    return _loss\n",
    "\n",
    "# autoencoder loss function\n",
    "def ae_loss(y, yhat, mu, std):\n",
    "    # function parameters\n",
    "    # y: encoder input\n",
    "    # yhat: decoder output (i.e. reconstructed encoder input)\n",
    "    # disc_fake_output: discriminator output for encoder generated prior\n",
    "    \n",
    "    _norm_loss = ae_norm_loss(yhat)\n",
    "    _pos_loss = ae_pos_loss(y, yhat)\n",
    "    _quat_loss = ae_quat_loss(y, yhat)\n",
    "\n",
    "    # kld loss\n",
    "    _ae_kld_loss = variational_loss(mu, std)\n",
    "    \n",
    "    _total_loss = 0.0\n",
    "    _total_loss += _norm_loss * ae_norm_loss_scale\n",
    "    _total_loss += _pos_loss * ae_pos_loss_scale\n",
    "    _total_loss += _quat_loss * ae_quat_loss_scale\n",
    "    _total_loss += _ae_kld_loss * ae_kld_loss_scale\n",
    "    \n",
    "    return _total_loss, _norm_loss, _pos_loss, _quat_loss, _ae_kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17deeb3b-2f50-4d75-b1a7-86732de37cda",
   "metadata": {},
   "source": [
    "## Create Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00706e2-6462-47c1-bf8c-a0093e412d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_train_step(target_poses):\n",
    "    \n",
    "    #print(\"train step target_poses \", target_poses.shape)\n",
    " \n",
    "    # let autoencoder preproduce target_poses (decoder output) and also return encoder output\n",
    "    encoder_output = encoder(target_poses)\n",
    "\n",
    "    encoder_output_mu = encoder_output[0]\n",
    "    encoder_output_std = encoder_output[1]\n",
    "    mu = torch.tanh(encoder_output_mu)\n",
    "    std = torch.abs(torch.tanh(encoder_output_std)) + 0.00001\n",
    "    decoder_input = encoder.reparameterize(mu, std)\n",
    "    \n",
    "    pred_poses = decoder(decoder_input)\n",
    "    \n",
    "    _ae_loss, _ae_norm_loss, _ae_pos_loss, _ae_quat_loss, _ae_kld_loss = ae_loss(target_poses, pred_poses, mu, std) \n",
    "\n",
    "    #print(\"_ae_pos_loss \", _ae_pos_loss)\n",
    "    \n",
    "    # Backpropagation\n",
    "    ae_optimizer.zero_grad()\n",
    "    _ae_loss.backward()\n",
    "    \n",
    "    #torch.nn.utils.clip_grad_norm(encoder.parameters(), 0.01)\n",
    "    #torch.nn.utils.clip_grad_norm(decoder.parameters(), 0.01)\n",
    "\n",
    "    ae_optimizer.step()\n",
    "    \n",
    "    return _ae_loss, _ae_norm_loss, _ae_pos_loss, _ae_quat_loss, _ae_kld_loss\n",
    "\n",
    "def ae_test_step(target_poses):\n",
    "    with torch.no_grad():\n",
    "        # let autoencoder preproduce target_poses (decoder output) and also return encoder output\n",
    "        encoder_output = encoder(target_poses)\n",
    "        \n",
    "        encoder_output_mu = encoder_output[0]\n",
    "        encoder_output_std = encoder_output[1]\n",
    "        mu = torch.tanh(encoder_output_mu)\n",
    "        std = torch.abs(torch.tanh(encoder_output_std)) + 0.00001\n",
    "        decoder_input = encoder.reparameterize(mu, std)\n",
    "        \n",
    "        pred_poses = decoder(decoder_input)\n",
    "        \n",
    "        _ae_loss, _ae_norm_loss, _ae_pos_loss, _ae_quat_loss, _ae_kld_loss = ae_loss(target_poses, pred_poses, mu, std)  \n",
    "    \n",
    "    return _ae_loss, _ae_norm_loss, _ae_pos_loss, _ae_quat_loss, _ae_kld_loss\n",
    "\n",
    "def train(train_dataloader, test_dataloader, epochs):\n",
    "    \n",
    "    global ae_kld_loss_scale\n",
    "    \n",
    "    loss_history = {}\n",
    "    loss_history[\"ae train\"] = []\n",
    "    loss_history[\"ae test\"] = []\n",
    "    loss_history[\"ae norm\"] = []\n",
    "    loss_history[\"ae pos\"] = []\n",
    "    loss_history[\"ae quat\"] = []\n",
    "    loss_history[\"ae kld\"] = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        ae_kld_loss_scale = kld_scales[epoch]\n",
    "        \n",
    "        #print(\"ae_kld_loss_scale \", ae_kld_loss_scale)\n",
    "        \n",
    "        ae_train_loss_per_epoch = []\n",
    "        ae_norm_loss_per_epoch = []\n",
    "        ae_pos_loss_per_epoch = []\n",
    "        ae_quat_loss_per_epoch = []\n",
    "        ae_prior_loss_per_epoch = []\n",
    "        ae_kld_loss_per_epoch = []\n",
    "        \n",
    "        for train_batch in train_dataloader:\n",
    "            train_batch = train_batch.to(device)\n",
    "            \n",
    "            _ae_loss, _ae_norm_loss, _ae_pos_loss, _ae_quat_loss, _ae_kld_loss = ae_train_step(train_batch)\n",
    "            \n",
    "            _ae_loss = _ae_loss.detach().cpu().numpy()\n",
    "            _ae_norm_loss = _ae_norm_loss.detach().cpu().numpy()\n",
    "            _ae_pos_loss = _ae_pos_loss.detach().cpu().numpy()\n",
    "            _ae_quat_loss = _ae_quat_loss.detach().cpu().numpy()\n",
    "            _ae_kld_loss = _ae_kld_loss.detach().cpu().numpy()\n",
    "            \n",
    "            #print(\"_ae_prior_loss \", _ae_prior_loss)\n",
    "            \n",
    "            ae_train_loss_per_epoch.append(_ae_loss)\n",
    "            ae_norm_loss_per_epoch.append(_ae_norm_loss)\n",
    "            ae_pos_loss_per_epoch.append(_ae_pos_loss)\n",
    "            ae_quat_loss_per_epoch.append(_ae_quat_loss)\n",
    "            ae_kld_loss_per_epoch.append(_ae_kld_loss)\n",
    "\n",
    "        ae_train_loss_per_epoch = np.mean(np.array(ae_train_loss_per_epoch))\n",
    "        ae_norm_loss_per_epoch = np.mean(np.array(ae_norm_loss_per_epoch))\n",
    "        ae_pos_loss_per_epoch = np.mean(np.array(ae_pos_loss_per_epoch))\n",
    "        ae_quat_loss_per_epoch = np.mean(np.array(ae_quat_loss_per_epoch))\n",
    "        ae_kld_loss_per_epoch = np.mean(np.array(ae_kld_loss_per_epoch))\n",
    "\n",
    "        ae_test_loss_per_epoch = []\n",
    "        \n",
    "        for test_batch in test_dataloader:\n",
    "            test_batch = test_batch.to(device)\n",
    "            \n",
    "            _ae_loss, _, _, _, _ = ae_test_step(test_batch)\n",
    "            \n",
    "            _ae_loss = _ae_loss.detach().cpu().numpy()\n",
    "            ae_test_loss_per_epoch.append(_ae_loss)\n",
    "        \n",
    "        ae_test_loss_per_epoch = np.mean(np.array(ae_test_loss_per_epoch))\n",
    "        \n",
    "        if epoch % model_save_interval == 0 and save_weights == True:\n",
    "            torch.save(encoder.state_dict(), \"results/weights/encoder_weights_epoch_{}\".format(epoch))\n",
    "            torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epoch))\n",
    "        \n",
    "        loss_history[\"ae train\"].append(ae_train_loss_per_epoch)\n",
    "        loss_history[\"ae test\"].append(ae_test_loss_per_epoch)\n",
    "        loss_history[\"ae norm\"].append(ae_norm_loss_per_epoch)\n",
    "        loss_history[\"ae pos\"].append(ae_pos_loss_per_epoch)\n",
    "        loss_history[\"ae quat\"].append(ae_quat_loss_per_epoch)\n",
    "        loss_history[\"ae kld\"].append(ae_kld_loss_per_epoch)\n",
    "        \n",
    "        print ('epoch {} : ae train: {:01.4f} ae test: {:01.4f} norm {:01.4f} pos {:01.4f} quat {:01.4f} kld {:01.4f} time {:01.2f}'.format(epoch + 1, ae_train_loss_per_epoch, ae_test_loss_per_epoch, ae_norm_loss_per_epoch, ae_pos_loss_per_epoch, ae_quat_loss_per_epoch, ae_kld_loss_per_epoch, time.time()-start))\n",
    "    \n",
    "        ae_scheduler.step()\n",
    "        \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cdcb6-f2da-409f-86bf-077b47fba3e8",
   "metadata": {},
   "source": [
    "## Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69773f95-cc5e-4cb3-b90f-50161216234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = train(train_dataloader, test_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d008644-52cf-4921-991a-2e3739e98483",
   "metadata": {},
   "source": [
    "## Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7deba-cc35-4092-a3cc-1922da215875",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_loss_as_csv(loss_history, \"results/histories/history_{}.csv\".format(epochs))\n",
    "utils.save_loss_as_image(loss_history, \"results/histories/history_{}.png\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a2b983-c538-4077-8742-76f53027d104",
   "metadata": {},
   "source": [
    "## Save Final Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24cbfac-f6d9-4e4d-866b-889ebd7cce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"results/weights/encoder_weights_epoch_{}\".format(epochs))\n",
    "torch.save(decoder.state_dict(), \"results/weights/decoder_weights_epoch_{}\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955d5cd-4fb2-4ac6-8f2b-9db6c1932933",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de74b5e-70c8-444e-80bb-35a938ddd990",
   "metadata": {},
   "source": [
    "## Motion Export Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74851d-0d72-4f76-88c2-13a601f01c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "poseRenderer = PoseRenderer(edge_list)\n",
    "\n",
    "def export_sequence_anim(pose_sequence, file_name):\n",
    "    \n",
    "    pose_count = pose_sequence.shape[0]\n",
    "    pose_sequence = np.reshape(pose_sequence, (pose_count, joint_count, joint_dim))\n",
    "    \n",
    "    pose_sequence = torch.tensor(np.expand_dims(pose_sequence, axis=0)).to(device)\n",
    "    zero_trajectory = torch.tensor(np.zeros((1, pose_count, 3), dtype=np.float32)).to(device)\n",
    "    \n",
    "    skel_sequence = forward_kinematics(pose_sequence, zero_trajectory)\n",
    "    \n",
    "    skel_sequence = skel_sequence.detach().cpu().numpy()\n",
    "    skel_sequence = np.squeeze(skel_sequence)    \n",
    "    \n",
    "    view_min, view_max = utils.get_equal_mix_max_positions(skel_sequence)\n",
    "    skel_images = poseRenderer.create_pose_images(skel_sequence, view_min, view_max, view_ele, view_azi, view_line_width, view_size, view_size)\n",
    "    skel_images[0].save(file_name, save_all=True, append_images=skel_images[1:], optimize=False, duration=33.0, loop=0)\n",
    "\n",
    "def export_sequence_bvh(pose_sequence, file_name):\n",
    "    \n",
    "    pose_count = pose_sequence.shape[0]\n",
    "\n",
    "    pred_dataset = {}\n",
    "    pred_dataset[\"frame_rate\"] = mocap_data[\"frame_rate\"]\n",
    "    pred_dataset[\"rot_sequence\"] = mocap_data[\"rot_sequence\"]\n",
    "    pred_dataset[\"skeleton\"] = mocap_data[\"skeleton\"]\n",
    "    pred_dataset[\"motion\"] = {}\n",
    "    pred_dataset[\"motion\"][\"pos_local\"] = np.repeat(np.expand_dims(pred_dataset[\"skeleton\"][\"offsets\"], axis=0), pose_count, axis=0)\n",
    "    pred_dataset[\"motion\"][\"rot_local\"] = pose_sequence\n",
    "    pred_dataset[\"motion\"][\"rot_local_euler\"] = mocap_tools.quat_to_euler_bvh(pred_dataset[\"motion\"][\"rot_local\"], pred_dataset[\"rot_sequence\"])\n",
    "\n",
    "    pred_bvh = mocap_tools.mocap_to_bvh(pred_dataset)\n",
    "    \n",
    "    bvh_tools.write(pred_bvh, file_name)\n",
    "\n",
    "def export_sequence_fbx(pose_sequence, file_name):\n",
    "    \n",
    "    pose_count = pose_sequence.shape[0]\n",
    "    \n",
    "    pred_dataset = {}\n",
    "    pred_dataset[\"frame_rate\"] = mocap_data[\"frame_rate\"]\n",
    "    pred_dataset[\"rot_sequence\"] = mocap_data[\"rot_sequence\"]\n",
    "    pred_dataset[\"skeleton\"] = mocap_data[\"skeleton\"]\n",
    "    pred_dataset[\"motion\"] = {}\n",
    "    pred_dataset[\"motion\"][\"pos_local\"] = np.repeat(np.expand_dims(pred_dataset[\"skeleton\"][\"offsets\"], axis=0), pose_count, axis=0)\n",
    "    pred_dataset[\"motion\"][\"rot_local\"] = pose_sequence\n",
    "    pred_dataset[\"motion\"][\"rot_local_euler\"] = mocap_tools.quat_to_euler(pred_dataset[\"motion\"][\"rot_local\"], pred_dataset[\"rot_sequence\"])\n",
    "    \n",
    "    pred_fbx = mocap_tools.mocap_to_fbx([pred_dataset])\n",
    "    \n",
    "    fbx_tools.write(pred_fbx, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de875b-4c1a-4f81-bc3d-a896c1a02fd3",
   "metadata": {},
   "source": [
    "## Motion Reconstruction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e66baeb-1cb6-4c6c-9ad0-24b14ebd6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(orig_sequence, frame_indices):\n",
    "    \n",
    "    encoder.eval()\n",
    "    \n",
    "    latent_vectors = []\n",
    "    \n",
    "    seq_excerpt_count = len(frame_indices)\n",
    "\n",
    "    for excerpt_index in range(seq_excerpt_count):\n",
    "        excerpt_start_frame = frame_indices[excerpt_index]\n",
    "        excerpt_end_frame = excerpt_start_frame + sequence_length\n",
    "\n",
    "        excerpt = orig_sequence[excerpt_start_frame:excerpt_end_frame]\n",
    "        excerpt = np.expand_dims(excerpt, axis=0)\n",
    "        excerpt = torch.from_numpy(excerpt).reshape(1, sequence_length, pose_dim).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            encoder_output = encoder(excerpt)\n",
    "\n",
    "            encoder_output_mu = encoder_output[0]\n",
    "            encoder_output_std = encoder_output[1]\n",
    "            mu = torch.tanh(encoder_output_mu)\n",
    "            std = torch.abs(torch.tanh(encoder_output_std)) + 0.00001\n",
    "            \n",
    "            latent_vector = encoder.reparameterize(mu, std)\n",
    "            \n",
    "        latent_vector = torch.squeeze(latent_vector)\n",
    "        latent_vector = latent_vector.detach().cpu().numpy()\n",
    "\n",
    "        latent_vectors.append(latent_vector)\n",
    "        \n",
    "    encoder.train()\n",
    "        \n",
    "    return latent_vectors\n",
    "\n",
    "def decode_sequence_encodings(sequence_encodings, seq_overlap, base_pose):\n",
    "    \n",
    "    decoder.eval()\n",
    "    \n",
    "    seq_env = np.hanning(sequence_length)\n",
    "    seq_excerpt_count = len(sequence_encodings)\n",
    "    gen_seq_length = (seq_excerpt_count - 1) * seq_overlap + sequence_length\n",
    "\n",
    "    gen_sequence = np.full(shape=(gen_seq_length, joint_count, joint_dim), fill_value=base_pose)\n",
    "    \n",
    "    for excerpt_index in range(len(sequence_encodings)):\n",
    "        latent_vector = sequence_encodings[excerpt_index]\n",
    "        latent_vector = np.expand_dims(latent_vector, axis=0)\n",
    "        latent_vector = torch.from_numpy(latent_vector).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            excerpt_dec = decoder(latent_vector)\n",
    "        \n",
    "        excerpt_dec = torch.squeeze(excerpt_dec)\n",
    "        excerpt_dec = excerpt_dec.detach().cpu().numpy()\n",
    "        excerpt_dec = np.reshape(excerpt_dec, (-1, joint_count, joint_dim))\n",
    "        \n",
    "        gen_frame = excerpt_index * seq_overlap\n",
    "        \n",
    "        for si in range(sequence_length):\n",
    "            for ji in range(joint_count): \n",
    "                current_quat = gen_sequence[gen_frame + si, ji, :]\n",
    "                target_quat = excerpt_dec[si, ji, :]\n",
    "                quat_mix = seq_env[si]\n",
    "                mix_quat = slerp(current_quat, target_quat, quat_mix )\n",
    "                gen_sequence[gen_frame + si, ji, :] = mix_quat\n",
    "        \n",
    "    gen_sequence = gen_sequence.reshape((-1, 4))\n",
    "    gen_sequence = gen_sequence / np.linalg.norm(gen_sequence, ord=2, axis=1, keepdims=True)\n",
    "    gen_sequence = gen_sequence.reshape((gen_seq_length, joint_count, joint_dim))\n",
    "    gen_sequence = qfix(gen_sequence)\n",
    "\n",
    "    decoder.train()\n",
    "    \n",
    "    return gen_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3ed61-3cd2-4f81-91bb-87e9ebb3e14c",
   "metadata": {},
   "source": [
    "## Latent Space Visualisation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb743f-c64d-4bb4-8cbe-ac22b7c4cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2d_latent_space_representation(sequence_excerpts):\n",
    "\n",
    "    encodings = []\n",
    "    \n",
    "    excerpt_count = sequence_excerpts.shape[0]\n",
    "    \n",
    "    for eI in range(0, excerpt_count, batch_size):\n",
    "        \n",
    "        excerpt_batch = sequence_excerpts[eI:eI+batch_size]\n",
    "        \n",
    "        #print(\"excerpt_batch s \", excerpt_batch.shape)\n",
    "        \n",
    "        excerpt_batch = torch.from_numpy(excerpt_batch).to(device)\n",
    "        \n",
    "        encoder_output = encoder(excerpt_batch)\n",
    "\n",
    "        encoder_output_mu = encoder_output[0]\n",
    "        encoder_output_std = encoder_output[1]\n",
    "        mu = torch.tanh(encoder_output_mu)\n",
    "        std = torch.abs(torch.tanh(encoder_output_std)) + 0.00001\n",
    "        \n",
    "        encoding_batch = encoder.reparameterize(mu, std)\n",
    "        \n",
    "        #print(\"encoding_batch s \", encoding_batch.shape)\n",
    "        \n",
    "        encoding_batch = encoding_batch.detach().cpu()\n",
    "\n",
    "        encodings.append(encoding_batch)\n",
    "        \n",
    "    encodings = torch.cat(encodings, dim=0)\n",
    "    \n",
    "    #print(\"encodings s \", encodings.shape)\n",
    "    \n",
    "    encodings = encodings.numpy()\n",
    "\n",
    "    # use TSNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, n_iter=5000, verbose=1)    \n",
    "    Z_tsne = tsne.fit_transform(encodings)\n",
    "    \n",
    "    return Z_tsne\n",
    "\n",
    "def create_2d_latent_space_image(Z_tsne, highlight_excerpt_ranges, file_name):\n",
    "    \n",
    "    Z_tsne_x = Z_tsne[:,0]\n",
    "    Z_tsne_y = Z_tsne[:,1]\n",
    "\n",
    "    plot_colors = [\"green\", \"red\", \"blue\", \"magenta\", \"orange\"]\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(Z_tsne_x, Z_tsne_y, '-', c=\"grey\",linewidth=0.2)\n",
    "    ax.scatter(Z_tsne_x, Z_tsne_y, s=0.1, c=\"grey\", alpha=0.5)\n",
    "    \n",
    "    for hI, hR in enumerate(highlight_excerpt_ranges):\n",
    "        ax.plot(Z_tsne_x[hR[0]:hR[1]], Z_tsne_y[hR[0]:hR[1]], '-', c=plot_colors[hI],linewidth=0.6)\n",
    "        ax.scatter(Z_tsne_x[hR[0]:hR[1]], Z_tsne_y[hR[0]:hR[1]], s=0.8, c=plot_colors[hI], alpha=0.5)\n",
    "        \n",
    "        ax.set_xlabel('$c_1$')\n",
    "        ax.set_ylabel('$c_2$')\n",
    "\n",
    "    fig.savefig(file_name, dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f961b5f-7513-49d1-aab8-34474f96ab21",
   "metadata": {},
   "source": [
    "## Create Latent Space Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c5cee-a179-4824-a76a-4b268f13be81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_tsne = create_2d_latent_space_representation(pose_sequence_excerpts)\n",
    "create_2d_latent_space_image(Z_tsne, [], \"latent_space_plot_epoch_{}.png\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4649b901-c7f5-4c13-8d60-b950dcfe7cef",
   "metadata": {},
   "source": [
    "## Perform Motion Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8003ed-24da-47a0-91c2-436d4dba3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sequence = all_mocap_data[0][\"motion\"][\"rot_local\"].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d203358-89b9-49a5-beb3-3f8ce9e4517c",
   "metadata": {},
   "source": [
    "## Export Original Motion Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c13d8c-6fb5-42a3-b3d0-57b77363b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_start = 1000\n",
    "seq_length = 1000\n",
    "\n",
    "seq_start_gui = widgets.IntText(value=seq_start, description=\"Sequence Start (Frames):\", style={'description_width': 'initial'})\n",
    "seq_length_gui = widgets.IntText(value=seq_length, description=\"Sequence Lenght (Frames):\", style={'description_width': 'initial'})\n",
    "\n",
    "display(seq_start_gui)\n",
    "display(seq_length_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77a8f86-5352-45e3-82d6-a67d8848d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_start = seq_start_gui.value\n",
    "seq_length = seq_length_gui.value\n",
    "\n",
    "export_sequence_anim(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.gif\".format(seq_start, seq_length))\n",
    "export_sequence_fbx(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.fbx\".format(seq_start, seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6f709-dd16-46b5-9392-416163054a70",
   "metadata": {},
   "source": [
    "## Reconstruct and Export  Motion Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c89d0-6f6b-41ef-ba32-e9beefb44050",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_start = 1000\n",
    "seq_length = 1000\n",
    "seq_overlap = 16 # 2 for 8, 32 for 128\n",
    "\n",
    "seq_start_gui = widgets.IntText(value=seq_start, description=\"Sequence Start (Frames):\", style={'description_width': 'initial'})\n",
    "seq_length_gui = widgets.IntText(value=seq_length, description=\"Sequence Lenght (Frames):\", style={'description_width': 'initial'})\n",
    "seq_overlap_gui = widgets.IntText(value=seq_overlap, description=\"Sequence Overlap (Frames):\", style={'description_width': 'initial'})\n",
    "\n",
    "display(seq_start_gui)\n",
    "display(seq_length_gui)\n",
    "display(seq_overlap_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33631f6d-52a0-47db-9835-cef278292c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_start = seq_start_gui.value\n",
    "seq_length = seq_length_gui.value\n",
    "seq_overlap = seq_overlap_gui.value\n",
    "\n",
    "base_pose = np.reshape(orig_sequence[0], (joint_count, joint_dim))\n",
    "seq_indices = [ frame_index for frame_index in range(seq_start, seq_start + seq_length, seq_overlap)]\n",
    "\n",
    "seq_encodings = encode_sequences(orig_sequence, seq_indices)\n",
    "gen_sequence = decode_sequence_encodings(seq_encodings, seq_overlap, base_pose)\n",
    "export_sequence_anim(gen_sequence, \"results/anims/rec_sequences_epoch_{}_seq_start_{}_length_{}.gif\".format(epochs, seq_start, seq_length))\n",
    "export_sequence_fbx(gen_sequence, \"results/anims/rec_sequences_epoch_{}_seq_start_{}_length_{}.fbx\".format(epochs, seq_start, seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90378be3-fb31-41e3-a38b-8bea3b04c484",
   "metadata": {},
   "source": [
    "## Latent Space Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c262ba-2fb7-4cb3-bbbf-23c79487c369",
   "metadata": {},
   "source": [
    "## Random Walk in Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d1755-ac1a-4e51-b16b-cbd6ed1427c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_start = 1000\n",
    "seq_length = 1000\n",
    "\n",
    "seq_start_gui = widgets.IntText(value=seq_start, description=\"Sequence Start (Frames):\", style={'description_width': 'initial'})\n",
    "seq_length_gui = widgets.IntText(value=seq_length, description=\"Sequence Lenght (Frames):\", style={'description_width': 'initial'})\n",
    "\n",
    "display(seq_start_gui)\n",
    "display(seq_length_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2527f-9f63-4849-b8d6-35a4714af957",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_start = seq_start_gui.value\n",
    "seq_length = seq_length_gui.value\n",
    "\n",
    "seq_indices = [seq_start]\n",
    "\n",
    "seq_encodings = encode_sequences(orig_sequence, seq_indices)\n",
    "\n",
    "for index in range(0, seq_length // seq_overlap):\n",
    "    random_step = np.random.random((latent_dim)).astype(np.float32) * 2.0\n",
    "    seq_encodings.append(seq_encodings[index] + random_step)\n",
    "    \n",
    "gen_sequence = decode_sequence_encodings(seq_encodings, seq_overlap, base_pose)\n",
    "export_sequence_anim(gen_sequence, \"results/anims/seq_randwalk_epoch_{}_seq_start_{}_length_{}.gif\".format(epochs, seq_start, seq_length))\n",
    "export_sequence_fbx(gen_sequence, \"results/anims/seq_randwalk_epoch_{}_seq_start_{}_length_{}.fbx\".format(epochs, seq_start, seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98bb45e-0141-4117-badb-006b65c73157",
   "metadata": {},
   "source": [
    "## Sequence Offset Following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64de6f2-ef35-4757-b154-6110b495baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_start = 1000\n",
    "seq_length = 1000\n",
    "\n",
    "seq_start_gui = widgets.IntText(value=seq_start, description=\"Sequence Start (Frames):\", style={'description_width': 'initial'})\n",
    "seq_length_gui = widgets.IntText(value=seq_length, description=\"Sequence Lenght (Frames):\", style={'description_width': 'initial'})\n",
    "\n",
    "display(seq_start_gui)\n",
    "display(seq_length_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f16f722-edb7-4812-9d1a-c19983f16c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_start = seq_start_gui.value\n",
    "seq_length = seq_length_gui.value\n",
    "\n",
    "seq_indices = [ seq_index for seq_index in range(seq_start, seq_start + seq_length, seq_overlap)]\n",
    "\n",
    "seq_encodings = encode_sequences(orig_sequence, seq_indices)\n",
    "\n",
    "offset_seq_encodings = []\n",
    "\n",
    "for index in range(len(seq_encodings)):\n",
    "    sin_value = np.sin(index / (len(seq_encodings) - 1) * np.pi * 4.0)\n",
    "    offset = np.ones(shape=(latent_dim), dtype=np.float32) * sin_value * 4.0\n",
    "    offset_seq_encoding = seq_encodings[index] + offset\n",
    "    offset_seq_encoding = np.float32(offset_seq_encoding)\n",
    "    offset_seq_encodings.append(offset_seq_encoding)\n",
    "    \n",
    "gen_sequence = decode_sequence_encodings(offset_seq_encodings, seq_overlap, base_pose)\n",
    "export_sequence_anim(gen_sequence, \"results/anims/seq_offset_epoch_{}_seq_start_{}_length_{}.gif\".format(epochs, seq_start, seq_length))\n",
    "export_sequence_fbx(gen_sequence, \"results/anims/seq_offset_epoch_{}_seq_start_{}_length_{}.fbx\".format(epochs, seq_start, seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75829a-71ea-4106-b376-270e399fb35b",
   "metadata": {},
   "source": [
    "## Interpolate Two Motion Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24982bc-1c84-4083-883d-a3cc8b5b388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1_start = 1000\n",
    "seq2_start = 2000\n",
    "seq_length = 1000\n",
    "\n",
    "seq1_start_gui = widgets.IntText(value=seq1_start, description=\"Sequence 1 Start (Frames):\", style={'description_width': 'initial'})\n",
    "seq2_start_gui = widgets.IntText(value=seq2_start, description=\"Sequence 2 Start (Frames):\", style={'description_width': 'initial'})\n",
    "seq_length_gui = widgets.IntText(value=seq_length, description=\"Sequence Lenght (Frames):\", style={'description_width': 'initial'})\n",
    "\n",
    "display(seq1_start)\n",
    "display(seq2_start)\n",
    "display(seq_length_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7d9ae-24bf-4e61-9ecd-e0de9b5405bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1_start = seq1_start_gui.value\n",
    "seq2_start = seq2_start_gui.value\n",
    "seq_length = seq_length_gui.value\n",
    "\n",
    "seq1_indices = [ seq_index for seq_index in range(seq1_start, seq1_start + seq_length, seq_overlap)]\n",
    "seq2_indices = [ seq_index for seq_index in range(seq2_start, seq2_start + seq_length, seq_overlap)]\n",
    "\n",
    "seq1_encodings = encode_sequences(orig_sequence, seq1_indices)\n",
    "seq2_encodings = encode_sequences(orig_sequence, seq2_indices)\n",
    "\n",
    "mix_encodings = []\n",
    "\n",
    "for index in range(len(seq1_encodings)):\n",
    "    mix_factor = index / (len(seq1_indices) - 1)\n",
    "    mix_encoding = seq1_encodings[index] * (1.0 - mix_factor) + seq2_encodings[index] * mix_factor\n",
    "    mix_encodings.append(mix_encoding)\n",
    "\n",
    "gen_sequence = decode_sequence_encodings(mix_encodings, seq_overlap, base_pose)\n",
    "export_sequence_anim(gen_sequence, \"results/anims/seq_mix_epoch_{}_seq1_start_{}_seq2_start_{}_length_{}.gif\".format(epochs, seq1_start, seq2_start, seq_length))\n",
    "export_sequence_fbx(gen_sequence, \"results/anims/seq_mix_epoch_{}_seq1_start_{}_seq2_start_{}_length_{}.fbx\".format(epochs, seq1_start, seq2_start, seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd1bcf-c10c-4caf-a2a0-e357fede0529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
