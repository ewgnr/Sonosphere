{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd929332-f8eb-44e0-a76a-daf300d75e19",
   "metadata": {},
   "source": [
    "# Motion Continuation (Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610cc86f-ab41-4b33-b1d7-71e7bec482fe",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6929bb1-95af-4103-a35f-3030a4895b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, subprocess\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import networkx as nx\n",
    "import scipy.linalg as sclinalg\n",
    "\n",
    "from common import utils\n",
    "from common import bvh_tools as bvh\n",
    "from common import fbx_tools as fbx\n",
    "from common import mocap_tools as mocap\n",
    "from common.quaternion import qmul, qrot, qnormalize_np, slerp\n",
    "from common.pose_renderer import PoseRenderer\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e4574c-972c-4ee1-93b0-016fc196ffc4",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880f23e-64f0-47a6-9d10-308cbc46ee3f",
   "metadata": {},
   "source": [
    "### Compute Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe28a07-b203-4700-9539-050c8bb2c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b537524d-5d06-4484-b5cc-25846eaf0502",
   "metadata": {},
   "source": [
    "### Mocap Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b100049-9549-4064-ac46-6334fe05ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mocap_file_path = \"../../../Data/Mocap/\"\n",
    "mocap_files = [\"Daniel_ChineseRoom_Take1_50fps.fbx\"]\n",
    "mocap_valid_frame_ranges = [ [ 400, 24600 ] ]\n",
    "mocap_pos_scale = 1.0\n",
    "mocap_fps = 50\n",
    "mocap_loss_file_path = \"data/configs/\"\n",
    "mocap_loss_weights_file = None\n",
    "\n",
    "\n",
    "mocap_pos_scale_gui = widgets.FloatText(mocap_pos_scale, description=\"Mocap Position Scale:\", style={'description_width': 'initial'})\n",
    "mocap_fps_gui = widgets.IntText(mocap_fps, description=\"Mocap FPS:\", style={'description_width': 'initial'})\n",
    "\n",
    "mocap_files_all = [f for f in os.listdir(mocap_file_path) if os.path.isfile(os.path.join(mocap_file_path, f))]\n",
    "#print(mocap_files_all)\n",
    "\n",
    "mocap_valid_frame_ranges_gui = widgets.Text(str(mocap_valid_frame_ranges), description='Mocap Valid Frames:', style={'description_width': 'initial'})\n",
    "\n",
    "mocap_loss_file_all = [f for f in os.listdir(mocap_loss_file_path) if os.path.isfile(os.path.join(mocap_loss_file_path, f))]\n",
    "#print(mocap_loss_file_all)\n",
    "\n",
    "mocap_files_gui = widgets.SelectMultiple(\n",
    "    options=mocap_files_all,\n",
    "    value=mocap_files,  # default: first option selected; can be empty\n",
    "    description='Mocap Files:',\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "mocap_loss_weights_gui = widgets.Dropdown(\n",
    "    options=mocap_loss_file_all,\n",
    "    value=mocap_loss_weights_file,  # default: first option selected; can be empty\n",
    "    description='Mocap Loss Weight File:',\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(mocap_pos_scale_gui)\n",
    "display(mocap_fps_gui)\n",
    "display(mocap_files_gui)\n",
    "display(mocap_valid_frame_ranges_gui)\n",
    "display(mocap_loss_weights_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368eb3df-bfd3-4ace-a406-8f615b87f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "mocap_pos_scale = mocap_pos_scale_gui.value\n",
    "mocap_fps = mocap_fps_gui.value\n",
    "mocap_files = list(mocap_files_gui.value)\n",
    "\n",
    "matches = re.findall(r\"\\[\\s*([-+]?\\d*\\.?\\d+)\\s*,\\s*([-+]?\\d*\\.?\\d+)\\s*\\]\", mocap_valid_frame_ranges_gui.value)\n",
    "mocap_valid_frame_ranges = [[int(a), int(b)] for a, b in matches]\n",
    "\n",
    "mocap_loss_weights_file = mocap_loss_weights_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e39d5-7ecc-4c50-884c-9c71993c32ed",
   "metadata": {},
   "source": [
    "### Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1601208a-49f9-453d-96f6-420d86569d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer_dim = 512\n",
    "rnn_layer_count = 2\n",
    "\n",
    "save_weights = True\n",
    "load_weights = False\n",
    "rnn_weights_file = \"results/weights/rnn_weights_epoch_200\"\n",
    "\n",
    "rnn_layer_count_gui = widgets.IntText(value=rnn_layer_count, description=\"LSTM Layer Count:\", style={'description_width': 'initial'})\n",
    "rnn_layer_dim_gui = widgets.IntText(value=rnn_layer_dim, description=\"LSTM Layer Dim:\", style={'description_width': 'initial'})\n",
    "\n",
    "save_weights_gui = widgets.Checkbox(\n",
    "    value=save_weights,\n",
    "    description='Save Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "load_weights_gui = widgets.Checkbox(\n",
    "    value=load_weights,\n",
    "    description='Load Weights',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "rnn_weights_file_gui = widgets.Text(value=rnn_weights_file, description=\"RNN Weights File:\", style={'description_width': 'initial'}) \n",
    "\n",
    "display(rnn_layer_count_gui)\n",
    "display(rnn_layer_dim_gui)\n",
    "display(save_weights_gui)\n",
    "display(load_weights_gui)\n",
    "display(rnn_weights_file_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537986f4-bbab-41cb-b7d6-2a994e90656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer_count = rnn_layer_count_gui.value\n",
    "rnn_layer_dim = rnn_layer_dim_gui.value\n",
    "save_weights = save_weights_gui.value\n",
    "load_weights = load_weights_gui.value\n",
    "rnn_weights_file = rnn_weights_file_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bec4a4-1f7c-47a1-9001-1e31b903b2ab",
   "metadata": {},
   "source": [
    "### Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83b373a-37a1-4625-a7e5-56a2eb79c558",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'widgets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m     15\u001b[0m save_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m batch_size_gui \u001b[38;5;241m=\u001b[39m \u001b[43mwidgets\u001b[49m\u001b[38;5;241m.\u001b[39mIntText(value\u001b[38;5;241m=\u001b[39mbatch_size, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, style\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription_width\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     18\u001b[0m test_percentage_gui \u001b[38;5;241m=\u001b[39m widgets\u001b[38;5;241m.\u001b[39mFloatText(value\u001b[38;5;241m=\u001b[39mtest_percentage, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Ratio:\u001b[39m\u001b[38;5;124m\"\u001b[39m, style\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription_width\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     19\u001b[0m seq_input_length_gui \u001b[38;5;241m=\u001b[39m widgets\u001b[38;5;241m.\u001b[39mIntText(value\u001b[38;5;241m=\u001b[39mseq_input_length, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence Input Length:\u001b[39m\u001b[38;5;124m\"\u001b[39m, style\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription_width\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'widgets' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "test_percentage = 0.1\n",
    "\n",
    "seq_input_length = 64\n",
    "seq_output_length = 10 # this is only used for non-teacher forcing scenarios\n",
    "\n",
    "learning_rate = 1e-4\n",
    "norm_loss_scale = 0.1\n",
    "pos_loss_scale = 0.1\n",
    "quat_loss_scale = 0.9\n",
    "teacher_forcing_prob = 0.0\n",
    "model_save_interval = 10\n",
    "\n",
    "epochs = 200\n",
    "save_history = True\n",
    "\n",
    "batch_size_gui = widgets.IntText(value=batch_size, description=\"Batch Size:\", style={'description_width': 'initial'})\n",
    "test_percentage_gui = widgets.FloatText(value=test_percentage, description=\"Test Ratio:\", style={'description_width': 'initial'})\n",
    "seq_input_length_gui = widgets.IntText(value=seq_input_length, description=\"Sequence Input Length:\", style={'description_width': 'initial'})\n",
    "seq_output_length_gui = widgets.IntText(value=seq_output_length, description=\"Sequence Output Length:\", style={'description_width': 'initial'})\n",
    "learning_rate_gui = widgets.FloatText(value=learning_rate, description=\"Autoencoder Learning Rate:\", style={'description_width': 'initial'})\n",
    "norm_loss_scale_gui = widgets.FloatText(value=norm_loss_scale, description=\"Normalization Loss Scale:\", style={'description_width': 'initial'})\n",
    "pos_loss_scale_gui = widgets.FloatText(value=pos_loss_scale, description=\"Position Loss Scale:\", style={'description_width': 'initial'})\n",
    "quat_loss_scale_gui = widgets.FloatText(value=quat_loss_scale, description=\"Quaternion Loss Scale:\", style={'description_width': 'initial'})\n",
    "teacher_forcing_prob_gui = widgets.FloatText(value=teacher_forcing_prob, description=\"Teacher Forcing Probability:\", style={'description_width': 'initial'})\n",
    "model_save_interval_gui = widgets.IntText(value=model_save_interval, description=\"Model Save Interval:\", style={'description_width': 'initial'})\n",
    "epochs_gui = widgets.IntText(value=epochs, description=\"Epochs:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(batch_size_gui)\n",
    "display(test_percentage_gui)\n",
    "display(seq_input_length_gui)\n",
    "display(seq_output_length_gui)\n",
    "display(learning_rate_gui)\n",
    "display(norm_loss_scale_gui)\n",
    "display(pos_loss_scale_gui)\n",
    "display(quat_loss_scale_gui)\n",
    "display(teacher_forcing_prob_gui)\n",
    "display(model_save_interval_gui)\n",
    "display(epochs_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d07f2-90ae-4684-bd33-725fcea2624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = batch_size_gui.value\n",
    "test_percentage = test_percentage_gui.value\n",
    "seq_input_length = seq_input_length_gui.value\n",
    "seq_output_length = seq_output_length_gui.value\n",
    "learning_rate = learning_rate_gui.value\n",
    "norm_loss_scale = norm_loss_scale_gui.value\n",
    "pos_loss_scale = pos_loss_scale_gui.value\n",
    "quat_loss_scale = quat_loss_scale_gui.value\n",
    "teacher_forcing_prob = teacher_forcing_prob_gui.value\n",
    "model_save_interval = model_save_interval_gui.value\n",
    "epochs = epochs_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d60964-9d5e-44a4-b7a8-62d01932cd4e",
   "metadata": {},
   "source": [
    "### Visualization settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34403ee-bee2-4c61-9c85-103747d293c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_ele = 90.0\n",
    "view_azi = -90.0\n",
    "view_line_width = 1.0\n",
    "view_size = 4.0\n",
    "\n",
    "view_ele_gui = widgets.FloatText(value=view_ele, description=\"View Elevation:\", style={'description_width': 'initial'})\n",
    "view_azi_gui = widgets.FloatText(value=view_azi, description=\"View Azimuth:\", style={'description_width': 'initial'})\n",
    "view_line_width_gui = widgets.FloatText(value=view_line_width, description=\"View Line Width:\", style={'description_width': 'initial'})\n",
    "view_size_gui = widgets.FloatText(value=view_size, description=\"View Size:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(view_ele_gui)\n",
    "display(view_azi_gui)\n",
    "display(view_line_width_gui)\n",
    "display(view_size_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9381a0c-2810-4c67-9ea9-442850dcc42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_ele = view_ele_gui.value\n",
    "view_azi = view_azi_gui.value\n",
    "view_line_width = view_line_width_gui.value\n",
    "view_size = view_size_gui.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e5f0a-d515-4a44-812c-c3f429e3d524",
   "metadata": {},
   "source": [
    "## Load Mocap Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e22f2-536b-4872-a05c-850929552985",
   "metadata": {},
   "outputs": [],
   "source": [
    "bvh_tools = bvh.BVH_Tools()\n",
    "fbx_tools = fbx.FBX_Tools()\n",
    "mocap_tools = mocap.Mocap_Tools()\n",
    "\n",
    "all_mocap_data = []\n",
    "\n",
    "for mocap_file in mocap_files:\n",
    "    \n",
    "    print(\"process file \", mocap_file)\n",
    "    \n",
    "    if mocap_file.endswith(\".bvh\") or mocap_file.endswith(\".BVH\"):\n",
    "        bvh_data = bvh_tools.load(mocap_file_path + \"/\" + mocap_file)\n",
    "        mocap_data = mocap_tools.bvh_to_mocap(bvh_data)\n",
    "    elif mocap_file.endswith(\".fbx\") or mocap_file.endswith(\".FBX\"):\n",
    "        fbx_data = fbx_tools.load(mocap_file_path + \"/\" + mocap_file)\n",
    "        mocap_data = mocap_tools.fbx_to_mocap(fbx_data)[0] # first skeleton only\n",
    "    \n",
    "    mocap_data[\"skeleton\"][\"offsets\"] *= mocap_pos_scale\n",
    "    mocap_data[\"motion\"][\"pos_local\"] *= mocap_pos_scale\n",
    "    \n",
    "    # set x and z offset of root joint to zero\n",
    "    mocap_data[\"skeleton\"][\"offsets\"][0, 0] = 0.0 \n",
    "    mocap_data[\"skeleton\"][\"offsets\"][0, 2] = 0.0 \n",
    "    \n",
    "    if mocap_file.endswith(\".bvh\") or mocap_file.endswith(\".BVH\"):\n",
    "        mocap_data[\"motion\"][\"rot_local\"] = mocap_tools.euler_to_quat_bvh(mocap_data[\"motion\"][\"rot_local_euler\"], mocap_data[\"rot_sequence\"])\n",
    "    elif mocap_file.endswith(\".fbx\") or mocap_file.endswith(\".FBX\"):\n",
    "        mocap_data[\"motion\"][\"rot_local\"] = mocap_tools.euler_to_quat(mocap_data[\"motion\"][\"rot_local_euler\"], mocap_data[\"rot_sequence\"])\n",
    "\n",
    "    all_mocap_data.append(mocap_data)\n",
    "\n",
    "\n",
    "# retrieve mocap properties\n",
    "\n",
    "mocap_data = all_mocap_data[0]\n",
    "joint_count = mocap_data[\"motion\"][\"rot_local\"].shape[1]\n",
    "joint_dim = mocap_data[\"motion\"][\"rot_local\"].shape[2]\n",
    "pose_dim = joint_count * joint_dim\n",
    "\n",
    "offsets = mocap_data[\"skeleton\"][\"offsets\"].astype(np.float32)\n",
    "parents = mocap_data[\"skeleton\"][\"parents\"]\n",
    "children = mocap_data[\"skeleton\"][\"children\"]\n",
    "\n",
    "# create edge list\n",
    "def get_edge_list(children):\n",
    "    edge_list = []\n",
    "\n",
    "    for parent_joint_index in range(len(children)):\n",
    "        for child_joint_index in children[parent_joint_index]:\n",
    "            edge_list.append([parent_joint_index, child_joint_index])\n",
    "    \n",
    "    return edge_list\n",
    "\n",
    "edge_list = get_edge_list(children)\n",
    "\n",
    "# set joint loss weigths \n",
    "\n",
    "if mocap_loss_weights_file is not None:\n",
    "    with open(mocap_loss_weights_file) as f:\n",
    "        joint_loss_weights = json.load(f)\n",
    "        joint_loss_weights = joint_loss_weights[\"joint_loss_weights\"]\n",
    "else:\n",
    "    joint_loss_weights = [1.0]\n",
    "    joint_loss_weights *= joint_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6515aac-9b2f-4542-b91f-0bbed9444598",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc7e06f-5ee7-4dbb-b8fc-bcd48d3216b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_mocap_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m X \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, mocap_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mall_mocap_data\u001b[49m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmocap \u001b[39m\u001b[38;5;124m\"\u001b[39m, mocap_files[i])\n\u001b[0;32m      8\u001b[0m     pose_sequence \u001b[38;5;241m=\u001b[39m mocap_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmotion\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrot_local\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_mocap_data' is not defined"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i, mocap_data in enumerate(all_mocap_data):\n",
    "    \n",
    "    print(\"mocap \", mocap_files[i])\n",
    "    \n",
    "    pose_sequence = mocap_data[\"motion\"][\"rot_local\"]\n",
    "    pose_sequence = np.reshape(pose_sequence, (-1, pose_dim))\n",
    "    \n",
    "    print(\"shape \", pose_sequence.shape)\n",
    "    \n",
    "    valid_frame_range = mocap_valid_frame_ranges[i]\n",
    "\n",
    "    frame_range_start = valid_frame_range[0]\n",
    "    frame_range_end = valid_frame_range[1]\n",
    "\n",
    "    for pI in np.arange(frame_range_start, frame_range_end - seq_input_length - seq_output_length - 1):\n",
    "\n",
    "        X_sample = pose_sequence[pI:pI+seq_input_length]\n",
    "        X.append(X_sample.reshape((seq_input_length, pose_dim)))\n",
    "            \n",
    "        Y_sample = pose_sequence[pI+seq_input_length:pI+seq_input_length+seq_output_length]  \n",
    "        y.append(Y_sample.reshape((seq_output_length, pose_dim)))\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X = torch.from_numpy(X).to(torch.float32)\n",
    "y = torch.from_numpy(y).to(torch.float32)\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, ...], self.y[idx, ...]\n",
    "\n",
    "full_dataset = SequenceDataset(X, y)\n",
    "\n",
    "X_item, y_item = full_dataset[0]\n",
    "\n",
    "print(\"X_item s \", X_item.shape)\n",
    "print(\"y_item s \", y_item.shape)\n",
    "\n",
    "test_size = int(test_percentage * len(full_dataset))\n",
    "train_size = len(full_dataset) - test_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"X_batch s \", X_batch.shape)\n",
    "print(\"y_batch s \", y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39394a-d1e3-482a-9055-ee5552f4d502",
   "metadata": {},
   "source": [
    "## Create Reccurent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580909f-3e62-4f9f-bd5f-4f5bef05fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reccurent(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layer_count):\n",
    "        super(Reccurent, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_count = layer_count\n",
    "        self.output_dim = output_dim\n",
    "            \n",
    "        rnn_layers = []\n",
    "        \n",
    "        rnn_layers.append((\"rnn\", nn.LSTM(self.input_dim, self.hidden_dim, self.layer_count, batch_first=True)))\n",
    "        self.rnn_layers = nn.Sequential(OrderedDict(rnn_layers))\n",
    "        \n",
    "        dense_layers = []\n",
    "        dense_layers.append((\"dense\", nn.Linear(self.hidden_dim, self.output_dim)))\n",
    "        self.dense_layers = nn.Sequential(OrderedDict(dense_layers))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, (_, _) = self.rnn_layers(x)\n",
    "        \n",
    "        x = x[:, -1, :] # only last time step \n",
    "        x = self.dense_layers(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "rnn = Reccurent(pose_dim, rnn_layer_dim, pose_dim, rnn_layer_count).to(device)\n",
    "print(rnn)\n",
    "\n",
    "# test Reccurent model\n",
    "\n",
    "batch_x, _ = next(iter(train_loader))\n",
    "rnn_in = batch_x.to(device)\n",
    "rnn_out = rnn(rnn_in)\n",
    "\n",
    "print(\"rnn_in s \", rnn_in.shape)\n",
    "print(\"rnn_out s \", rnn_out.shape)\n",
    "\n",
    "if load_weights == True:\n",
    "    rnn.load_state_dict(torch.load(rnn_weights_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8a5bb-9000-4cc9-9b52-afde51f8ff63",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1ab3b-b85f-4d30-957b-4eeb7d7b3277",
   "metadata": {},
   "source": [
    "### Create Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b9f49-f76f-472b-907c-e3582d7a1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.336) # reduce the learning every 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9cafd8-51bb-4a11-8659-5f1ef29db4a3",
   "metadata": {},
   "source": [
    "### Create Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76995660-026d-460f-9141-1aa4bc6013e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_loss_weights = torch.tensor(joint_loss_weights, dtype=torch.float32)\n",
    "joint_loss_weights = joint_loss_weights.reshape(1, 1, -1).to(device)\n",
    "\n",
    "def norm_loss(yhat):\n",
    "    _yhat = yhat.view(-1, 4)\n",
    "    _norm = torch.norm(_yhat, dim=1)\n",
    "    _diff = (_norm - 1.0) ** 2\n",
    "    _loss = torch.mean(_diff)\n",
    "    return _loss\n",
    "\n",
    "def forward_kinematics(rotations, root_positions):\n",
    "    \"\"\"\n",
    "    Perform forward kinematics using the given trajectory and local rotations.\n",
    "    Arguments (where N = batch size, L = sequence length, J = number of joints):\n",
    "     -- rotations: (N, L, J, 4) tensor of unit quaternions describing the local rotations of each joint.\n",
    "     -- root_positions: (N, L, 3) tensor describing the root joint positions.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(rotations.shape) == 4\n",
    "    assert rotations.shape[-1] == 4\n",
    "    \n",
    "    toffsets = torch.tensor(offsets).to(device)\n",
    "    \n",
    "    positions_world = []\n",
    "    rotations_world = []\n",
    "\n",
    "    expanded_offsets = toffsets.expand(rotations.shape[0], rotations.shape[1], offsets.shape[0], offsets.shape[1])\n",
    "\n",
    "    # Parallelize along the batch and time dimensions\n",
    "    for jI in range(offsets.shape[0]):\n",
    "        if parents[jI] == -1:\n",
    "            positions_world.append(root_positions)\n",
    "            rotations_world.append(rotations[:, :, 0])\n",
    "        else:\n",
    "            positions_world.append(qrot(rotations_world[parents[jI]], expanded_offsets[:, :, jI]) \\\n",
    "                                   + positions_world[parents[jI]])\n",
    "            if len(children[jI]) > 0:\n",
    "                rotations_world.append(qmul(rotations_world[parents[jI]], rotations[:, :, jI]))\n",
    "            else:\n",
    "                # This joint is a terminal node -> it would be useless to compute the transformation\n",
    "                rotations_world.append(None)\n",
    "\n",
    "    return torch.stack(positions_world, dim=3).permute(0, 1, 3, 2)\n",
    "\n",
    "def pos_loss(y, yhat):\n",
    "    \n",
    "    #print(\"pos_loss\")\n",
    "    #print(\"y s \", y.shape)\n",
    "    #print(\"yhat s \", yhat.shape)\n",
    "    \n",
    "    # y and yhat shapes: batch_size, seq_length, pose_dim\n",
    "\n",
    "    # normalize tensors\n",
    "    _yhat = yhat.view(-1, 4)\n",
    "\n",
    "    _yhat_norm = nn.functional.normalize(_yhat, p=2, dim=1)\n",
    "    _y_rot = y.view((y.shape[0], y.shape[1], -1, 4))\n",
    "    _yhat_rot = _yhat.view((y.shape[0], y.shape[1], -1, 4))\n",
    "    \n",
    "    #print(\"_y_rot s \", _y_rot.shape)\n",
    "    #print(\"_yhat_rot s \", _yhat_rot.shape)\n",
    "\n",
    "    zero_trajectory = torch.zeros((y.shape[0], y.shape[1], 3), dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "    _y_pos = forward_kinematics(_y_rot, zero_trajectory)\n",
    "    _yhat_pos = forward_kinematics(_yhat_rot, zero_trajectory)\n",
    "    \n",
    "    #print(\"_y_pos s \", _y_pos.shape)\n",
    "    #print(\"_yhat_pos s \", _yhat_pos.shape)\n",
    "\n",
    "    _pos_diff = torch.norm((_y_pos - _yhat_pos), dim=3)\n",
    "    \n",
    "    #print(\"_pos_diff s \", _pos_diff.shape)\n",
    "    \n",
    "    _pos_diff_weighted = _pos_diff * joint_loss_weights\n",
    "    \n",
    "    _loss = torch.mean(_pos_diff_weighted)\n",
    "\n",
    "    return _loss\n",
    "\n",
    "def quat_loss(y, yhat):\n",
    "    \n",
    "    #print(\"quat_loss\")\n",
    "    #print(\"y s \", y.shape)\n",
    "    #print(\"yhat s \", yhat.shape)\n",
    "    \n",
    "    # y and yhat shapes: batch_size, seq_length, pose_dim\n",
    "    \n",
    "    # normalize quaternion\n",
    "    \n",
    "    _y = y.view((-1, 4))\n",
    "    _yhat = yhat.view((-1, 4))\n",
    "    _yhat_norm = nn.functional.normalize(_yhat, p=2, dim=1)\n",
    "    \n",
    "    #print(\"_y s \", _y.shape)\n",
    "    #print(\"_yhat_norm s \", _yhat_norm.shape)\n",
    "    \n",
    "    # inverse of quaternion: https://www.mathworks.com/help/aeroblks/quaternioninverse.html\n",
    "    _yhat_inv = _yhat_norm * torch.tensor([[1.0, -1.0, -1.0, -1.0]], dtype=torch.float32).to(device)\n",
    "\n",
    "    # calculate difference quaternion\n",
    "    _diff = qmul(_yhat_inv, _y)\n",
    "    # length of complex part\n",
    "    _len = torch.norm(_diff[:, 1:], dim=1)\n",
    "    # atan2\n",
    "    _atan = torch.atan2(_len, _diff[:, 0])\n",
    "    # abs\n",
    "    _abs = torch.abs(_atan)\n",
    "    \n",
    "    _abs = _abs.reshape(-1, 1, joint_count)\n",
    "    \n",
    "    #print(\"_abs s \", _abs.shape)\n",
    "    \n",
    "    _abs_weighted = _abs * joint_loss_weights\n",
    "    \n",
    "    _loss = torch.mean(_abs_weighted)   \n",
    "    return _loss\n",
    "\n",
    "# autoencoder loss function\n",
    "def loss(y, yhat):\n",
    "    _norm_loss = norm_loss(yhat)\n",
    "    _pos_loss = pos_loss(y, yhat)\n",
    "    _quat_loss = quat_loss(y, yhat)\n",
    "    \n",
    "    _total_loss = 0.0\n",
    "    _total_loss += _norm_loss * norm_loss_scale\n",
    "    _total_loss += _pos_loss * pos_loss_scale\n",
    "    _total_loss += _quat_loss * quat_loss_scale\n",
    "    \n",
    "    return _total_loss, _norm_loss, _pos_loss, _quat_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e3327-76f7-4ba8-835c-f5fa05ef7a5f",
   "metadata": {},
   "source": [
    "### Create Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf8754e-56be-403d-a06c-6eaee7be1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(pose_sequences, target_poses, teacher_forcing):\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    #print(\"ar_train_step\")    \n",
    "    #print(\"teacher_forcing \", teacher_forcing)\n",
    "    #print(\"pose_sequences s \", pose_sequences.shape)\n",
    "    #print(\"target_poses s \", target_poses.shape)\n",
    "\n",
    "    #_input_poses = pose_sequences.detach().clone()    \n",
    "    _input_poses = pose_sequences  \n",
    "    output_poses_length = target_poses.shape[1]\n",
    "    \n",
    "    #print(\"output_poses_length \", output_poses_length)\n",
    "    \n",
    "    _pred_poses_for_loss = []\n",
    "    _target_poses_for_loss = []\n",
    "    \n",
    "    for o_i in range(1, output_poses_length):\n",
    "        \n",
    "        #print(\"_input_poses s \", _input_poses.shape)\n",
    "        \n",
    "        _pred_poses = rnn(_input_poses)\n",
    "        _pred_poses = torch.unsqueeze(_pred_poses, axis=1)\n",
    "        \n",
    "        #print(\"_pred_poses s \", _pred_poses.shape)\n",
    "        \n",
    "        _target_poses = target_poses[:,o_i,:].detach().clone()\n",
    "        _target_poses = torch.unsqueeze(_target_poses, axis=1)\n",
    "\n",
    "        #print(\"_target_poses s \", _target_poses.shape)\n",
    "        \n",
    "        _pred_poses_for_loss.append(_pred_poses)\n",
    "        _target_poses_for_loss.append(_target_poses)\n",
    "        \n",
    "        # shift input pose seqeunce one pose to the right\n",
    "        # remove pose from beginning input pose sequence\n",
    "        # detach necessary to avoid error concerning running backprob a second time\n",
    "        _input_poses = _input_poses[:, 1:, :].detach().clone()\n",
    "        _target_poses = _target_poses.detach().clone()\n",
    "        _pred_poses = _pred_poses.detach().clone()\n",
    "        \n",
    "        # add predicted or target pose to end of input pose sequence\n",
    "        if teacher_forcing == True:\n",
    "            _input_poses = torch.concat((_input_poses, _target_poses), axis=1)\n",
    "        else:\n",
    "            #_pred_poses = torch.reshape(_pred_poses, (_pred_poses.shape[0], 1, joint_count, joint_dim))\n",
    "            _input_poses = torch.cat((_input_poses, _pred_poses), axis=1)\n",
    "            \n",
    "        #print(\"_input_poses s \", _input_poses.shape)\n",
    "\n",
    "        \n",
    "        #print(\"_input_poses 2 s \", _input_poses.shape)\n",
    "        \n",
    "    _pred_poses_for_loss = torch.cat(_pred_poses_for_loss, dim=1)\n",
    "    _target_poses_for_loss = torch.cat(_target_poses_for_loss, dim=1)\n",
    "    \n",
    "    #print(\"_pred_poses_for_loss 2 s \", _pred_poses_for_loss.shape)\n",
    "    #print(\"_target_poses_for_loss 2 s \", _target_poses_for_loss.shape)\n",
    "    \n",
    "    _loss, _norm_loss, _pos_loss, _quat_loss = loss(_target_poses_for_loss, _pred_poses_for_loss) \n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    _loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print(\"_ar_loss_total mean s \", _ar_loss_total.shape)\n",
    "    \n",
    "    #return _ar_loss, _ar_norm_loss, _ar_quat_loss\n",
    "    \n",
    "    return _loss, _norm_loss, _pos_loss, _quat_loss\n",
    "\n",
    "def test_step(pose_sequences, target_poses, teacher_forcing):\n",
    "    \n",
    "    rnn.eval()\n",
    "\n",
    "    #print(\"ar_train_step\")    \n",
    "    #print(\"teacher_forcing \", teacher_forcing)\n",
    "    #print(\"pose_sequences s \", pose_sequences.shape)\n",
    "    #print(\"target_poses s \", target_poses.shape)\n",
    "\n",
    "    #_input_poses = pose_sequences.detach().clone()    \n",
    "    _input_poses = pose_sequences  \n",
    "    output_poses_length = target_poses.shape[1]\n",
    "    \n",
    "    #print(\"output_poses_length \", output_poses_length)\n",
    "    \n",
    "    _pred_poses_for_loss = []\n",
    "    _target_poses_for_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for o_i in range(1, output_poses_length):\n",
    "            \n",
    "            #print(\"_input_poses s \", _input_poses.shape)\n",
    "            \n",
    "            _pred_poses = rnn(_input_poses)\n",
    "            _pred_poses = torch.unsqueeze(_pred_poses, axis=1)\n",
    "            \n",
    "            #print(\"_pred_poses s \", _pred_poses.shape)\n",
    "            \n",
    "            _target_poses = target_poses[:,o_i,:].detach().clone()\n",
    "            _target_poses = torch.unsqueeze(_target_poses, axis=1)\n",
    "    \n",
    "            #print(\"_target_poses s \", _target_poses.shape)\n",
    "            \n",
    "            _pred_poses_for_loss.append(_pred_poses)\n",
    "            _target_poses_for_loss.append(_target_poses)\n",
    "            \n",
    "            # shift input pose seqeunce one pose to the right\n",
    "            # remove pose from beginning input pose sequence\n",
    "            # detach necessary to avoid error concerning running backprob a second time\n",
    "            _input_poses = _input_poses[:, 1:, :].detach().clone()\n",
    "            _target_poses = _target_poses.detach().clone()\n",
    "            _pred_poses = _pred_poses.detach().clone()\n",
    "            \n",
    "            # add predicted or target pose to end of input pose sequence\n",
    "            if teacher_forcing == True:\n",
    "                _input_poses = torch.concat((_input_poses, _target_poses), axis=1)\n",
    "            else:\n",
    "                #_pred_poses = torch.reshape(_pred_poses, (_pred_poses.shape[0], 1, joint_count, joint_dim))\n",
    "                _input_poses = torch.cat((_input_poses, _pred_poses), axis=1)\n",
    "                \n",
    "            #print(\"_input_poses s \", _input_poses.shape)\n",
    "    \n",
    "            \n",
    "            #print(\"_input_poses 2 s \", _input_poses.shape)\n",
    "            \n",
    "        _pred_poses_for_loss = torch.cat(_pred_poses_for_loss, dim=1)\n",
    "        _target_poses_for_loss = torch.cat(_target_poses_for_loss, dim=1)\n",
    "        \n",
    "        #print(\"_pred_poses_for_loss 2 s \", _pred_poses_for_loss.shape)\n",
    "        #print(\"_target_poses_for_loss 2 s \", _target_poses_for_loss.shape)\n",
    "        \n",
    "        _loss, _norm_loss, _pos_loss, _quat_loss = loss(_target_poses_for_loss, _pred_poses_for_loss) \n",
    "    \n",
    "    #print(\"_ar_loss_total mean s \", _ar_loss_total.shape)\n",
    "    \n",
    "    #return _ar_loss, _ar_norm_loss, _ar_quat_loss\n",
    "    \n",
    "    rnn.train()\n",
    "    \n",
    "    return _loss, _norm_loss, _pos_loss, _quat_loss\n",
    "\n",
    "\n",
    "def train(train_dataloader, test_dataloader, epochs):\n",
    "    \n",
    "    loss_history = {}\n",
    "    loss_history[\"train\"] = []\n",
    "    loss_history[\"test\"] = []\n",
    "    loss_history[\"norm\"] = []\n",
    "    loss_history[\"pos\"] = []\n",
    "    loss_history[\"quat\"] = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        _train_loss_per_epoch = []\n",
    "        _norm_loss_per_epoch = []\n",
    "        _pos_loss_per_epoch = []\n",
    "        _quat_loss_per_epoch = []\n",
    "\n",
    "        for train_batch in train_dataloader:\n",
    "            input_pose_sequences = train_batch[0].to(device)\n",
    "            target_poses = train_batch[1].to(device)\n",
    "            \n",
    "            use_teacher_forcing = np.random.uniform() < teacher_forcing_prob\n",
    "            \n",
    "            _loss, _norm_loss, _pos_loss, _quat_loss = train_step(input_pose_sequences, target_poses, use_teacher_forcing)\n",
    "            \n",
    "            _loss = _loss.detach().cpu().numpy()\n",
    "            _norm_loss = _norm_loss.detach().cpu().numpy()\n",
    "            _pos_loss = _pos_loss.detach().cpu().numpy()\n",
    "            _quat_loss = _quat_loss.detach().cpu().numpy()\n",
    "            \n",
    "            _train_loss_per_epoch.append(_loss)\n",
    "            _norm_loss_per_epoch.append(_norm_loss)\n",
    "            _pos_loss_per_epoch.append(_pos_loss)\n",
    "            _quat_loss_per_epoch.append(_quat_loss)\n",
    "\n",
    "        _train_loss_per_epoch = np.mean(np.array(_train_loss_per_epoch))\n",
    "        _norm_loss_per_epoch = np.mean(np.array(_norm_loss_per_epoch))\n",
    "        _pos_loss_per_epoch = np.mean(np.array(_pos_loss_per_epoch))\n",
    "        _quat_loss_per_epoch = np.mean(np.array(_quat_loss_per_epoch))\n",
    "\n",
    "        _test_loss_per_epoch = []\n",
    "        \n",
    "        for test_batch in test_dataloader:\n",
    "            input_pose_sequences = train_batch[0].to(device)\n",
    "            target_poses = train_batch[1].to(device)\n",
    "            \n",
    "            use_teacher_forcing = np.random.uniform() < teacher_forcing_prob\n",
    "            \n",
    "            _loss, _, _, _ = test_step(input_pose_sequences, target_poses, use_teacher_forcing)\n",
    "            #_loss, _, _ = test_step(input_pose_sequences, target_poses)\n",
    "            \n",
    "            _loss = _loss.detach().cpu().numpy()\n",
    "            \n",
    "            _test_loss_per_epoch.append(_loss)\n",
    "        \n",
    "        _test_loss_per_epoch = np.mean(np.array(_test_loss_per_epoch))\n",
    "        \n",
    "        if epoch % model_save_interval == 0 and save_weights == True:\n",
    "            torch.save(rnn.state_dict(), \"results/weights/rnn_weights_epoch_{}\".format(epoch))\n",
    "        \n",
    "        loss_history[\"train\"].append(_train_loss_per_epoch)\n",
    "        loss_history[\"test\"].append(_test_loss_per_epoch)\n",
    "        loss_history[\"norm\"].append(_norm_loss_per_epoch)\n",
    "        loss_history[\"pos\"].append(_pos_loss_per_epoch)\n",
    "        loss_history[\"quat\"].append(_quat_loss_per_epoch)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print ('epoch {} : train: {:01.4f} test: {:01.4f} norm {:01.4f} pos {:01.4f} quat {:01.4f} time {:01.2f}'.format(epoch + 1, _train_loss_per_epoch, _test_loss_per_epoch, _norm_loss_per_epoch, _pos_loss_per_epoch, _quat_loss_per_epoch, time.time()-start))\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dfa652-b9c8-47bc-9dfa-ddee58f232b8",
   "metadata": {},
   "source": [
    "## Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4262c2-6b42-4953-af65-3de8f6d2f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = train(train_loader, test_loader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af754794-a52a-459a-a2a5-aa435df854f7",
   "metadata": {},
   "source": [
    "## Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfa3af-2014-48da-a3a1-3495c452e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_loss_as_csv(loss_history, \"results/histories/rnn_history_{}.csv\".format(epochs))\n",
    "utils.save_loss_as_image(loss_history, \"results/histories/rnn_history_{}.png\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a724ab0-de4a-4dd5-a047-973e48d15b0a",
   "metadata": {},
   "source": [
    "## Save Final Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d83eb1-beb9-4649-87ba-907e5c82d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn.state_dict(), \"results/weights/rnn_weights_epoch_{}\".format(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e288353f-a15a-4d19-a9ee-8beef6a51d78",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1be5a55-8a2f-4493-8b7c-f161bc96aa87",
   "metadata": {},
   "source": [
    "## Motion Export Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16cdc36-c82d-4271-a3ce-cca18d52d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "poseRenderer = PoseRenderer(edge_list)\n",
    "\n",
    "def export_sequence_anim(pose_sequence, file_name):\n",
    "    \n",
    "    pose_count = pose_sequence.shape[0]\n",
    "    pose_sequence = np.reshape(pose_sequence, (pose_count, joint_count, joint_dim))\n",
    "    \n",
    "    pose_sequence = torch.tensor(np.expand_dims(pose_sequence, axis=0)).to(device)\n",
    "    zero_trajectory = torch.tensor(np.zeros((1, pose_count, 3), dtype=np.float32)).to(device)\n",
    "    \n",
    "    skel_sequence = forward_kinematics(pose_sequence, zero_trajectory)\n",
    "    \n",
    "    skel_sequence = skel_sequence.detach().cpu().numpy()\n",
    "    skel_sequence = np.squeeze(skel_sequence)    \n",
    "    \n",
    "    view_min, view_max = utils.get_equal_mix_max_positions(skel_sequence)\n",
    "    skel_images = poseRenderer.create_pose_images(skel_sequence, view_min, view_max, view_ele, view_azi, view_line_width, view_size, view_size)\n",
    "    skel_images[0].save(file_name, save_all=True, append_images=skel_images[1:], optimize=False, duration=33.0, loop=0)\n",
    "\n",
    "def export_sequence_bvh(pose_sequence, file_name):\n",
    "    \n",
    "    pose_count = pose_sequence.shape[0]\n",
    "\n",
    "    pred_dataset = {}\n",
    "    pred_dataset[\"frame_rate\"] = mocap_data[\"frame_rate\"]\n",
    "    pred_dataset[\"rot_sequence\"] = mocap_data[\"rot_sequence\"]\n",
    "    pred_dataset[\"skeleton\"] = mocap_data[\"skeleton\"]\n",
    "    pred_dataset[\"motion\"] = {}\n",
    "    pred_dataset[\"motion\"][\"pos_local\"] = np.repeat(np.expand_dims(pred_dataset[\"skeleton\"][\"offsets\"], axis=0), pose_count, axis=0)\n",
    "    pred_dataset[\"motion\"][\"rot_local\"] = pose_sequence\n",
    "    pred_dataset[\"motion\"][\"rot_local_euler\"] = mocap_tools.quat_to_euler_bvh(pred_dataset[\"motion\"][\"rot_local\"], pred_dataset[\"rot_sequence\"])\n",
    "\n",
    "    pred_bvh = mocap_tools.mocap_to_bvh(pred_dataset)\n",
    "    \n",
    "    bvh_tools.write(pred_bvh, file_name)\n",
    "\n",
    "def export_sequence_fbx(pose_sequence, file_name):\n",
    "    \n",
    "    pose_count = pose_sequence.shape[0]\n",
    "    \n",
    "    pred_dataset = {}\n",
    "    pred_dataset[\"frame_rate\"] = mocap_data[\"frame_rate\"]\n",
    "    pred_dataset[\"rot_sequence\"] = mocap_data[\"rot_sequence\"]\n",
    "    pred_dataset[\"skeleton\"] = mocap_data[\"skeleton\"]\n",
    "    pred_dataset[\"motion\"] = {}\n",
    "    pred_dataset[\"motion\"][\"pos_local\"] = np.repeat(np.expand_dims(pred_dataset[\"skeleton\"][\"offsets\"], axis=0), pose_count, axis=0)\n",
    "    pred_dataset[\"motion\"][\"rot_local\"] = pose_sequence\n",
    "    pred_dataset[\"motion\"][\"rot_local_euler\"] = mocap_tools.quat_to_euler(pred_dataset[\"motion\"][\"rot_local\"], pred_dataset[\"rot_sequence\"])\n",
    "    \n",
    "    pred_fbx = mocap_tools.mocap_to_fbx([pred_dataset])\n",
    "    \n",
    "    fbx_tools.write(pred_fbx, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc3fe4-7817-4149-b5a2-1cfc8c63cae9",
   "metadata": {},
   "source": [
    "## Motion Continuation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9939578-a87d-4f30-acff-9eadc574729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pred_sequence(pose_sequence, pose_count):\n",
    "    \n",
    "    rnn.eval()\n",
    "\n",
    "    start_seq = pose_sequence\n",
    "    start_seq = torch.from_numpy(start_seq).to(device)\n",
    "    start_seq = torch.reshape(start_seq, (seq_input_length, pose_dim))\n",
    "    \n",
    "    next_seq = start_seq\n",
    "    \n",
    "    pred_poses = []\n",
    "    \n",
    "    for i in range(pose_count):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_pose = rnn(torch.unsqueeze(next_seq, axis=0))\n",
    "\n",
    "        # normalize pred pose\n",
    "        pred_pose = torch.squeeze(pred_pose)\n",
    "        pred_pose = pred_pose.reshape((-1, 4))\n",
    "        pred_pose = nn.functional.normalize(pred_pose, p=2, dim=1)\n",
    "        pred_pose = pred_pose.reshape((1, pose_dim))\n",
    "\n",
    "        pred_poses.append(pred_pose)\n",
    "    \n",
    "        #print(\"next_seq s \", next_seq.shape)\n",
    "        #print(\"pred_pose s \", pred_pose.shape)\n",
    "\n",
    "        next_seq = torch.cat([next_seq[1:,:], pred_pose], axis=0)\n",
    "\n",
    "    pred_poses = torch.cat(pred_poses, dim=0)\n",
    "    pred_poses = pred_poses.reshape((pose_count, joint_count, joint_dim))\n",
    "\n",
    "    rnn.train()\n",
    "    \n",
    "    return pred_poses.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30155421-e68e-43d4-a769-c40388e8c859",
   "metadata": {},
   "source": [
    "## Perform Motion Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25752ef-5e26-45c4-a487-a7b6ba45fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_index = 0\n",
    "seq_start = 1000\n",
    "seq_length = 1000\n",
    "\n",
    "seq_index_gui = widgets.IntText(value=seq_index, description=\"Motion Sequence Index:\", style={'description_width': 'initial'})\n",
    "seq_start_gui = widgets.IntText(value=seq_start, description=\"Motion Start Frame:\", style={'description_width': 'initial'})\n",
    "seq_length_gui = widgets.IntText(value=seq_length, description=\"Motion Frame Count:\", style={'description_width': 'initial'})\n",
    "\n",
    "display(seq_index_gui)\n",
    "display(seq_start_gui)\n",
    "display(seq_length_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a051f7-050e-459b-8bfe-415932d5d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_index = seq_index_gui.value\n",
    "seq_start = seq_start_gui.value\n",
    "seq_length = seq_length_gui.value\n",
    "\n",
    "# create original sequence\n",
    "\n",
    "orig_sequence = all_mocap_data[seq_index][\"motion\"][\"rot_local\"].astype(np.float32)\n",
    "\n",
    "export_sequence_anim(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.gif\".format(seq_start, seq_length))\n",
    "export_sequence_fbx(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.fbx\".format(seq_start, seq_length))\n",
    "#export_sequence_bvh(orig_sequence[seq_start:seq_start+seq_length], \"results/anims/orig_sequence_seq_start_{}_length_{}.bvh\".format(seq_start, seq_length))\n",
    "\n",
    "# create predicted sequence\n",
    "\n",
    "orig_sequence = all_mocap_data[seq_index][\"motion\"][\"rot_local\"].astype(np.float32)\n",
    "pred_sequence = create_pred_sequence(orig_sequence[seq_start:seq_start+seq_input_length], seq_length)\n",
    "\n",
    "export_sequence_anim(pred_sequence, \"results/anims/pred_sequence_epoch_{}_seq_start_{}_length_{}.gif\".format(epochs, seq_start, seq_length))\n",
    "export_sequence_fbx(pred_sequence, \"results/anims/pred_sequence_epoch_{}_seq_start_{}_length_{}.fbx\".format(epochs, seq_start, seq_length))\n",
    "#export_sequence_bvh(pred_sequence, \"results/anims/pred_sequence_epoch_{}_seq_start_{}_length_{}.bvh\".format(epochs, seq_start, seq_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb3365-6f30-4209-b513-240dff1d3133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
