{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba15785d-f47a-4e46-b995-c508d7351bff",
   "metadata": {},
   "source": [
    "# Audio Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047b354-ae03-441b-8124-b42a8dfe4c87",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0eed90-6961-41fe-8dc8-32ce50b4bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "import audio_analysis\n",
    "from matplotlib import pyplot as plt\n",
    "import wave\n",
    "import time\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d20c55-75c0-4bf8-a706-e94f4eb2676e",
   "metadata": {},
   "source": [
    "## Audio Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3c58b-8262-4007-b306-540aab295da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = \"../../Data/Audio/Gutenberg/Night_and_Day_by_Virginia_Woolf_48khz.wav\"\n",
    "audio_sample_rate = 48000\n",
    "audio_channel_count = 1\n",
    "audio_range_sec = [ 10.0, 70.0 ]\n",
    "audio_excerpt_length = 1.0 # in secs\n",
    "audio_excerpt_offset = 0.5 # in secs\n",
    "\n",
    "audio_feature_names = [\"root mean square\", \"mfcc\"]\n",
    "\n",
    "audio_audio_file_path_gui = widgets.Text(value=audio_file_path, description=\"Audio File Path:\", style={'description_width': 'initial'}) \n",
    "audio_sample_rate_gui = widgets.IntText(value=audio_sample_rate, description=\"Audio Sample Rate:\", style={'description_width': 'initial'})\n",
    "audio_channel_count_gui = widgets.IntText(value=audio_channel_count, description=\"Audio Channel Count:\", style={'description_width': 'initial'})\n",
    "audio_range_sec_gui_1 = widgets.FloatText(value=audio_range_sec[0], description=\"Audio Range Start (secs):\", style={'description_width': 'initial'})\n",
    "audio_range_sec_gui_2 = widgets.FloatText(value=audio_range_sec[1], description=\"Audio Range End (secs):\", style={'description_width': 'initial'})\n",
    "audio_excerpt_length_gui = widgets.FloatText(value=audio_excerpt_length, description=\"Audio Excerpt Length (secs):\", style={'description_width': 'initial'})\n",
    "audio_excerpt_offset_gui = widgets.FloatText(value=audio_excerpt_offset, description=\"Audio Excerpt Offset (secs):\", style={'description_width': 'initial'})\n",
    "\n",
    "audio_feature_names_gui = widgets.SelectMultiple(\n",
    "    options=[ \"root mean square\",  \"chroma stft\", \"chroma cqt\", \"chroma cens\", \"chroma vqt\", \"mel spectrogram\", \n",
    "\"mfcc\", \"spectral centroid\", \"spectral bandwidth\", \"spectral contrast\", \"spectral flatness\", \"spectral rolloff\", \"tempo\", \"tempogram\", \"tempogram ratio\" ],\n",
    "    value=[\"root mean square\"],\n",
    "    description='Audio Features',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "display(audio_audio_file_path_gui)\n",
    "display(audio_sample_rate_gui)\n",
    "display(audio_channel_count_gui)\n",
    "display(audio_range_sec_gui_1)\n",
    "display(audio_range_sec_gui_2)\n",
    "display(audio_excerpt_length_gui)\n",
    "display(audio_excerpt_offset_gui)\n",
    "display(audio_feature_names_gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0976061a-c974-4455-98c3-b175055b1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = audio_audio_file_path_gui.value\n",
    "audio_sample_rate = audio_sample_rate_gui.value\n",
    "audio_channel_count = audio_channel_count_gui.value\n",
    "audio_range_sec[0] = audio_range_sec_gui_1.value\n",
    "audio_range_sec[1] = audio_range_sec_gui_2.value\n",
    "audio_excerpt_length = audio_excerpt_length_gui.value\n",
    "audio_excerpt_offset = audio_excerpt_offset_gui.value\n",
    "audio_feature_names = list(audio_feature_names_gui.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcbe325-08dd-47ad-ac20-f8c95ad89d0b",
   "metadata": {},
   "source": [
    "## Load Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fc1e1b-6ce5-4dd3-afca-6c4312405397",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_waveform, _ = librosa.load(audio_file_path, sr=audio_sample_rate)\n",
    "\n",
    "if audio_channel_count > 1:\n",
    "    audio_waveform = audio_waveform[0]\n",
    "\n",
    "audio_waveform = audio_waveform[int(audio_range_sec[0] * audio_sample_rate):int(audio_range_sec[1] * audio_sample_rate)]\n",
    "audio_waveform_sc = audio_waveform.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b2607-099a-44e0-a63a-7ccd7fa08259",
   "metadata": {},
   "source": [
    "## Create Audio Excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2e736-d3b9-436f-8aec-10fcf3fdd224",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_excerpts = []\n",
    "\n",
    "audio_excerpt_length_sc = int(audio_excerpt_length * audio_sample_rate)\n",
    "audio_excerpt_offset_sc = int(audio_excerpt_offset * audio_sample_rate)\n",
    "\n",
    "for sI in range(0, audio_waveform_sc - audio_excerpt_length_sc, audio_excerpt_offset_sc):\n",
    "    \n",
    "    audio_excerpt = audio_waveform[sI:sI + audio_excerpt_length_sc]\n",
    "    audio_excerpts.append(audio_excerpt)\n",
    "    \n",
    "audio_waveforms = np.stack(audio_excerpts, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44598b18-62de-4810-ada2-7bfadce000f2",
   "metadata": {},
   "source": [
    "## Calculate Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310713b3-2abe-4ffd-bbed-3189b7d7bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features = {}\n",
    "\n",
    "audio_features[\"waveform\"] = audio_waveforms\n",
    "if \"root mean square\" in audio_feature_names:\n",
    "    audio_features[\"root mean square\"] = audio_analysis.rms(audio_waveforms)\n",
    "if \"chroma stft\" in audio_feature_names:\n",
    "    audio_features[\"chroma stft\"] = audio_analysis.chroma_stft(audio_waveforms, audio_sample_rate)\n",
    "if \"chroma cqt\" in audio_feature_names:\n",
    "    audio_features[\"chroma cqt\"] = audio_analysis.chroma_cqt(audio_waveforms, audio_sample_rate)\n",
    "if \"chroma cens\" in audio_feature_names:\n",
    "    audio_features[\"chroma cens\"] = audio_analysis.chroma_cens(audio_waveforms, audio_sample_rate)\n",
    "if \"chroma vqt\" in audio_feature_names:\n",
    "    audio_features[\"chroma vqt\"] = audio_analysis.chroma_vqt(audio_waveforms, audio_sample_rate)\n",
    "if \"mel spectrogram\" in audio_feature_names:\n",
    "    audio_features[\"mel spectrogram\"] = audio_analysis.mel_spectrogram(audio_waveforms, audio_sample_rate)\n",
    "if \"mfcc\" in audio_feature_names:\n",
    "    audio_features[\"mfcc\"] = audio_analysis.mfcc(audio_waveforms, audio_sample_rate)\n",
    "if \"spectral centroid\" in audio_feature_names:\n",
    "    audio_features[\"spectral centroid\"] = audio_analysis.spectral_centroid(audio_waveforms, audio_sample_rate)\n",
    "if \"spectral bandwidth\" in audio_feature_names:\n",
    "    audio_features[\"spectral bandwidth\"] = audio_analysis.spectral_bandwidth(audio_waveforms, audio_sample_rate)\n",
    "if \"spectral contrast\" in audio_feature_names:\n",
    "    audio_features[\"spectral contrast\"] = audio_analysis.spectral_contrast(audio_waveforms, audio_sample_rate)\n",
    "if \"spectral flatness\" in audio_feature_names:\n",
    "    audio_features[\"spectral flatness\"] = audio_analysis.spectral_flatness(audio_waveforms)\n",
    "if \"spectral rolloff\" in audio_feature_names:\n",
    "    audio_features[\"spectral rolloff\"] = audio_analysis.spectral_rolloff(audio_waveforms, audio_sample_rate)\n",
    "if \"tempo\" in audio_feature_names:\n",
    "    audio_features[\"tempo\"] = audio_analysis.tempo(audio_waveforms, audio_sample_rate)\n",
    "if \"tempogram\" in audio_feature_names:\n",
    "    audio_features[\"tempogram\"] = audio_analysis.tempogram(audio_waveforms, audio_sample_rate)\n",
    "if \"tempogram ratio\" in audio_feature_names:\n",
    "    audio_features[\"tempogram ratio\"] = audio_analysis.tempogram_ratio(audio_waveforms, audio_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079d5ef-c6e6-4cd7-b7e4-dc905cf868cf",
   "metadata": {},
   "source": [
    "## Normalise Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceeff43-0c67-4963-8759-a43d2ffd2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio_feature_name in list(audio_features.keys()):\n",
    "    \n",
    "    #print(audio_feature_name)\n",
    "    \n",
    "    audio_feature = audio_features[audio_feature_name]\n",
    "    \n",
    "    audio_feature_mean = np.mean(audio_feature)\n",
    "    audio_feature_std = np.std(audio_feature)\n",
    "    \n",
    "    audio_feature_norm = (audio_feature - audio_feature_mean) / audio_feature_std\n",
    "    \n",
    "    #print(\"audio_feature_norm s \", audio_feature_norm.shape)\n",
    "\n",
    "    audio_features[audio_feature_name + \" norm\"] = audio_feature_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5eafd-a07f-4879-88ce-a191ca0364d6",
   "metadata": {},
   "source": [
    "## Find Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ffea1-d628-428d-9100-741fb22c7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all waveforms and audio features\n",
    "audio_features_proc = []\n",
    "for audio_feature_name in audio_feature_names:\n",
    "    audio_norm_feature_name = audio_feature_name + \" norm\"\n",
    "    audio_feature = audio_features[audio_norm_feature_name]\n",
    "    #print(\"name \", audio_norm_feature_name, \" shape \", audio_feature.shape)\n",
    "    audio_features_proc.append(audio_feature)\n",
    "    \n",
    "audio_features_proc = np.concatenate(audio_features_proc, axis=1)\n",
    "audio_waveforms_proc = np.copy(audio_waveforms)\n",
    "\n",
    "# select first audio feature to begin search with\n",
    "nn_current_index = 0\n",
    "nn_current_waveform = audio_waveforms_proc[nn_current_index]\n",
    "nn_current_feature = audio_features_proc[nn_current_index]\n",
    "nn_current_feature = np.expand_dims(nn_current_feature, 0)\n",
    "\n",
    "# prepare empty waveform to copy waveforms corresponding to nearest features into\n",
    "nn_element_count = audio_features_proc.shape[0]\n",
    "gen_waveform_sample_count = 2 * audio_excerpt_length_sc + (nn_element_count - 1) * audio_excerpt_offset_sc\n",
    "gen_waveform = np.zeros(gen_waveform_sample_count)\n",
    "\n",
    "# create amplitude enevelope for blending audio waveforms into gen_waveform\n",
    "audio_sample_overlap = audio_excerpt_length_sc - audio_excerpt_offset_sc\n",
    "hann_window = torch.hann_window(audio_sample_overlap * 2).numpy()\n",
    "\n",
    "amplitude_envelope = np.ones([audio_excerpt_length_sc])\n",
    "amplitude_envelope[:audio_sample_overlap] *= hann_window[:audio_sample_overlap]\n",
    "amplitude_envelope[-audio_sample_overlap:] *= hann_window[audio_sample_overlap:]\n",
    "\n",
    "# add first waveform to gen waveform\n",
    "gen_waveform[:audio_excerpt_length_sc] += nn_current_waveform * amplitude_envelope\n",
    "\n",
    "# iterate through all neighbors\n",
    "sI = audio_excerpt_length_sc -  audio_sample_overlap # sample index for waveform insertion in gen waveform\n",
    "\n",
    "remaining_neighbor_gui = widgets.Label(value=str(nn_element_count))\n",
    "\n",
    "display(remaining_neighbor_gui)\n",
    "\n",
    "while nn_element_count > 0:\n",
    "\n",
    "    remaining_neighbor_gui.value = str(nn_element_count)\n",
    "\n",
    "    # search nearest element\n",
    "    nn_distances = np.linalg.norm(audio_features_proc - nn_current_feature, axis=1)\n",
    "    k = 2\n",
    "    nn_indices = nn_distances.argsort()[:k]\n",
    "\n",
    "    # replace current element with nearest element\n",
    "    nn_previous_index = nn_current_index\n",
    "    nn_current_index = nn_indices[1]\n",
    "    nn_current_waveform = audio_waveforms_proc[nn_current_index]\n",
    "    nn_current_feature = audio_features_proc[nn_current_index]\n",
    "    nn_current_feature = np.expand_dims(nn_current_feature, 0)\n",
    "    \n",
    "    # blend waveform corresponding to current element into gen waveform\n",
    "    gen_waveform[sI:sI + audio_excerpt_length_sc] += nn_current_waveform * amplitude_envelope\n",
    "    \n",
    "    # remove previous element\n",
    "    if nn_previous_index == 0:\n",
    "        audio_waveforms_proc = np.copy(audio_waveforms_proc[nn_previous_index + 1:])\n",
    "        audio_features_proc = np.copy(audio_features_proc[nn_previous_index + 1:])\n",
    "    elif nn_previous_index == audio_waveforms_proc.shape[0] - 1:\n",
    "        audio_waveforms_proc = np.copy(audio_waveforms_proc[:nn_previous_index])\n",
    "        audio_features_proc = np.copy(audio_features_proc[:nn_previous_index])\n",
    "    else:\n",
    "        audio_waveforms_proc = np.copy(np.concatenate([audio_waveforms_proc[:nn_previous_index], audio_waveforms_proc[nn_previous_index + 1:]], axis=0))\n",
    "        audio_features_proc = np.copy(np.concatenate([audio_features_proc[:nn_previous_index], audio_features_proc[nn_previous_index + 1:]], axis=0))\n",
    "\n",
    "    nn_element_count -= 1\n",
    "    sI += audio_excerpt_length_sc - audio_sample_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef9187b-f5b1-41b4-aba3-00507c640763",
   "metadata": {},
   "source": [
    "## Play Generated Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64cf8ab-a64f-4d04-b9b8-7f9748036d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(gen_waveform,rate=audio_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142075d-e729-44f6-9e24-3bfa84afdda1",
   "metadata": {},
   "source": [
    "## Save Generated Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc51ee3-745a-4ca4-b46d-04920bfca56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_waveform = torch.tensor(gen_waveform)\n",
    "gen_waveform = gen_waveform.unsqueeze(0)\n",
    "torchaudio.save(\"data_proc/gen_waveform.wav\", gen_waveform, audio_sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
